{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V4.2 Update: in readme.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "1f996713-dd2b-4dcf-bcb6-2be1ded48001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch XLA not found, will attempt to install\n",
            "TPU support installed. Please restart the runtime now.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will attempt to install\")\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install pygame-ce pymunk stable-baselines3 stable-baselines3[extra] shimmy>=2.0 optuna\n",
        "!pip install -q cloud-tpu-client\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "916c6da2-1956-4451-a637-9544c0b3580d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'=2.0'\t capture   game_history   logs\t models   sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6bbqyvjZ1PZu"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhEqO-xFu4AI"
      },
      "source": [
        "## Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gJwfQb_Yuz1I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "class Recorder:\n",
        "\n",
        "    def __init__(self, task: str = \"game_history_record\"):\n",
        "        \"\"\"\n",
        "        tasks:\n",
        "        1. game_history_record\n",
        "        2. temp_memory\n",
        "        \"\"\"\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \"\"\n",
        "        if task == \"game_history_record\":\n",
        "            collection_name = self.get_newest_record_name()\n",
        "            self.json_file_path = CURRENT_DIR + \"./game_history/\" + collection_name + \".json\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)\n",
        "\n",
        "        if os.path.exists(self.json_file_path):\n",
        "            print(\"Loading the json memory file\")\n",
        "            self.memory = self.load(self.json_file_path)\n",
        "        else:\n",
        "            print(\"The json memory file does not exist. Creating new file.\")\n",
        "            self.memory = {\"game_records\": []}  # Direct dictionary instead of json.loads\n",
        "            with open(self.json_file_path, \"w\") as f:\n",
        "                json.dump(self.memory, f)\n",
        "\n",
        "    def get(self):\n",
        "        print(\"Getting the json memory\")\n",
        "        return self.memory\n",
        "\n",
        "    def add_no_limit(self, data: float, ):\n",
        "        \"\"\"\n",
        "        Add a records.\n",
        "\n",
        "        Args:\n",
        "            role: The role of the sender (e.g., 'user', 'assistant')\n",
        "            message: The message content\n",
        "        \"\"\"\n",
        "        self.memory[\"game_records\"].append({\n",
        "            \"game_total_duration\": data,\n",
        "            \"timestamp\": str(datetime.datetime.now())\n",
        "        })\n",
        "\n",
        "        self.save(self.json_file_path)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump(self.memory, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving memory to {file_path}: {e}\")\n",
        "\n",
        "    def load(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading memory from {file_path}: {e}\")\n",
        "            return {\"game_records\": []}\n",
        "\n",
        "    def get_newest_record_name(self) -> str:\n",
        "        \"\"\"\n",
        "        傳回最新的對話歷史資料和集的名稱 (game_YYYY_MM)\n",
        "            - 例如: \"game_2022-01\"\n",
        "        \"\"\"\n",
        "\n",
        "        this_month = datetime.datetime.now().strftime(\"%Y-%m\")\n",
        "        return \"record_\" + this_month"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnA8wZtosmeN"
      },
      "source": [
        "## Shapes & Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qL9XJcPV03-M"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Shape:\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape: Optional[pymunk.Shape] = None,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a physical shape with associated body.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the body\n",
        "            velocity: Initial velocity (vx, vy) of the body\n",
        "            body: The pymunk Body to attach to this shape\n",
        "            shape: The pymunk Shape for collision detection\n",
        "        \"\"\"\n",
        "\n",
        "        self.body = body\n",
        "        self.default_position = position\n",
        "        self.default_velocity = velocity\n",
        "        self.body.position = position\n",
        "        self.body.velocity = velocity\n",
        "        self.default_angular_velocity = 0\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the body to its default position, velocity and angular velocity.\"\"\"\n",
        "        self.body.position = self.default_position\n",
        "        self.body.velocity = self.default_velocity\n",
        "        self.body.angular_velocity = self.default_angular_velocity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dR0LWM9r03-N"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "\n",
        "# from shapes.shape import Shape\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Circle(Shape):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape_radio: float = 20,\n",
        "                shape_mass: float = 1,\n",
        "                shape_friction: float = 0.1,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a circular physics object.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the circle\n",
        "            velocity: Initial velocity (vx, vy) of the circle\n",
        "            body: The pymunk Body to attach this circle to\n",
        "            shape_radio: Radius of the circle in pixels\n",
        "            shape_mass: Mass of the circle\n",
        "            shape_friction: Friction coefficient for the circle\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(position, velocity, body)\n",
        "        self.shape_radio = shape_radio\n",
        "        self.shape = pymunk.Circle(self.body, shape_radio)\n",
        "        self.shape.mass = shape_mass\n",
        "        self.shape.friction = shape_friction\n",
        "        self.shape.elasticity = 0.8  # Add some bounce to make the simulation more interesting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vECK-IK203-N"
      },
      "source": [
        "## Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3o-tue0N03-N"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "class Levels:\n",
        "    def __init__(self, space, window_x: int = 1000, window_y: int = 600):\n",
        "        self.space = space\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "\n",
        "    def create_player(self,\n",
        "                      window_x: int = 1000,\n",
        "                      window_y: int = 600,\n",
        "                     ):\n",
        "        \"\"\"Create the ball with physics properties\"\"\"\n",
        "        dynamic_body = pymunk.Body()  # Ball body\n",
        "        ball_radius = int(window_x / 67)\n",
        "        player = Circle(\n",
        "            position=(window_x / 2, window_y / 5),\n",
        "            velocity=(0, 0),\n",
        "            body=dynamic_body,\n",
        "            shape_radio=ball_radius,\n",
        "            shape_friction=100,\n",
        "        )\n",
        "        # Store initial values for reset\n",
        "        default_player_position = (window_x / 2, window_y / 5)\n",
        "        return {\n",
        "            \"type\": \"player\",\n",
        "            \"shape\": player,\n",
        "            \"default_position\": default_player_position,\n",
        "            \"body\": dynamic_body,\n",
        "            \"ball_radius\": ball_radius,\n",
        "        }\n",
        "\n",
        "    def create_platform(self,\n",
        "                        platform_shape: str = \"circle\",\n",
        "                        platform_proportion: float = 0.4,\n",
        "                        window_x: int = 1000,\n",
        "                        window_y: int = 600,\n",
        "                       ):\n",
        "        \"\"\"\n",
        "        Create the platform with physics properties\n",
        "        platform_shape: circle, rectangle\n",
        "        platform_length: Length of a rectangle or Diameter of a circle\n",
        "        \"\"\"\n",
        "        platform_length = int(window_x * platform_proportion)\n",
        "\n",
        "        # Create game bodies\n",
        "        kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
        "        kinematic_body.position = (window_x / 2, (window_y / 3) * 2)\n",
        "        default_kinematic_position = kinematic_body.position\n",
        "\n",
        "        if platform_shape == \"circle\":\n",
        "            platform_length = platform_length / 2 # radius\n",
        "            platform = pymunk.Circle(kinematic_body, platform_length)\n",
        "            platform.mass = 1  # 质量对 Kinematic 物体无意义，但需要避免除以零错误\n",
        "            platform.friction = 0.7\n",
        "\n",
        "        elif platform_shape == \"rectangle\":\n",
        "            platform_length = platform_length\n",
        "            vs = [(-platform_length/2, -10),\n",
        "                (platform_length/2, -10),\n",
        "                (platform_length/2, 10),\n",
        "                (-platform_length/2, 10)]\n",
        "\n",
        "            platform = pymunk.Poly(kinematic_body, vs)\n",
        "        platform.friction = 0.7\n",
        "        platform.rotation = 0\n",
        "        kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        return {\n",
        "            \"type\": \"platform\",\n",
        "            \"shape\": platform,\n",
        "            \"default_position\": default_kinematic_position,\n",
        "            \"body\": kinematic_body,\n",
        "            \"platform_length\": platform_length,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU2cMKZZ03-N"
      },
      "source": [
        "### Level1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hMgnftJ703-N"
      },
      "outputs": [],
      "source": [
        "class Level1(Levels):\n",
        "    \"\"\"\n",
        "    Level 1: Basic setup with a dynamic body and a static kinematic body.\n",
        "    \"\"\"\n",
        "    def __init__(self, space):\n",
        "        super().__init__(space)\n",
        "        self.space = space\n",
        "\n",
        "\n",
        "    def setup(self, window_x, window_y):\n",
        "        player = super().create_player(window_x=window_x, window_y=window_y)\n",
        "        platform = super().create_platform(window_x=window_x, window_y=window_y)\n",
        "        self.space.add(player[\"body\"], player[\"shape\"].shape)\n",
        "        self.space.add(platform[\"body\"], platform[\"shape\"])\n",
        "        self.dynamic_body = player[\"body\"]\n",
        "        self.kinematic_body = platform[\"body\"]\n",
        "        self.default_player_position = player[\"default_position\"]\n",
        "\n",
        "        return (player), (platform)\n",
        "\n",
        "    def action(self):\n",
        "        \"\"\"\n",
        "        shape state changes in the game\n",
        "        \"\"\"\n",
        "        # Noting to do in this level\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the level to its initial state.\n",
        "        \"\"\"\n",
        "        self.dynamic_body.position = self.default_player_position\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        self.space.reindex_shapes_for_body(self.dynamic_body)\n",
        "        self.space.reindex_shapes_for_body(self.kinematic_body)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utdqc0xf03-O"
      },
      "source": [
        "### Level2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KW5YaemX03-O"
      },
      "outputs": [],
      "source": [
        "class Level2(Levels):\n",
        "    \"\"\"\n",
        "    Level 1: Basic setup with a dynamic body and a static kinematic body.\n",
        "    \"\"\"\n",
        "    def __init__(self, space):\n",
        "        super().__init__(space)\n",
        "        self.space = space\n",
        "        self.last_angular_velocity_change_time = time.time()\n",
        "        self.angular_velocity_change_timeout = 5 # sec\n",
        "\n",
        "\n",
        "    def setup(self, window_x, window_y):\n",
        "        player = super().create_player(window_x=window_x, window_y=window_y)\n",
        "        platform = super().create_platform(window_x=window_x, window_y=window_y)\n",
        "        self.space.add(player[\"body\"], player[\"shape\"].shape)\n",
        "        self.space.add(platform[\"body\"], platform[\"shape\"])\n",
        "        self.dynamic_body = player[\"body\"]\n",
        "        self.kinematic_body = platform[\"body\"]\n",
        "        self.default_player_position = player[\"default_position\"]\n",
        "\n",
        "        return (player), (platform)\n",
        "\n",
        "    def action(self):\n",
        "        \"\"\"\n",
        "        shape state changes in the game\n",
        "        \"\"\"\n",
        "\n",
        "        if time.time() - self.last_angular_velocity_change_time > self.angular_velocity_change_timeout:\n",
        "            self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "            self.last_angular_velocity_change_time = time.time()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the level to its initial state.\n",
        "        \"\"\"\n",
        "        self.dynamic_body.position = self.default_player_position\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "        self.last_angular_velocity_change_time = time.time()\n",
        "\n",
        "        self.space.reindex_shapes_for_body(self.dynamic_body)\n",
        "        self.space.reindex_shapes_for_body(self.kinematic_body)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5GpDMbr03-O"
      },
      "source": [
        "### Level3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4JZ3BX0p03-O"
      },
      "outputs": [],
      "source": [
        "class Level3:\n",
        "    def __init__(dynamic_body, kinematic_body):\n",
        "        pass\n",
        "\n",
        "    def setup():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj7VhE-K03-O"
      },
      "source": [
        "### --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PTCyvj_J03-O"
      },
      "outputs": [],
      "source": [
        "def get_level(level: int, space):\n",
        "    \"\"\"\n",
        "    Get the level object based on the level number.\n",
        "    \"\"\"\n",
        "    if level == 1:\n",
        "        return Level1(space)\n",
        "    elif level == 2:\n",
        "        return Level2(space)\n",
        "    elif level == 3:\n",
        "        return Level3()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid level number\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8d5fKltI62"
      },
      "source": [
        "## Game class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wiw5Rjks-xw",
        "outputId": "53d916a7-2562-425d-cef6-98aa0104a6e2"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "import pygame\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from typing import Dict, Tuple, Optional\n",
        "from IPython.display import display, Image, clear_output\n",
        "from io import BytesIO\n",
        "# from shapes.circle import Circle\n",
        "# from record import Recorder\n",
        "\n",
        "class BalancingBallGame:\n",
        "    \"\"\"\n",
        "    A physics-based balancing ball game that can run standalone or be used as a Gym environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Game constants\n",
        "\n",
        "\n",
        "    # Visual settings for indie style\n",
        "    BACKGROUND_COLOR = (41, 50, 65)  # Dark blue background\n",
        "    BALL_COLOR = (255, 213, 79)  # Bright yellow ball\n",
        "    PLATFORM_COLOR = (235, 64, 52)  # Red platform\n",
        "    PARTICLE_COLORS = [(252, 186, 3), (252, 127, 3), (252, 3, 3)]  # Fire-like particles\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode: str = \"human\",\n",
        "                 sound_enabled: bool = True,\n",
        "                 difficulty: str = \"medium\",\n",
        "                 window_x: int = 1000,\n",
        "                 window_y: int = 600,\n",
        "                 max_step: int = 30000,\n",
        "                 player_ball_speed: int = 5,\n",
        "                 reward_staying_alive: float = 0.1,\n",
        "                 reward_ball_centered: float = 0.2,\n",
        "                 penalty_falling: float = -10.0,\n",
        "                 level: int = 2,\n",
        "                 fps: int = 120,\n",
        "                 platform_shape: str = \"circle\",\n",
        "                 platform_proportion: int = 0.4,\n",
        "                 capture_per_second: int = None,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialize the balancing ball game.\n",
        "\n",
        "        Args:\n",
        "            render_mode: \"human\" for visible window, \"rgb_array\" for gym env, \"headless\" for no rendering\n",
        "            sound_enabled: Whether to enable sound effects\n",
        "            difficulty: Game difficulty level (\"easy\", \"medium\", \"hard\")\n",
        "            max_step: 1 step = 1/fps, if fps = 120, 1 step = 1/120\n",
        "            reward_staying_alive: float = 0.1,\n",
        "            reward_ball_centered: float = 0.2,\n",
        "            penalty_falling: float = -10.0,\n",
        "            fps: frame per second\n",
        "            platform_proportion: platform_length = window_x * platform_proportion\n",
        "            capture_per_second: save game screen as a image every second, None means no capture\n",
        "        \"\"\"\n",
        "        # Game parameters\n",
        "        self.max_step = max_step\n",
        "        self.reward_staying_alive = reward_staying_alive\n",
        "        self.reward_ball_centered = reward_ball_centered\n",
        "        self.penalty_falling = penalty_falling\n",
        "        self.fps = fps\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "        self.player_ball_speed = player_ball_speed\n",
        "\n",
        "        self.recorder = Recorder(\"game_history_record\")\n",
        "        self.render_mode = render_mode\n",
        "        self.sound_enabled = sound_enabled\n",
        "        self.difficulty = difficulty\n",
        "\n",
        "        platform_length = int(window_x * platform_proportion)\n",
        "        self._get_x_axis_max_reward_rate(platform_length)\n",
        "\n",
        "        # Initialize physics space\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = (0, 1000)\n",
        "        self.space.damping = 0.9\n",
        "\n",
        "        # # Create game bodies\n",
        "        # self.dynamic_body = pymunk.Body()  # Ball body\n",
        "        # self.kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
        "        # self.kinematic_body.position = (self.window_x / 2, (self.window_y / 3) * 2)\n",
        "        # self.default_kinematic_position = self.kinematic_body.position\n",
        "\n",
        "        # # Create game objects\n",
        "        # self._create_player()\n",
        "        # self._create_platform(platform_shape=platform_shape, platform_length=platform_length)\n",
        "        # # self._create_platform(\"rectangle\")\n",
        "\n",
        "        # # Add all objects to space\n",
        "        # self.space.add(self.dynamic_body, self.kinematic_body,\n",
        "        #                self.circle.shape, self.platform)\n",
        "\n",
        "        self.level = get_level(level, self.space)\n",
        "        player, platform = self.level.setup(self.window_x, self.window_y)\n",
        "        self.dynamic_body = player[\"body\"]\n",
        "        self.kinematic_body = platform[\"body\"]\n",
        "        self.ball_radius = player[\"ball_radius\"]\n",
        "        self.platform_length = platform[\"platform_length\"]\n",
        "\n",
        "        # Game state tracking\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Initialize Pygame if needed\n",
        "        if self.render_mode in [\"human\", \"rgb_array\", \"rgb_array_and_human\", \"rgb_array_and_human_in_colab\"]:\n",
        "            self._setup_pygame()\n",
        "        else:\n",
        "            print(\"render_mode is not human or rgb_array, so no pygame setup.\")\n",
        "\n",
        "        # Set difficulty parameters\n",
        "        self._apply_difficulty()\n",
        "        self.capture_per_second = capture_per_second\n",
        "\n",
        "        # Create folders for captures if needed\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \".\"\n",
        "        os.makedirs(os.path.dirname(CURRENT_DIR + \"/capture/\"), exist_ok=True)\n",
        "\n",
        "    def _setup_pygame(self):\n",
        "        \"\"\"Set up PyGame for rendering\"\"\"\n",
        "        pygame.init()\n",
        "        self.frame_count = 0\n",
        "\n",
        "        if self.sound_enabled:\n",
        "            self._load_sounds()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.screen = pygame.display.set_mode((self.window_x, self.window_y))\n",
        "            pygame.display.set_caption(\"Balancing Ball - Indie Game\")\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\": # todo\n",
        "            from pymunk.pygame_util import DrawOptions\n",
        "\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))  # Create hidden surface\n",
        "\n",
        "            # Set up display in Colab\n",
        "            self.draw_options = DrawOptions(self.screen)\n",
        "            html_display = ipd.HTML('''\n",
        "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
        "                </div>\n",
        "            ''')\n",
        "            self.display_handle = display(html_display, display_id='pygame_display')\n",
        "\n",
        "            self.last_update_time = time.time()\n",
        "            self.update_interval = 1.0 / 15  # Update display at 15 FPS to avoid overwhelming Colab\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid render mode. Using headless mode.\")\n",
        "\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Create custom draw options for indie style\n",
        "\n",
        "    def _load_sounds(self):\n",
        "        \"\"\"Load game sound effects\"\"\"\n",
        "        try:\n",
        "            pygame.mixer.init()\n",
        "            self.sound_bounce = pygame.mixer.Sound(\"assets/bounce.wav\") if os.path.exists(\"assets/bounce.wav\") else None\n",
        "            self.sound_fall = pygame.mixer.Sound(\"assets/fall.wav\") if os.path.exists(\"assets/fall.wav\") else None\n",
        "        except Exception:\n",
        "            print(\"Sound loading error\")\n",
        "            self.sound_enabled = False\n",
        "            pass\n",
        "\n",
        "    def _apply_difficulty(self):\n",
        "        \"\"\"Apply difficulty settings to the game\"\"\"\n",
        "        if self.difficulty == \"easy\":\n",
        "            self.max_platform_speed = 1.5\n",
        "            self.ball_elasticity = 0.5\n",
        "        elif self.difficulty == \"medium\":\n",
        "            self.max_platform_speed = 2.5\n",
        "            self.ball_elasticity = 0.7\n",
        "        else:  # hard\n",
        "            self.max_platform_speed = 3.5\n",
        "            self.ball_elasticity = 0.9\n",
        "\n",
        "        # self.circle.shape.elasticity = self.ball_elasticity\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"Reset the game state and return the initial observation\"\"\"\n",
        "        # Reset physics objects\n",
        "        # self.dynamic_body.position = self.default_ball_position\n",
        "        # self.dynamic_body.velocity = (0, 0)\n",
        "        # self.dynamic_body.angular_velocity = 0\n",
        "\n",
        "        # self.kinematic_body.position = self.default_kinematic_position\n",
        "        # self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "        self.level.reset()\n",
        "\n",
        "        # Reset game state\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Return initial observation\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict]:\n",
        "        \"\"\"\n",
        "        Take a step in the game using the given action.\n",
        "\n",
        "        Args:\n",
        "            action: Float value between -1.0 and 1.0 controlling platform rotation\n",
        "\n",
        "        Returns:\n",
        "            observation: Game state observation\n",
        "            reward: Reward for this step\n",
        "            terminated: Whether episode is done\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        # Apply action to platform rotation\n",
        "        if action == 0:\n",
        "            action_value = (0 - self.player_ball_speed)\n",
        "        elif action == 1:\n",
        "            action_value = self.player_ball_speed\n",
        "        elif action == 2:\n",
        "            action_value = 0\n",
        "\n",
        "        self.dynamic_body.angular_velocity += action_value\n",
        "        self.level.action()\n",
        "\n",
        "        # Step the physics simulation\n",
        "        self.space.step(1/self.fps)\n",
        "\n",
        "        # Update particle effects\n",
        "        self._update_particles()\n",
        "\n",
        "        # Check game state\n",
        "        self.steps += 1\n",
        "        terminated = False\n",
        "\n",
        "        # Check if ball falls off screen\n",
        "        ball_x = self.dynamic_body.position[0]\n",
        "        # ball_y = self.dynamic_body.position[1]\n",
        "        if (self.dynamic_body.position[1] > self.kinematic_body.position[1] or\n",
        "            ball_x < 0 or\n",
        "            ball_x > self.window_x or\n",
        "            self.steps >= self.max_step\n",
        "            ):\n",
        "\n",
        "            print(\"Score: \", self.score)\n",
        "            terminated = True\n",
        "            reward = self.penalty_falling if self.steps < self.max_step else 0\n",
        "            self.game_over = True\n",
        "\n",
        "            result = {\n",
        "                \"game_total_duration\": f\"{time.time() - self.start_time:.2f}\",\n",
        "                \"score\": self.score,\n",
        "            }\n",
        "            self.recorder.add_no_limit(result)\n",
        "\n",
        "            if self.sound_enabled and self.sound_fall:\n",
        "                self.sound_fall.play()\n",
        "\n",
        "        step_reward = self._reward_calculator(ball_x)\n",
        "        self.score += step_reward\n",
        "        # print(\"ball_x: \", ball_x, \", self.score: \", self.score)\n",
        "        return self._get_observation(), step_reward, terminated\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Convert game state to observation for RL agent\"\"\"\n",
        "        # update particles and draw them\n",
        "        screen_data = self.render() # 获取数据\n",
        "\n",
        "        if self.capture_per_second is not None and self.frame_count % self.capture_per_second == 0:  # Every second at 60 FPS\n",
        "            pygame.image.save(self.screen, f\"capture/frame_{self.frame_count/60}.png\")\n",
        "\n",
        "        self.frame_count += 1\n",
        "        return screen_data\n",
        "\n",
        "\n",
        "    def _update_particles(self):\n",
        "        \"\"\"Update particle effects for indie visual style\"\"\"\n",
        "        # Create new particles when ball hits platform\n",
        "        if abs(self.dynamic_body.position[1] - (self.kinematic_body.position[1] - 20)) < 5 and abs(self.dynamic_body.velocity[1]) > 100:\n",
        "            for _ in range(5):\n",
        "                self.particles.append({\n",
        "                    'x': self.dynamic_body.position[0],\n",
        "                    'y': self.dynamic_body.position[1] + self.ball_radius,\n",
        "                    'vx': random.uniform(-2, 2),\n",
        "                    'vy': random.uniform(1, 3),\n",
        "                    'life': 30,\n",
        "                    'size': random.uniform(2, 5),\n",
        "                    'color': random.choice(self.PARTICLE_COLORS)\n",
        "                })\n",
        "\n",
        "            if self.sound_enabled and self.sound_bounce:\n",
        "                self.sound_bounce.play()\n",
        "\n",
        "        # Update existing particles\n",
        "        for particle in self.particles[:]:\n",
        "            particle['x'] += particle['vx']\n",
        "            particle['y'] += particle['vy']\n",
        "            particle['life'] -= 1\n",
        "            if particle['life'] <= 0:\n",
        "                self.particles.remove(particle)\n",
        "\n",
        "    def render(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Render the current game state\"\"\"\n",
        "        if self.render_mode == \"headless\":\n",
        "            return None\n",
        "\n",
        "        # Clear screen with background color\n",
        "        self.screen.fill(self.BACKGROUND_COLOR)\n",
        "\n",
        "        # Custom drawing (for indie style)\n",
        "        self._draw_indie_style()\n",
        "\n",
        "\n",
        "        # Update display if in human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            # Draw game information\n",
        "            self._draw_game_info()\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.fps)\n",
        "            return None\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            # Return RGB array for gym environment\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\":\n",
        "            self.space.debug_draw(self.draw_options)\n",
        "            current_time = time.time()\n",
        "            if current_time - self.last_update_time >= self.update_interval:\n",
        "                # Convert Pygame surface to an image that can be displayed in Colab\n",
        "                buffer = BytesIO()\n",
        "                pygame.image.save(self.screen, buffer, 'PNG')\n",
        "                buffer.seek(0)\n",
        "                img_data = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "\n",
        "                # Update the HTML image\n",
        "                self.display_handle.update(ipd.HTML(f'''\n",
        "                    <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                        <img id=\"pygame-img\" src=\"data:image/png;base64,{img_data}\" style=\"width:100%;\">\n",
        "                    </div>\n",
        "                '''))\n",
        "\n",
        "                self.last_update_time = current_time\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def _draw_indie_style(self):\n",
        "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
        "        # # Draw platform with gradient and glow\n",
        "        # platform_points = []\n",
        "        # for v in self.platform.get_vertices():\n",
        "        #     x, y = v.rotated(self.kinematic_body.angle) + self.kinematic_body.position\n",
        "        #     platform_points.append((int(x), int(y)))\n",
        "\n",
        "        # pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
        "        # pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
        "\n",
        "        platform_pos = (int(self.kinematic_body.position[0]), int(self.kinematic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
        "\n",
        "        # Draw rotation direction indicator\n",
        "        self._draw_rotation_indicator(platform_pos, self.platform_length, self.kinematic_body.angular_velocity)\n",
        "\n",
        "        # Draw ball with gradient and glow\n",
        "        ball_pos = (int(self.dynamic_body.position[0]), int(self.dynamic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.BALL_COLOR, ball_pos, self.ball_radius)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
        "\n",
        "        # Draw particles\n",
        "        for particle in self.particles:\n",
        "            alpha = min(255, int(255 * (particle['life'] / 30)))\n",
        "            pygame.draw.circle(\n",
        "                self.screen,\n",
        "                particle['color'],\n",
        "                (int(particle['x']), int(particle['y'])),\n",
        "                int(particle['size'])\n",
        "            )\n",
        "\n",
        "    def _draw_rotation_indicator(self, position, radius, angular_velocity):\n",
        "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
        "        # Only draw the indicator if there's some rotation\n",
        "        if abs(angular_velocity) < 0.1:\n",
        "            return\n",
        "\n",
        "        # Calculate indicator properties based on angular velocity\n",
        "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
        "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
        "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
        "\n",
        "        # Draw arrow indicators along the platform's circumference\n",
        "        start_angle = self.kinematic_body.angle\n",
        "\n",
        "        for i in range(num_arrows):\n",
        "            # Calculate arrow position\n",
        "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
        "\n",
        "            # Calculate arrow start and end points\n",
        "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
        "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
        "\n",
        "            # Determine arrow direction based on angular velocity\n",
        "            if angular_velocity > 0:  # Clockwise\n",
        "                arrow_end_angle = arrow_angle + 0.3\n",
        "            else:  # Counter-clockwise\n",
        "                arrow_end_angle = arrow_angle - 0.3\n",
        "\n",
        "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
        "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
        "\n",
        "            # Draw arrow line\n",
        "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
        "\n",
        "            # Draw arrowhead\n",
        "            arrowhead_size = 7\n",
        "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)\n",
        "\n",
        "    def _draw_game_info(self):\n",
        "        \"\"\"Draw game information on screen\"\"\"\n",
        "        # Create texts\n",
        "        time_text = f\"Time: {time.time() - self.start_time:.1f}\"\n",
        "        score_text = f\"Score: {self.score}\"\n",
        "\n",
        "        # Render texts\n",
        "        time_surface = self.font.render(time_text, True, (255, 255, 255))\n",
        "        score_surface = self.font.render(score_text, True, (255, 255, 255))\n",
        "\n",
        "        # Draw text backgrounds\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (5, 5, time_surface.get_width() + 10, time_surface.get_height() + 5))\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (self.window_x - score_surface.get_width() - 15, 5,\n",
        "                         score_surface.get_width() + 10, score_surface.get_height() + 5))\n",
        "\n",
        "        # Draw texts\n",
        "        self.screen.blit(time_surface, (10, 10))\n",
        "        self.screen.blit(score_surface, (self.window_x - score_surface.get_width() - 10, 10))\n",
        "\n",
        "        # Draw game over screen\n",
        "        if self.game_over:\n",
        "            game_over_text = \"GAME OVER - Press R to restart\"\n",
        "            game_over_surface = self.font.render(game_over_text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw semi-transparent background\n",
        "            overlay = pygame.Surface((self.window_x, self.window_y), pygame.SRCALPHA)\n",
        "            overlay.fill((0, 0, 0, 128))\n",
        "            self.screen.blit(overlay, (0, 0))\n",
        "\n",
        "            # Draw text\n",
        "            self.screen.blit(game_over_surface,\n",
        "                           (self.window_x/2 - game_over_surface.get_width()/2,\n",
        "                            self.window_y/2 - game_over_surface.get_height()/2))\n",
        "\n",
        "    def _get_x_axis_max_reward_rate(self, platform_length):\n",
        "        \"\"\"\n",
        "        ((self.platform_length / 2) - 5) for calculate the distance to the\n",
        "        center of game window coordinates. The closer you are, the higher the reward.\n",
        "\n",
        "        When the ball is to be 10 points away from the center coordinates,\n",
        "        it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        \"\"\"\n",
        "        self.reward_width = (platform_length / 2) - 5\n",
        "        self.x_axis_max_reward_rate = 2 / self.reward_width\n",
        "        print(\"self.x_axis_max_reward_rate: \", self.x_axis_max_reward_rate)\n",
        "\n",
        "    def _reward_calculator(self, ball_x):\n",
        "        # score & reward\n",
        "        step_reward = 1/100\n",
        "\n",
        "        rw = abs(ball_x - self.window_x/2)\n",
        "        if rw < self.reward_width:\n",
        "            x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
        "            step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
        "\n",
        "            if self.steps % 500 == 0:\n",
        "                step_reward += self.steps/100\n",
        "                print(\"check point: \", self.steps/500)\n",
        "\n",
        "            return step_reward\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def _reward_calculator2(self, ball_x):\n",
        "        # Base reward for staying alive\n",
        "        step_reward = 0.1\n",
        "\n",
        "        # Distance from center (normalized)\n",
        "        distance_from_center = abs(ball_x - self.window_x/2) / (self.window_x/2)\n",
        "\n",
        "        # Smooth reward based on position (highest at center)\n",
        "        position_reward = max(0, 1.0 - distance_from_center)\n",
        "\n",
        "        # Apply position reward (with higher weight for better position)\n",
        "        step_reward += position_reward * 0.3\n",
        "\n",
        "        # Small bonus for surviving longer (but not dominant)\n",
        "        survival_bonus = min(0.2, self.steps / 10000)\n",
        "        step_reward += survival_bonus\n",
        "\n",
        "        # Checkpoint bonuses remain meaningful but don't explode\n",
        "        if self.steps % 1000 == 0 and self.steps > 0:\n",
        "            step_reward += 1.0\n",
        "            print(f\"Checkpoint reached: {self.steps}\")\n",
        "\n",
        "        return step_reward\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the game and clean up resources\"\"\"\n",
        "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
        "            pygame.quit()\n",
        "\n",
        "    def run_standalone(self):\n",
        "        \"\"\"Run the game in standalone mode with keyboard controls\"\"\"\n",
        "        if self.render_mode not in [\"human\", \"rgb_array_and_human_in_colab\"]:\n",
        "            raise ValueError(\"Standalone mode requires render_mode='human' or 'rgb_array_and_human_in_colab'\")\n",
        "\n",
        "        running = True\n",
        "        while running:\n",
        "            # Handle events\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.QUIT:\n",
        "                    running = False\n",
        "                elif event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_r and self.game_over:\n",
        "                        self.reset()\n",
        "\n",
        "            # Process keyboard controls\n",
        "            keys = pygame.key.get_pressed()\n",
        "            # In order to fit the model action space, the model can currently only output 0 and 1, so 2 is no action\n",
        "            action = 2\n",
        "            if keys[pygame.K_LEFT]:\n",
        "                action = 0\n",
        "            if keys[pygame.K_RIGHT]:\n",
        "                action = 1\n",
        "\n",
        "            # Take game step\n",
        "            if not self.game_over:\n",
        "                self.step(action)\n",
        "\n",
        "            # Render\n",
        "            self.render()\n",
        "\n",
        "        self.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjobL-nozI81"
      },
      "source": [
        "## GYM env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MBzvHTN1zJu1"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "class BalancingBallEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for the Balancing Ball game\n",
        "    \"\"\"\n",
        "    metadata = {'render_modes': ['human', 'rgb_array', 'rgb_array_and_human_in_colab']}\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode=\"rgb_array\",\n",
        "                 difficulty=\"medium\",\n",
        "                 level=2,\n",
        "                 fps=30,\n",
        "                 obs_type=\"game_screen\",\n",
        "                 image_size=(84, 84),\n",
        "                ):\n",
        "        \"\"\"\n",
        "        render_mode: how to render the environment\n",
        "            Example: \"human\" or \"rgb_array\"\n",
        "        fps: Frames per second,\n",
        "            Example: 30\n",
        "        obs_type: type of observation\n",
        "            Example: \"game_screen\" or \"state_based\"\n",
        "        image_size: Size to resize images to (height, width)\n",
        "            Example: (84, 84) - standard for many RL implementations\n",
        "        \"\"\"\n",
        "\n",
        "        super(BalancingBallEnv, self).__init__()\n",
        "\n",
        "        # Action space: discrete - 0: left, 1: right\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Initialize game\n",
        "        self.window_x = 300\n",
        "        self.window_y = 180\n",
        "        self.platform_shape = \"circle\"\n",
        "        self.platform_proportion = 0.333\n",
        "\n",
        "        # Image preprocessing settings\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.stack_size = 3  # Number of frames to stack\n",
        "        self.observation_stack = []  # Initialize the stack\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.game = BalancingBallGame(\n",
        "            render_mode=render_mode,\n",
        "            sound_enabled=(render_mode == \"human\"),\n",
        "            difficulty=difficulty,\n",
        "            window_x = self.window_x,\n",
        "            window_y = self.window_y,\n",
        "            level = level,\n",
        "            fps = fps,\n",
        "            platform_shape = self.platform_shape,\n",
        "            platform_proportion = self.platform_proportion,\n",
        "        )\n",
        "\n",
        "        if obs_type == \"game_screen\":\n",
        "            channels = 1\n",
        "\n",
        "            # Image observation space with stacked frames\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=0, high=255,\n",
        "                shape=(self.image_size[0], self.image_size[1], channels * self.stack_size),\n",
        "                dtype=np.uint8,\n",
        "            )\n",
        "            self.step = self.step_game_screen\n",
        "            self.reset = self.reset_game_screen\n",
        "        elif obs_type == \"state_based\":\n",
        "            # State-based observation space: [ball_x, ball_y, ball_vx, ball_vy, platform_x, platform_y, platform_angular_velocity]\n",
        "            # Normalize values to be between -1 and 1\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]),\n",
        "                high=np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
        "                dtype=np.float32\n",
        "            )\n",
        "            self.step = self.step_state_based\n",
        "            self.reset = self.reset_state_based\n",
        "        else:\n",
        "            raise ValueError(\"obs_type must be 'game_screen' or 'state_based'\")\n",
        "\n",
        "        # Platform_length /= 2 when for calculate the distance to the\n",
        "        # center of game window coordinates. The closer you are, the higher the reward.\n",
        "        self.platform_reward_length = (self.game.platform_length / 2) - 5\n",
        "\n",
        "        # When the ball is to be 10 points away from the center coordinates,\n",
        "        # it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        self.x_axis_max_reward_rate = 0.5 / self.platform_reward_length\n",
        "\n",
        "    def _preprocess_observation(self, observation):\n",
        "        \"\"\"Process raw game observation for RL training\n",
        "\n",
        "        Args:\n",
        "            observation: RGB image from the game\n",
        "\n",
        "        Returns:\n",
        "            Processed observation ready for RL\n",
        "        \"\"\"\n",
        "        observation = np.transpose(observation, (1, 0, 2))\n",
        "\n",
        "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = np.expand_dims(observation, axis=-1)  # Add channel dimension back\n",
        "\n",
        "        # Resize to target size\n",
        "        if observation.shape[0] != self.image_size[0] or observation.shape[1] != self.image_size[1]:\n",
        "            # For grayscale, temporarily remove the channel dimension for cv2.resize\n",
        "            observation = cv2.resize(\n",
        "                observation.squeeze(-1),\n",
        "                (self.image_size[1], self.image_size[0]),\n",
        "                interpolation=cv2.INTER_AREA\n",
        "            )\n",
        "            observation = np.expand_dims(observation, axis=-1)  # Add channel dimension back\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step_game_screen(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Take step in the game\n",
        "        obs, step_reward, terminated = self.game.step(action)\n",
        "\n",
        "        # Preprocess the observation\n",
        "        obs = self._preprocess_observation(obs)\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "        # Stack the frames\n",
        "        self.observation_stack.append(obs)\n",
        "        if len(self.observation_stack) > self.stack_size:\n",
        "            self.observation_stack.pop(0)  # Remove the oldest frame\n",
        "\n",
        "        # If the stack isn't full yet, pad it with the current frame\n",
        "        while len(self.observation_stack) < self.stack_size:\n",
        "            self.observation_stack.insert(0, obs)  # Pad with current frame at the beginning\n",
        "\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return stacked_obs, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset_game_screen(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        observation = self.game.reset()\n",
        "\n",
        "        # Preprocess the observation\n",
        "        observation = self._preprocess_observation(observation)\n",
        "\n",
        "        # Reset the observation stack\n",
        "        self.observation_stack = []\n",
        "\n",
        "        # Fill the stack with the initial observation\n",
        "        for _ in range(self.stack_size):\n",
        "            self.observation_stack.append(observation)\n",
        "\n",
        "        # Create stacked observation\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        info = {}\n",
        "        return stacked_obs, info\n",
        "\n",
        "    def _get_state_based_observation(self):\n",
        "        \"\"\"Convert game state to state-based observation for RL agent\"\"\"\n",
        "        # Normalize positions by window dimensions\n",
        "        ball_x = self.game.dynamic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
        "        ball_y = self.game.dynamic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
        "\n",
        "        # Normalize velocities (assuming max velocity around 1000)\n",
        "        max_velocity = 1000\n",
        "        ball_vx = np.clip(self.game.dynamic_body.velocity[0] / max_velocity, -1, 1)\n",
        "        ball_vy = np.clip(self.game.dynamic_body.velocity[1] / max_velocity, -1, 1)\n",
        "\n",
        "        # Normalize platform position\n",
        "        platform_x = self.game.kinematic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
        "        platform_y = self.game.kinematic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
        "\n",
        "        # Normalize angular velocity (assuming max around 10)\n",
        "        max_angular_velocity = 10\n",
        "        platform_angular_velocity = np.clip(self.game.kinematic_body.angular_velocity / max_angular_velocity, -1, 1)\n",
        "\n",
        "        return np.array([\n",
        "            ball_x,\n",
        "            ball_y,\n",
        "            ball_vx,\n",
        "            ball_vy,\n",
        "            platform_x,\n",
        "            platform_y,\n",
        "            platform_angular_velocity\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step_state_based(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Take step in the game\n",
        "        _, step_reward, terminated = self.game.step(action)\n",
        "\n",
        "        # Get state-based observation\n",
        "        observation = self._get_state_based_observation()\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return observation, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset_state_based(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        self.game.reset()\n",
        "        observation = self._get_state_based_observation()\n",
        "\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment\"\"\"\n",
        "        return self.game.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        self.game.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgvyLDYKPlb"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ebr3cvEiKPlc"
      },
      "outputs": [],
      "source": [
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "def run_standalone_game(render_mode=\"human\", difficulty=\"medium\", capture_per_second=3, window_x=1000, window_y=600, level=2):\n",
        "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "\n",
        "    platform_shape = \"circle\"\n",
        "    platform_proportion = 0.333\n",
        "\n",
        "    game = BalancingBallGame(\n",
        "        render_mode = render_mode,\n",
        "        difficulty = difficulty,\n",
        "        window_x = window_x,\n",
        "        window_y = window_y,\n",
        "        platform_shape = platform_shape,\n",
        "        platform_proportion = platform_proportion,\n",
        "        level = level,\n",
        "        fps = 30,\n",
        "        capture_per_second = 3,\n",
        "    )\n",
        "\n",
        "    game.run_standalone()\n",
        "\n",
        "def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
        "    \"\"\"Test the OpenAI Gym environment\"\"\"\n",
        "    import time\n",
        "    # from gym_env import BalancingBallEnv\n",
        "\n",
        "    fps = 30\n",
        "    env = BalancingBallEnv(\n",
        "        render_mode=\"rgb_array_and_human_in_colab\",\n",
        "        difficulty=difficulty,\n",
        "        fps=fps,\n",
        "    )\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample a random action (for testing only)\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            # Render\n",
        "            env.render()\n",
        "\n",
        "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy  # MLP policy instead of CNN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "class Train:\n",
        "    def __init__(self,\n",
        "                 learning_rate=0.0003,\n",
        "                 n_steps=2048,\n",
        "                 batch_size=64,\n",
        "                 n_epochs=10,\n",
        "                 gamma=0.99,\n",
        "                 gae_lambda=0.95,\n",
        "                 ent_coef=0.01,\n",
        "                 vf_coef=0.5,\n",
        "                 max_grad_norm=0.5,\n",
        "                 policy_kwargs=None,\n",
        "                 n_envs=4,\n",
        "                 difficulty=\"medium\",\n",
        "                 level=2,\n",
        "                 load_model=None,\n",
        "                 log_dir=\"./logs/\",\n",
        "                 model_dir=\"./models/\",\n",
        "                 obs_type=\"game_screen\",\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        self.log_dir = log_dir\n",
        "        self.model_dir = model_dir\n",
        "        self.n_envs = n_envs\n",
        "        self.obs_type = obs_type\n",
        "        self.level = level\n",
        "\n",
        "        # Setup environments\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array_and_human_in_colab\", difficulty=difficulty, obs_type=obs_type),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        self.env = env  # No need for VecTransposeImage with state-based observations\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array_and_human_in_colab\", difficulty=difficulty, obs_type=obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = eval_env  # No need for VecTransposeImage\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=log_dir,\n",
        "            )\n",
        "        else:\n",
        "            hyper_param = {\n",
        "                'learning_rate': 0.0003,\n",
        "                'gamma': 0.99,\n",
        "                'clip_range': 0.2,\n",
        "                'gae_lambda': 0.95,\n",
        "                'ent_coef': 0.01,\n",
        "                'vf_coef': 0.5,\n",
        "            }\n",
        "            # hyper_param = {\n",
        "            #     'learning_rate': 9.610262132782771e-05,\n",
        "            #     'gamma': 0.9354902732764846,\n",
        "            #     'clip_range': 0.10781723398003445,\n",
        "            #     'gae_lambda': 0.7317659739640645,\n",
        "            #     'ent_coef': 0.04688706780862235,\n",
        "            #     'vf_coef': 0.9117913569235723,\n",
        "            # }\n",
        "            policy_kwargs = {\n",
        "                \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "                \"net_arch\": [256, 256],  # MLP architecture\n",
        "            }\n",
        "\n",
        "            policy = ActorCriticCnnPolicy if obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "            print(\"obs type: \", self.obs_type)\n",
        "            print(\"policy: \", policy)\n",
        "            # MLP policy for state-based observations, CNN policy for image-based observations\n",
        "            self.model = PPO(\n",
        "                policy=policy,\n",
        "                env=self.env,\n",
        "                learning_rate=hyper_param[\"learning_rate\"],\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=hyper_param[\"gamma\"],\n",
        "                clip_range=hyper_param[\"clip_range\"],\n",
        "                gae_lambda=hyper_param[\"gae_lambda\"],\n",
        "                ent_coef=hyper_param[\"ent_coef\"],\n",
        "                vf_coef=hyper_param[\"vf_coef\"],\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=log_dir,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=1,\n",
        "            )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty, level=self.level, obs_type=obs_type)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self,\n",
        "                  total_timesteps=1000000,\n",
        "                  save_freq=10000,\n",
        "                  eval_freq=10000,\n",
        "                  eval_episodes=5,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball_\" + str(self.obs_type),\n",
        "        )\n",
        "\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=eval_freq // self.n_envs,\n",
        "            n_eval_episodes=eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=[checkpoint_callback, eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final_\" + str(self.obs_type))\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10, difficulty=\"medium\"):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         difficulty=args.difficulty,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#             difficulty=args.difficulty\n",
        "#         )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpEVcsjfs45Q"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "B4gwCvLVs45Q"
      },
      "outputs": [],
      "source": [
        "class Optuna_optimize:\n",
        "    def __init__(self, obs_type=\"game_screen\"):\n",
        "        self.obs_type = obs_type\n",
        "        self.env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=self.obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty, obs_type=obs_type)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def optuna_parameter_tuning(self, n_trials):\n",
        "        print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "        pruner = optuna.pruners.HyperbandPruner(\n",
        "            min_resource=100,        # 最小资源量\n",
        "            max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "            reduction_factor=3     # 折减因子 (eta)\n",
        "        )\n",
        "\n",
        "        # 建立 study 物件，並指定剪枝器\n",
        "        study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "        # 執行優化\n",
        "        try:\n",
        "            study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "            # 分析結果\n",
        "            print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "            print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "            import pandas as pd\n",
        "            df = study.trials_dataframe()\n",
        "            print(df.head())\n",
        "        finally:\n",
        "            self.env.close()\n",
        "            del self.env\n",
        "\n",
        "\n",
        "    def objective(self, trial):\n",
        "        import gc\n",
        "\n",
        "        # 1. 建議超參數\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 0.9, 0.999)\n",
        "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "        gae_lambda = trial.suggest_float('gae_lambda', 0.5, 2)\n",
        "        ent_coef = trial.suggest_float('ent_coef', 0.005, 0.05)\n",
        "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "        features_dim = trial.suggest_categorical('features_dim', [32, 64, 128, 256, 512])\n",
        "        policy_kwargs = {\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "            \"net_arch\": [256, 256],  # MLP architecture\n",
        "        }\n",
        "\n",
        "        n_steps=2048\n",
        "        batch_size=64\n",
        "        n_epochs=10\n",
        "        # gamma=0.99\n",
        "        # gae_lambda=0.95\n",
        "        # ent_coef=0.01\n",
        "        # vf_coef=0.5\n",
        "        max_grad_norm=0.5\n",
        "\n",
        "        # 2. 建立環境\n",
        "\n",
        "\n",
        "        policy = ActorCriticCnnPolicy if self.obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "        print(\"obs type: \", self.obs_type)\n",
        "        print(\"policy: \", policy)\n",
        "        # 3. 建立模型\n",
        "        model = PPO(\n",
        "                policy=policy,  # MLP policy for state-based observations\n",
        "                env=self.env,\n",
        "                learning_rate=learning_rate,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=gamma,\n",
        "                clip_range=clip_range,\n",
        "                gae_lambda=gae_lambda,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=None,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=0,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # 4. 訓練模型\n",
        "            model.learn(total_timesteps=30000)\n",
        "            # 5. 評估模型\n",
        "            mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "        finally:\n",
        "            # Always cleanup\n",
        "            del model\n",
        "            gc.collect()\n",
        "\n",
        "            if TPU_AVAILABLE:\n",
        "                import torch_xla.core.xla_model as xm\n",
        "                xm.mark_step()\n",
        "\n",
        "        return mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Ndr9hGp2CZzF",
        "outputId": "0afea83f-4521-4ba0-d83c-166568668385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The json memory file does not exist. Creating new file.\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
              "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
              "                </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <div id=\"pygame-output\" style=\"width:100%;\">\n",
              "                        <img id=\"pygame-img\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAC0CAIAAAChXYa4AAAFNElEQVR4nO3dPY8VVQDH4YFAYoREC0NLTGgwsYGIpeA3oDKxwwITKxs7Sqhs6Ix8ABtLrdfamC3dysTQagMBoomBWGyyuSz3ZWbuOfOfl+epIAu755yc35yZWcieuXrtVgPknE0PAJZOhBAmQggTIYSJEMJECGEihDARQpgIIUyEECZCCBMhhIkQwkQIYSKEMBFCmAghTIQQJkIIEyGEiRDCRAhhIoQwEUKYCCFMhBAmQggTIYSJEMJECGEihDARQpgIIUyEECZCCBMhhIkQwkQIYSKEMBFCmAghTIQQJkIIEyGEiRDCRAhhIoQwEUKYCCFMhBAmQggTIYSJEMJECGEihDARQpgIIUyEECbCIRwdHhwdHqRHwUiduXrtVnoMc3bS3kff/9E0zW9fXvng+qe9P9ure5db/smz9x/3/ioMTIQVHRd4nN+qF4/utvnr7ZNrQ5ajdS49AF5TNrxNn1mQo+IkrOjo8ODDT243TfPW59+e+tCpw7Bee9upcQychHUd5/fvD9+s/nZVKr/Vry7FLCdhXUeHB2ufCbPtbaLGCBFWd+Huo5Nf//TznZtfXAkOpg0pDsz3Cat78eju8RPgs0sP/nr73Fe//p0e0Q6v7l0e50E9VyIcyLNLD5qm+eX9izf/fJ4eSys6HIwXM9Wt7ubvPn7vs9+fBAfTidc2w/BMWNc8zhMdViXCWuaR3yopVuKZsIr5FdjMdFJjIMLyZrxZZzy1ILejJS1nj7o1LchJWMxyCmwWNtnaRFjGAjflAqdciQgLWOx2XOzEyxLhvha+ERc+/SJEuBdbsLEIexNhfzbfCUuxDxH2ZNudYkF6EyGEibAPV/21LEs/IuzMVtvC4vQgwm5ssp0sUVcihDARduAa35KF6kSEbdlYnViu9kQIYSJsxXW9B4vWkgh3s5l6s3RtiBDCRLiDa/meLOBOIoQwEW7jKl6EZdxOhBAmQggT4UZuogqymFuIEMJEuJ4rd3GWdBMRQpgIIUyEa7hxqsTCriVCCBMhhIkQwkR4mueWqizvm0QIYSKEMBFCmAghTIQQJsLXeHc3AIt8igghTIQQJkIIEyGEiRDCRAhhIpy88w+fnH/4JD0K+juXHgBlrHb439fvBkdCVyKcvOPkViMU5LSIcCZWYxPktIhwhgQ5LSKcOUGOnwgXRJDjdObqtVvpMYzLAv+N/6bvcFQK8uz9xzU+7XQ5Cdl9QjoeqxIhr1kbpPvVqkTIRie9eYCsSoTs5o1OVSKkG0EW5+3oGgt8Qbq/lq9YvRp9k5OQMpyQvYmQ8rYFefGdl8+fJgY1XiKkrk1BcsJ/6l3Dc0slL58/dQy+SYQQJkIIE+F67kiLs6SbiBDCRLiRK3dBFnMLEUKYCCFMhNu4iSrCMm4nQggT4Q6u4nuygDuJEMJEuJtreW8Xbt9ID2ECRNiKDnu4cPvGP9d/TI9iAkQIYSJsy2HYiWOwPRF2oMOWFNiJCClPgZ2IsBuH4U6WqCsRdmaTbWFxehBhH7baWpalHxFCmAh7ctU/xYL0JsL+bLsTlmIfItyLzddYhL2JcF8L34ILn34RIixgsRtxsRMvS4RlLHA7LnDKlYiwmEVtykVNtjY/JLS8ef+MUfkV5yQsb8bbdMZTCxJhFbPcrLOc1Bi4Ha1rHrem8qvKSVjXDLbvDKYwck7CgUzxSJTfMJyEA5nchp7cgKfLSTi08R+J8huYCDPGmaL8IkQYNoYatZclwlFIpSi/MRDhuAxTo/ZGRYTjVTZI4Y2WCKekfZaSm5D/Ad+2epheUJoxAAAAAElFTkSuQmCC\" style=\"width:100%;\">\n",
              "                    </div>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "obs type:  game_screen\n",
            "policy:  <class 'stable_baselines3.common.policies.ActorCriticCnnPolicy'>\n",
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Starting training...\n",
            "Logging to ./logs/PPO_2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000002530BE5B610> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002532CCFACD0>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score:  13.104452732905127\n",
            "Score:  18.166712733706902\n",
            "Score:  6.842190326301453\n",
            "Score:  11.30030661851889\n",
            "Score:  9.127875585841176\n",
            "Score:  13.604410677608081\n",
            "Score:  8.743324659461782\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Run training with memory-optimized settings\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Use fewer total timesteps for TPU to avoid memory issues\u001b[39;00m\n\u001b[32m     50\u001b[39m total_timesteps = \u001b[32m250000\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m model = \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fewer eval episodes on TPU\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\n\u001b[32m     57\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# # Force memory cleanup after training\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# if TPU_AVAILABLE:\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m#     del model\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m#     gc.collect()\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m#     xm.mark_step()\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mTrain.train_ppo\u001b[39m\u001b[34m(self, total_timesteps, save_freq, eval_freq, eval_episodes)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;28mself\u001b[39m.model.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/ppo_balancing_ball_final_\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.obs_type))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[32m    201\u001b[39m     obs_tensor = obs_as_tensor(\u001b[38;5;28mself\u001b[39m._last_obs, \u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     actions, values, log_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m actions = actions.cpu().numpy()\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:654\u001b[39m, in \u001b[36mActorCriticPolicy.forward\u001b[39m\u001b[34m(self, obs, deterministic)\u001b[39m\n\u001b[32m    652\u001b[39m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[32m    653\u001b[39m values = \u001b[38;5;28mself\u001b[39m.value_net(latent_vf)\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m distribution = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m actions = distribution.get_actions(deterministic=deterministic)\n\u001b[32m    656\u001b[39m log_prob = distribution.log_prob(actions)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:697\u001b[39m, in \u001b[36mActorCriticPolicy._get_action_dist_from_latent\u001b[39m\u001b[34m(self, latent_pi)\u001b[39m\n\u001b[32m    694\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.action_dist.proba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m.log_std)\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_dist, CategoricalDistribution):\n\u001b[32m    696\u001b[39m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_dist, MultiCategoricalDistribution):\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.action_dist.proba_distribution(action_logits=mean_actions)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:288\u001b[39m, in \u001b[36mCategoricalDistribution.proba_distribution\u001b[39m\u001b[34m(self, action_logits)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfCategoricalDistribution, action_logits: th.Tensor) -> SelfCategoricalDistribution:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28mself\u001b[39m.distribution = \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\AI\\Balancing_Ball_for_RL\\.conda\\Lib\\site-packages\\torch\\distributions\\categorical.py:66\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28mself\u001b[39m.logits = logits - \u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m._param = \u001b[38;5;28mself\u001b[39m.probs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logits\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# Memory-optimized training setup\n",
        "def get_tpu_memory_info():\n",
        "    \"\"\"Get memory information from TPU device if available\"\"\"\n",
        "    # if TPU_AVAILABLE:\n",
        "    #     try:\n",
        "    #         # This is just for diagnostic purposes\n",
        "    #         import subprocess\n",
        "    #         result = subprocess.run(['python3', '-c', 'import torch_xla; print(torch_xla._XLAC._xla_get_memory_info(torch_xla._XLAC._xla_get_default_device()))'],\n",
        "    #                                stdout=subprocess.PIPE, text=True)\n",
        "    #         print(f\"TPU Memory Info: {result.stdout}\")\n",
        "    #     except:\n",
        "    #         print(\"Could not get detailed TPU memory info\")\n",
        "    pass\n",
        "\n",
        "# Display memory information\n",
        "get_tpu_memory_info()\n",
        "\n",
        "n_envs = 1\n",
        "batch_size = 64\n",
        "n_steps = 2048\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = False\n",
        "\n",
        "if do_optimization:\n",
        "    optuna_optimizer = Optuna_optimize(obs_type=\"game_screen\")\n",
        "    # # Force TPU memory cleanup before starting\n",
        "    # if TPU_AVAILABLE:\n",
        "    #     gc.collect()\n",
        "    #     xm.mark_step()\n",
        "\n",
        "    n_trials = 10\n",
        "    best_trial = optuna_optimizer.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    print(f\"best_trial found: {best_trial}\")\n",
        "else:\n",
        "    # Create trainer\n",
        "    training = Train(\n",
        "        n_steps=n_steps,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=n_envs,\n",
        "        level=1,\n",
        "        load_model=None,  # Start fresh with state-based env\n",
        "        obs_type='game_screen',\n",
        "    )\n",
        "    # Run training with memory-optimized settings\n",
        "    # Use fewer total timesteps for TPU to avoid memory issues\n",
        "    total_timesteps = 250000\n",
        "\n",
        "    model = training.train_ppo(\n",
        "        total_timesteps=total_timesteps,\n",
        "        eval_episodes=3,  # Fewer eval episodes on TPU\n",
        "        save_freq=5000,\n",
        "        eval_freq=5000\n",
        "    )\n",
        "\n",
        "    # # Force memory cleanup after training\n",
        "    # if TPU_AVAILABLE:\n",
        "    #     del model\n",
        "    #     gc.collect()\n",
        "    #     xm.mark_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "outputs": [],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "outputs": [],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        n_steps=1024,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=1  # Use 1 env for evaluation\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "        difficulty=\"medium\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4h3jBARKPld"
      },
      "outputs": [],
      "source": [
        "# run_standalone_game(render_mode=\"rgb_array_and_human_in_colab\", difficulty=\"medium\", window_x=1000, window_y=600)\n",
        "test_gym_env(difficulty=\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCFm3AoWs45R"
      },
      "outputs": [],
      "source": [
        "# Example of creating the environment with grayscale images\n",
        "env = BalancingBallEnv(\n",
        "    render_mode=\"rgb_array\",\n",
        "    difficulty=\"medium\",\n",
        "    fps=30,\n",
        "    obs_type=\"game_screen\",\n",
        "    image_size=(84, 84)  # Standard size for many RL frameworks\n",
        ")\n",
        "\n",
        "# Reset environment to get initial observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Print observation shape to verify\n",
        "print(f\"Observation shape: {obs.shape}\")  # Should be (84, 84, 3) for grayscale with 3 stacked frames\n",
        "\n",
        "# Display a sample observation (first frame only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(obs[:,:,0], cmap='gray')\n",
        "plt.title(\"Grayscale Observation (First Frame)\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
