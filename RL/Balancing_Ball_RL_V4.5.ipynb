{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V4.2 Update: in readme.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "d07254d4-4661-4d9e-d62f-661fa286a860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch XLA not found, will attempt to install\n"
          ]
        }
      ],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will attempt to install\")\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install pygame-ce pymunk stable-baselines3 stable-baselines3[extra] shimmy>=2.0 optuna\n",
        "!pip install -q cloud-tpu-client\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2nILk-pwsMG"
      },
      "outputs": [],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bbqyvjZ1PZu"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhEqO-xFu4AI"
      },
      "source": [
        "## Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJwfQb_Yuz1I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "class Recorder:\n",
        "\n",
        "    def __init__(self, task: str = \"game_history_record\"):\n",
        "        \"\"\"\n",
        "        tasks:\n",
        "        1. game_history_record\n",
        "        2. temp_memory\n",
        "        \"\"\"\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \"\"\n",
        "        if task == \"game_history_record\":\n",
        "            collection_name = self.get_newest_record_name()\n",
        "            self.json_file_path = CURRENT_DIR + \"./game_history/\" + collection_name + \".json\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)\n",
        "\n",
        "        if os.path.exists(self.json_file_path):\n",
        "            print(\"Loading the json memory file\")\n",
        "            self.memory = self.load(self.json_file_path)\n",
        "        else:\n",
        "            print(\"The json memory file does not exist. Creating new file.\")\n",
        "            self.memory = {\"game_records\": []}  # Direct dictionary instead of json.loads\n",
        "            with open(self.json_file_path, \"w\") as f:\n",
        "                json.dump(self.memory, f)\n",
        "\n",
        "    def get(self):\n",
        "        print(\"Getting the json memory\")\n",
        "        return self.memory\n",
        "\n",
        "    def add_no_limit(self, data: float, ):\n",
        "        \"\"\"\n",
        "        Add a records.\n",
        "\n",
        "        Args:\n",
        "            role: The role of the sender (e.g., 'user', 'assistant')\n",
        "            message: The message content\n",
        "        \"\"\"\n",
        "        self.memory[\"game_records\"].append({\n",
        "            \"game_total_duration\": data,\n",
        "            \"timestamp\": str(datetime.datetime.now())\n",
        "        })\n",
        "\n",
        "        self.save(self.json_file_path)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump(self.memory, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving memory to {file_path}: {e}\")\n",
        "\n",
        "    def load(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading memory from {file_path}: {e}\")\n",
        "            return {\"game_records\": []}\n",
        "\n",
        "    def get_newest_record_name(self) -> str:\n",
        "        \"\"\"\n",
        "        傳回最新的對話歷史資料和集的名稱 (game_YYYY_MM)\n",
        "            - 例如: \"game_2022-01\"\n",
        "        \"\"\"\n",
        "\n",
        "        this_month = datetime.datetime.now().strftime(\"%Y-%m\")\n",
        "        return \"record_\" + this_month"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnA8wZtosmeN"
      },
      "source": [
        "## Shapes & Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wMhHMWCsmVD"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Shape:\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape: Optional[pymunk.Shape] = None,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a physical shape with associated body.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the body\n",
        "            velocity: Initial velocity (vx, vy) of the body\n",
        "            body: The pymunk Body to attach to this shape\n",
        "            shape: The pymunk Shape for collision detection\n",
        "        \"\"\"\n",
        "\n",
        "        self.body = body\n",
        "        self.default_position = position\n",
        "        self.default_velocity = velocity\n",
        "        self.body.position = position\n",
        "        self.body.velocity = velocity\n",
        "        self.default_angular_velocity = 0\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the body to its default position, velocity and angular velocity.\"\"\"\n",
        "        self.body.position = self.default_position\n",
        "        self.body.velocity = self.default_velocity\n",
        "        self.body.angular_velocity = self.default_angular_velocity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfw5tBxBswyF"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "\n",
        "# from shapes.shape import Shape\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Circle(Shape):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape_radio: float = 20,\n",
        "                shape_mass: float = 1,\n",
        "                shape_friction: float = 0.1,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a circular physics object.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the circle\n",
        "            velocity: Initial velocity (vx, vy) of the circle\n",
        "            body: The pymunk Body to attach this circle to\n",
        "            shape_radio: Radius of the circle in pixels\n",
        "            shape_mass: Mass of the circle\n",
        "            shape_friction: Friction coefficient for the circle\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(position, velocity, body)\n",
        "        self.shape_radio = shape_radio\n",
        "        self.shape = pymunk.Circle(self.body, shape_radio)\n",
        "        self.shape.mass = shape_mass\n",
        "        self.shape.friction = shape_friction\n",
        "        self.shape.elasticity = 0.8  # Add some bounce to make the simulation more interesting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8d5fKltI62"
      },
      "source": [
        "## Game class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wiw5Rjks-xw"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "import pygame\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import display, Image, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import IPython.display as ipd\n",
        "# from shapes.circle import Circle\n",
        "# from record import Recorder\n",
        "\n",
        "class BalancingBallGame:\n",
        "    \"\"\"\n",
        "    A physics-based balancing ball game that can run standalone or be used as a Gym environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Game constants\n",
        "\n",
        "\n",
        "    # Visual settings for indie style\n",
        "    BACKGROUND_COLOR = (41, 50, 65)  # Dark blue background\n",
        "    BALL_COLOR = (255, 213, 79)  # Bright yellow ball\n",
        "    PLATFORM_COLOR = (235, 64, 52)  # Red platform\n",
        "    PARTICLE_COLORS = [(252, 186, 3), (252, 127, 3), (252, 3, 3)]  # Fire-like particles\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode: str = \"human\",\n",
        "                 sound_enabled: bool = True,\n",
        "                 difficulty: str = \"medium\",\n",
        "                 window_x: int = 1000,\n",
        "                 window_y: int = 600,\n",
        "                 max_step: int = 30000,\n",
        "                 player_ball_speed: int = 5,\n",
        "                 reward_staying_alive: float = 0.1,\n",
        "                 reward_ball_centered: float = 0.2,\n",
        "                 penalty_falling: float = -10.0,\n",
        "                 fps: int = 120,\n",
        "                 platform_shape: str = \"circle\",\n",
        "                 platform_proportion: int = 0.4,\n",
        "                 capture_per_second: int = None,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialize the balancing ball game.\n",
        "\n",
        "        Args:\n",
        "            render_mode: \"human\" for visible window, \"rgb_array\" for gym env, \"headless\" for no rendering\n",
        "            sound_enabled: Whether to enable sound effects\n",
        "            difficulty: Game difficulty level (\"easy\", \"medium\", \"hard\")\n",
        "            max_step: 1 step = 1/fps, if fps = 120, 1 step = 1/120\n",
        "            reward_staying_alive: float = 0.1,\n",
        "            reward_ball_centered: float = 0.2,\n",
        "            penalty_falling: float = -10.0,\n",
        "            fps: frame per second\n",
        "            platform_proportion: platform_length = window_x * platform_proportion\n",
        "            capture_per_second: save game screen as a image every second, None means no capture\n",
        "        \"\"\"\n",
        "        # Game parameters\n",
        "        self.max_step = max_step\n",
        "        self.reward_staying_alive = reward_staying_alive\n",
        "        self.reward_ball_centered = reward_ball_centered\n",
        "        self.penalty_falling = penalty_falling\n",
        "        self.fps = fps\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "        self.player_ball_speed = player_ball_speed\n",
        "\n",
        "        self.recorder = Recorder(\"game_history_record\")\n",
        "        self.render_mode = render_mode\n",
        "        self.sound_enabled = sound_enabled\n",
        "        self.difficulty = difficulty\n",
        "\n",
        "        platform_length = int(window_x * platform_proportion)\n",
        "        self._get_x_axis_max_reward_rate(platform_length)\n",
        "\n",
        "        # Initialize physics space\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = (0, 1000)\n",
        "        self.space.damping = 0.9\n",
        "\n",
        "        # Create game bodies\n",
        "        self.dynamic_body = pymunk.Body()  # Ball body\n",
        "        self.kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
        "        self.kinematic_body.position = (self.window_x / 2, (self.window_y / 3) * 2)\n",
        "        self.default_kinematic_position = self.kinematic_body.position\n",
        "\n",
        "        # Create game objects\n",
        "        self._create_ball()\n",
        "        self._create_platform(platform_shape=platform_shape, platform_length=platform_length)\n",
        "        # self._create_platform(\"rectangle\")\n",
        "\n",
        "        # Add all objects to space\n",
        "        self.space.add(self.dynamic_body, self.kinematic_body,\n",
        "                       self.circle.shape, self.platform)\n",
        "\n",
        "        # Game state tracking\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Initialize Pygame if needed\n",
        "        if self.render_mode in [\"human\", \"rgb_array\", \"rgb_array_and_human\", \"rgb_array_and_human_in_colab\"]:\n",
        "            self._setup_pygame()\n",
        "        else:\n",
        "            print(\"render_mode is not human or rgb_array, so no pygame setup.\")\n",
        "\n",
        "        # Set difficulty parameters\n",
        "        self._apply_difficulty()\n",
        "        self.capture_per_second = capture_per_second\n",
        "\n",
        "        # Create folders for captures if needed\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \".\"\n",
        "        os.makedirs(os.path.dirname(CURRENT_DIR + \"/capture/\"), exist_ok=True)\n",
        "\n",
        "    def _setup_pygame(self):\n",
        "        \"\"\"Set up PyGame for rendering\"\"\"\n",
        "        pygame.init()\n",
        "        self.frame_count = 0\n",
        "\n",
        "        if self.sound_enabled:\n",
        "            self._load_sounds()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.screen = pygame.display.set_mode((self.window_x, self.window_y))\n",
        "            pygame.display.set_caption(\"Balancing Ball - Indie Game\")\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\": # todo\n",
        "            from pymunk.pygame_util import DrawOptions\n",
        "\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))  # Create hidden surface\n",
        "\n",
        "            # Set up display in Colab\n",
        "            self.draw_options = DrawOptions(self.screen)\n",
        "            html_display = ipd.HTML('''\n",
        "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
        "                </div>\n",
        "            ''')\n",
        "            self.display_handle = display(html_display, display_id='pygame_display')\n",
        "\n",
        "            self.last_update_time = time.time()\n",
        "            self.update_interval = 1.0 / 15  # Update display at 15 FPS to avoid overwhelming Colab\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid render mode. Using headless mode.\")\n",
        "\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Create custom draw options for indie style\n",
        "\n",
        "    def _load_sounds(self):\n",
        "        \"\"\"Load game sound effects\"\"\"\n",
        "        try:\n",
        "            pygame.mixer.init()\n",
        "            self.sound_bounce = pygame.mixer.Sound(\"assets/bounce.wav\") if os.path.exists(\"assets/bounce.wav\") else None\n",
        "            self.sound_fall = pygame.mixer.Sound(\"assets/fall.wav\") if os.path.exists(\"assets/fall.wav\") else None\n",
        "        except Exception:\n",
        "            print(\"Sound loading error\")\n",
        "            self.sound_enabled = False\n",
        "            pass\n",
        "\n",
        "    def _create_ball(self):\n",
        "        \"\"\"Create the ball with physics properties\"\"\"\n",
        "        self.ball_radius = int(self.window_x / 67)\n",
        "        self.circle = Circle(\n",
        "            position=(self.window_x / 2, self.window_y / 3),\n",
        "            velocity=(0, 0),\n",
        "            body=self.dynamic_body,\n",
        "            shape_radio=self.ball_radius,\n",
        "            shape_friction=100,\n",
        "        )\n",
        "        # Store initial values for reset\n",
        "        self.default_ball_position = self.dynamic_body.position\n",
        "\n",
        "    def _create_platform(self,\n",
        "                         platform_shape: str = \"circle\",\n",
        "                         platform_length: int = 200\n",
        "                        ):\n",
        "        \"\"\"\n",
        "        Create the platform with physics properties\n",
        "        platform_shape: circle, rectangle\n",
        "        platform_length: Length of a rectangle or Diameter of a circle\n",
        "        \"\"\"\n",
        "        if platform_shape == \"circle\":\n",
        "            self.platform_length = platform_length / 2 # radius\n",
        "            self.platform = pymunk.Circle(self.kinematic_body, self.platform_length)\n",
        "            self.platform.mass = 1  # 质量对 Kinematic 物体无意义，但需要避免除以零错误\n",
        "            self.platform.friction = 0.7\n",
        "        elif platform_shape == \"rectangle\":\n",
        "            self.platform_length = platform_length\n",
        "            vs = [(-self.platform_length/2, -10),\n",
        "                (self.platform_length/2, -10),\n",
        "                (self.platform_length/2, 10),\n",
        "                (-self.platform_length/2, 10)]\n",
        "\n",
        "            self.platform = pymunk.Poly(self.kinematic_body, vs)\n",
        "        self.platform.friction = 0.7\n",
        "        self.platform_rotation = 0\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "    def _apply_difficulty(self):\n",
        "        \"\"\"Apply difficulty settings to the game\"\"\"\n",
        "        if self.difficulty == \"easy\":\n",
        "            self.max_platform_speed = 1.5\n",
        "            self.ball_elasticity = 0.5\n",
        "        elif self.difficulty == \"medium\":\n",
        "            self.max_platform_speed = 2.5\n",
        "            self.ball_elasticity = 0.7\n",
        "        else:  # hard\n",
        "            self.max_platform_speed = 3.5\n",
        "            self.ball_elasticity = 0.9\n",
        "\n",
        "        self.circle.shape.elasticity = self.ball_elasticity\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"Reset the game state and return the initial observation\"\"\"\n",
        "        # Reset physics objects\n",
        "        self.dynamic_body.position = self.default_ball_position\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "\n",
        "        self.kinematic_body.position = self.default_kinematic_position\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        # Reset game state\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Return initial observation\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict]:\n",
        "        \"\"\"\n",
        "        Take a step in the game using the given action.\n",
        "\n",
        "        Args:\n",
        "            action: Float value between -1.0 and 1.0 controlling platform rotation\n",
        "\n",
        "        Returns:\n",
        "            observation: Game state observation\n",
        "            reward: Reward for this step\n",
        "            terminated: Whether episode is done\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        # Apply action to platform rotation\n",
        "        action_value = (0 - self.player_ball_speed) if action == 0 else self.player_ball_speed\n",
        "\n",
        "        self.dynamic_body.angular_velocity += action_value\n",
        "\n",
        "        # Step the physics simulation\n",
        "        self.space.step(1/self.fps)\n",
        "\n",
        "        # Update particle effects\n",
        "        self._update_particles()\n",
        "\n",
        "        # Check game state\n",
        "        self.steps += 1\n",
        "        terminated = False\n",
        "        reward = self.reward_staying_alive\n",
        "\n",
        "        # Calculate reward for keeping ball centered on platform\n",
        "        ball_x = self.dynamic_body.position[0]\n",
        "\n",
        "        # Check if ball falls off screen\n",
        "        if (self.dynamic_body.position[1] > self.kinematic_body.position[1] or\n",
        "            self.dynamic_body.position[0] < 0 or\n",
        "            self.dynamic_body.position[0] > self.window_x or\n",
        "            self.steps >= self.max_step\n",
        "            ):\n",
        "\n",
        "            print(\"Score: \", self.score)\n",
        "            terminated = True\n",
        "            reward = self.penalty_falling if self.steps < self.max_step else 0\n",
        "            self.game_over = True\n",
        "\n",
        "            result = {\n",
        "                \"game_total_duration\": f\"{time.time() - self.start_time:.2f}\",\n",
        "                \"score\": self.score,\n",
        "            }\n",
        "            self.recorder.add_no_limit(result)\n",
        "\n",
        "            if self.sound_enabled and self.sound_fall:\n",
        "                self.sound_fall.play()\n",
        "\n",
        "        step_reward = self._reward_calculator(ball_x)\n",
        "        self.score += step_reward\n",
        "        # print(\"ball_x: \", ball_x, \", self.score: \", self.score)\n",
        "        return self._get_observation(), step_reward, terminated\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Convert game state to observation for RL agent\"\"\"\n",
        "        # update particles and draw them\n",
        "        screen_data = self.render() # 获取数据\n",
        "\n",
        "        if self.capture_per_second is not None and self.frame_count % self.capture_per_second == 0:  # Every second at 60 FPS\n",
        "            pygame.image.save(self.screen, f\"capture/frame_{self.frame_count/60}.png\")\n",
        "\n",
        "        self.frame_count += 1\n",
        "        return screen_data\n",
        "\n",
        "\n",
        "    def _update_particles(self):\n",
        "        \"\"\"Update particle effects for indie visual style\"\"\"\n",
        "        # Create new particles when ball hits platform\n",
        "        if abs(self.dynamic_body.position[1] - (self.kinematic_body.position[1] - 20)) < 5 and abs(self.dynamic_body.velocity[1]) > 100:\n",
        "            for _ in range(5):\n",
        "                self.particles.append({\n",
        "                    'x': self.dynamic_body.position[0],\n",
        "                    'y': self.dynamic_body.position[1] + self.ball_radius,\n",
        "                    'vx': random.uniform(-2, 2),\n",
        "                    'vy': random.uniform(1, 3),\n",
        "                    'life': 30,\n",
        "                    'size': random.uniform(2, 5),\n",
        "                    'color': random.choice(self.PARTICLE_COLORS)\n",
        "                })\n",
        "\n",
        "            if self.sound_enabled and self.sound_bounce:\n",
        "                self.sound_bounce.play()\n",
        "\n",
        "        # Update existing particles\n",
        "        for particle in self.particles[:]:\n",
        "            particle['x'] += particle['vx']\n",
        "            particle['y'] += particle['vy']\n",
        "            particle['life'] -= 1\n",
        "            if particle['life'] <= 0:\n",
        "                self.particles.remove(particle)\n",
        "\n",
        "    def render(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Render the current game state\"\"\"\n",
        "        if self.render_mode == \"headless\":\n",
        "            return None\n",
        "\n",
        "        # Clear screen with background color\n",
        "        self.screen.fill(self.BACKGROUND_COLOR)\n",
        "\n",
        "        # Custom drawing (for indie style)\n",
        "        self._draw_indie_style()\n",
        "\n",
        "\n",
        "        # Update display if in human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            # Draw game information\n",
        "            self._draw_game_info()\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.fps)\n",
        "            return None\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            # Return RGB array for gym environment\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\":\n",
        "            self.space.debug_draw(self.draw_options)\n",
        "            current_time = time.time()\n",
        "            if current_time - self.last_update_time >= self.update_interval:\n",
        "                # Convert Pygame surface to an image that can be displayed in Colab\n",
        "                buffer = BytesIO()\n",
        "                pygame.image.save(self.screen, buffer, 'PNG')\n",
        "                buffer.seek(0)\n",
        "                img_data = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "\n",
        "                # Update the HTML image\n",
        "                self.display_handle.update(ipd.HTML(f'''\n",
        "                    <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                        <img id=\"pygame-img\" src=\"data:image/png;base64,{img_data}\" style=\"width:100%;\">\n",
        "                    </div>\n",
        "                '''))\n",
        "\n",
        "                self.last_update_time = current_time\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def _draw_indie_style(self):\n",
        "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
        "        # # Draw platform with gradient and glow\n",
        "        # platform_points = []\n",
        "        # for v in self.platform.get_vertices():\n",
        "        #     x, y = v.rotated(self.kinematic_body.angle) + self.kinematic_body.position\n",
        "        #     platform_points.append((int(x), int(y)))\n",
        "\n",
        "        # pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
        "        # pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
        "\n",
        "        platform_pos = (int(self.kinematic_body.position[0]), int(self.kinematic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
        "\n",
        "        # Draw rotation direction indicator\n",
        "        self._draw_rotation_indicator(platform_pos, self.platform_length, self.kinematic_body.angular_velocity)\n",
        "\n",
        "        # Draw ball with gradient and glow\n",
        "        ball_pos = (int(self.dynamic_body.position[0]), int(self.dynamic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.BALL_COLOR, ball_pos, self.ball_radius)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
        "\n",
        "        # Draw particles\n",
        "        for particle in self.particles:\n",
        "            alpha = min(255, int(255 * (particle['life'] / 30)))\n",
        "            pygame.draw.circle(\n",
        "                self.screen,\n",
        "                particle['color'],\n",
        "                (int(particle['x']), int(particle['y'])),\n",
        "                int(particle['size'])\n",
        "            )\n",
        "\n",
        "    def _draw_rotation_indicator(self, position, radius, angular_velocity):\n",
        "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
        "        # Only draw the indicator if there's some rotation\n",
        "        if abs(angular_velocity) < 0.1:\n",
        "            return\n",
        "\n",
        "        # Calculate indicator properties based on angular velocity\n",
        "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
        "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
        "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
        "\n",
        "        # Draw arrow indicators along the platform's circumference\n",
        "        start_angle = self.kinematic_body.angle\n",
        "\n",
        "        for i in range(num_arrows):\n",
        "            # Calculate arrow position\n",
        "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
        "\n",
        "            # Calculate arrow start and end points\n",
        "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
        "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
        "\n",
        "            # Determine arrow direction based on angular velocity\n",
        "            if angular_velocity > 0:  # Clockwise\n",
        "                arrow_end_angle = arrow_angle + 0.3\n",
        "            else:  # Counter-clockwise\n",
        "                arrow_end_angle = arrow_angle - 0.3\n",
        "\n",
        "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
        "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
        "\n",
        "            # Draw arrow line\n",
        "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
        "\n",
        "            # Draw arrowhead\n",
        "            arrowhead_size = 7\n",
        "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)\n",
        "\n",
        "    def _draw_game_info(self):\n",
        "        \"\"\"Draw game information on screen\"\"\"\n",
        "        # Create texts\n",
        "        time_text = f\"Time: {time.time() - self.start_time:.1f}\"\n",
        "        score_text = f\"Score: {self.score}\"\n",
        "\n",
        "        # Render texts\n",
        "        time_surface = self.font.render(time_text, True, (255, 255, 255))\n",
        "        score_surface = self.font.render(score_text, True, (255, 255, 255))\n",
        "\n",
        "        # Draw text backgrounds\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (5, 5, time_surface.get_width() + 10, time_surface.get_height() + 5))\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (self.window_x - score_surface.get_width() - 15, 5,\n",
        "                         score_surface.get_width() + 10, score_surface.get_height() + 5))\n",
        "\n",
        "        # Draw texts\n",
        "        self.screen.blit(time_surface, (10, 10))\n",
        "        self.screen.blit(score_surface, (self.window_x - score_surface.get_width() - 10, 10))\n",
        "\n",
        "        # Draw game over screen\n",
        "        if self.game_over:\n",
        "            game_over_text = \"GAME OVER - Press R to restart\"\n",
        "            game_over_surface = self.font.render(game_over_text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw semi-transparent background\n",
        "            overlay = pygame.Surface((self.window_x, self.window_y), pygame.SRCALPHA)\n",
        "            overlay.fill((0, 0, 0, 128))\n",
        "            self.screen.blit(overlay, (0, 0))\n",
        "\n",
        "            # Draw text\n",
        "            self.screen.blit(game_over_surface,\n",
        "                           (self.window_x/2 - game_over_surface.get_width()/2,\n",
        "                            self.window_y/2 - game_over_surface.get_height()/2))\n",
        "\n",
        "    def _get_x_axis_max_reward_rate(self, platform_length):\n",
        "        \"\"\"\n",
        "        ((self.platform_length / 2) - 5) for calculate the distance to the\n",
        "        center of game window coordinates. The closer you are, the higher the reward.\n",
        "\n",
        "        When the ball is to be 10 points away from the center coordinates,\n",
        "        it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        \"\"\"\n",
        "        self.reward_width = (platform_length / 2) - 5\n",
        "        self.x_axis_max_reward_rate = 2 / self.reward_width\n",
        "        print(\"self.x_axis_max_reward_rate: \", self.x_axis_max_reward_rate)\n",
        "\n",
        "    def _reward_calculator(self, ball_x):\n",
        "        # score & reward\n",
        "        step_reward = 1/100\n",
        "\n",
        "        rw = abs(ball_x - self.window_x/2)\n",
        "        if rw < self.reward_width:\n",
        "            x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
        "            step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
        "\n",
        "            if self.steps % 500 == 0:\n",
        "                step_reward += self.steps/100\n",
        "                print(\"check point: \", self.steps/500)\n",
        "\n",
        "            return step_reward\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def _reward_calculator2(self, ball_x):\n",
        "        # Base reward for staying alive\n",
        "        step_reward = 0.1\n",
        "\n",
        "        # Distance from center (normalized)\n",
        "        distance_from_center = abs(ball_x - self.window_x/2) / (self.window_x/2)\n",
        "\n",
        "        # Smooth reward based on position (highest at center)\n",
        "        position_reward = max(0, 1.0 - distance_from_center)\n",
        "\n",
        "        # Apply position reward (with higher weight for better position)\n",
        "        step_reward += position_reward * 0.3\n",
        "\n",
        "        # Small bonus for surviving longer (but not dominant)\n",
        "        survival_bonus = min(0.2, self.steps / 10000)\n",
        "        step_reward += survival_bonus\n",
        "\n",
        "        # Checkpoint bonuses remain meaningful but don't explode\n",
        "        if self.steps % 1000 == 0 and self.steps > 0:\n",
        "            step_reward += 1.0\n",
        "            print(f\"Checkpoint reached: {self.steps}\")\n",
        "\n",
        "        return step_reward\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the game and clean up resources\"\"\"\n",
        "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
        "            pygame.quit()\n",
        "\n",
        "    def run_standalone(self):\n",
        "        \"\"\"Run the game in standalone mode with keyboard controls\"\"\"\n",
        "        if self.render_mode not in [\"human\"]:\n",
        "            raise ValueError(\"Standalone mode requires render_mode='human'\")\n",
        "\n",
        "        running = True\n",
        "        while running:\n",
        "            # Handle events\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.QUIT:\n",
        "                    running = False\n",
        "                elif event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_r and self.game_over:\n",
        "                        self.reset()\n",
        "\n",
        "            # Process keyboard controls\n",
        "            keys = pygame.key.get_pressed()\n",
        "            action = 0\n",
        "            if keys[pygame.K_LEFT]:\n",
        "                action = 0 - self.player_ball_speed\n",
        "            if keys[pygame.K_RIGHT]:\n",
        "                action = self.player_ball_speed\n",
        "\n",
        "            # Take game step\n",
        "            if not self.game_over:\n",
        "                self.step(action)\n",
        "\n",
        "            # Render\n",
        "            self.render()\n",
        "\n",
        "        self.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlW6s_8EKPlb"
      },
      "source": [
        "## Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43f8apvbKPlb"
      },
      "outputs": [],
      "source": [
        "class Levels:\n",
        "    def __init__(dynamic_body, kinematic_body):\n",
        "        pass\n",
        "\n",
        "    def"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm68AGmWIaDR"
      },
      "source": [
        "### Level1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IKbIlgaIXor"
      },
      "outputs": [],
      "source": [
        "class Level1:\n",
        "    def __init__(self, dynamic_body, kinematic_body):\n",
        "        self.dynamic_body = dynamic_body\n",
        "        self.kinematic_body = kinematic_body\n",
        "\n",
        "    def setup():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHL2FZgFIY95"
      },
      "source": [
        "### Level2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSQqrLe2IYUh"
      },
      "outputs": [],
      "source": [
        "class Level2:\n",
        "    def __init__(dynamic_body, kinematic_body):\n",
        "        pass\n",
        "\n",
        "    def setup():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC2_6BtrIXWq"
      },
      "source": [
        "### Level3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax2pKrx9IZjB"
      },
      "outputs": [],
      "source": [
        "class Level3:\n",
        "    def __init__(dynamic_body, kinematic_body):\n",
        "        pass\n",
        "\n",
        "    def setup():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjobL-nozI81"
      },
      "source": [
        "## GYM env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBzvHTN1zJu1"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "class BalancingBallEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for the Balancing Ball game\n",
        "    \"\"\"\n",
        "    metadata = {'render_modes': ['human', 'rgb_array']}\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode=\"rgb_array\",\n",
        "                 difficulty=\"medium\",\n",
        "                 fps=30,\n",
        "                 obs_type=\"game_screen\",\n",
        "                 image_size=(84, 84),\n",
        "                ):\n",
        "        \"\"\"\n",
        "        render_mode: how to render the environment\n",
        "            Example: \"human\" or \"rgb_array\"\n",
        "        fps: Frames per second,\n",
        "            Example: 30\n",
        "        obs_type: type of observation\n",
        "            Example: \"game_screen\" or \"state_based\"\n",
        "        image_size: Size to resize images to (height, width)\n",
        "            Example: (84, 84) - standard for many RL implementations\n",
        "        \"\"\"\n",
        "\n",
        "        super(BalancingBallEnv, self).__init__()\n",
        "\n",
        "        # Action space: discrete - 0: left, 1: right\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Initialize game\n",
        "        self.window_x = 300\n",
        "        self.window_y = 180\n",
        "        self.platform_shape = \"circle\"\n",
        "        self.platform_proportion = 0.333\n",
        "\n",
        "        # Image preprocessing settings\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.stack_size = 3  # Number of frames to stack\n",
        "        self.observation_stack = []  # Initialize the stack\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.game = BalancingBallGame(\n",
        "            render_mode=render_mode,\n",
        "            sound_enabled=(render_mode == \"human\"),\n",
        "            difficulty=difficulty,\n",
        "            window_x = self.window_x,\n",
        "            window_y = self.window_y,\n",
        "            fps = fps,\n",
        "            platform_shape = self.platform_shape,\n",
        "            platform_proportion = self.platform_proportion,\n",
        "        )\n",
        "\n",
        "        if obs_type == \"game_screen\":\n",
        "            channels = 1\n",
        "\n",
        "            # Image observation space with stacked frames\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=0, high=255,\n",
        "                shape=(self.image_size[0], self.image_size[1], channels * self.stack_size),\n",
        "                dtype=np.uint8,\n",
        "            )\n",
        "            self.step = self.step_game_screen\n",
        "            self.reset = self.reset_game_screen\n",
        "        elif obs_type == \"state_based\":\n",
        "            # State-based observation space: [ball_x, ball_y, ball_vx, ball_vy, platform_x, platform_y, platform_angular_velocity]\n",
        "            # Normalize values to be between -1 and 1\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]),\n",
        "                high=np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
        "                dtype=np.float32\n",
        "            )\n",
        "            self.step = self.step_state_based\n",
        "            self.reset = self.reset_state_based\n",
        "        else:\n",
        "            raise ValueError(\"obs_type must be 'game_screen' or 'state_based'\")\n",
        "\n",
        "        # Platform_length /= 2 when for calculate the distance to the\n",
        "        # center of game window coordinates. The closer you are, the higher the reward.\n",
        "        self.platform_reward_length = (self.game.platform_length / 2) - 5\n",
        "\n",
        "        # When the ball is to be 10 points away from the center coordinates,\n",
        "        # it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        self.x_axis_max_reward_rate = 0.5 / self.platform_reward_length\n",
        "\n",
        "    def _preprocess_observation(self, observation):\n",
        "        \"\"\"Process raw game observation for RL training\n",
        "\n",
        "        Args:\n",
        "            observation: RGB image from the game\n",
        "\n",
        "        Returns:\n",
        "            Processed observation ready for RL\n",
        "        \"\"\"\n",
        "        observation = np.transpose(observation, (1, 0, 2))\n",
        "\n",
        "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = np.expand_dims(observation, axis=-1)  # Add channel dimension back\n",
        "\n",
        "        # Resize to target size\n",
        "        if observation.shape[0] != self.image_size[0] or observation.shape[1] != self.image_size[1]:\n",
        "            # For grayscale, temporarily remove the channel dimension for cv2.resize\n",
        "            observation = cv2.resize(\n",
        "                observation.squeeze(-1),\n",
        "                (self.image_size[1], self.image_size[0]),\n",
        "                interpolation=cv2.INTER_AREA\n",
        "            )\n",
        "            observation = np.expand_dims(observation, axis=-1)  # Add channel dimension back\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step_game_screen(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Take step in the game\n",
        "        obs, step_reward, terminated = self.game.step(action)\n",
        "\n",
        "        # Preprocess the observation\n",
        "        obs = self._preprocess_observation(obs)\n",
        "\n",
        "        # Stack the frames\n",
        "        self.observation_stack.append(obs)\n",
        "        if len(self.observation_stack) > self.stack_size:\n",
        "            self.observation_stack.pop(0)  # Remove the oldest frame\n",
        "\n",
        "        # If the stack isn't full yet, pad it with the current frame\n",
        "        while len(self.observation_stack) < self.stack_size:\n",
        "            self.observation_stack.insert(0, obs)  # Pad with current frame at the beginning\n",
        "\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return stacked_obs, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset_game_screen(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        observation = self.game.reset()\n",
        "\n",
        "        # Preprocess the observation\n",
        "        observation = self._preprocess_observation(observation)\n",
        "\n",
        "        # Reset the observation stack\n",
        "        self.observation_stack = []\n",
        "\n",
        "        # Fill the stack with the initial observation\n",
        "        for _ in range(self.stack_size):\n",
        "            self.observation_stack.append(observation)\n",
        "\n",
        "        # Create stacked observation\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        info = {}\n",
        "        return stacked_obs, info\n",
        "\n",
        "    def _get_state_based_observation(self):\n",
        "        \"\"\"Convert game state to state-based observation for RL agent\"\"\"\n",
        "        # Normalize positions by window dimensions\n",
        "        ball_x = self.game.dynamic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
        "        ball_y = self.game.dynamic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
        "\n",
        "        # Normalize velocities (assuming max velocity around 1000)\n",
        "        max_velocity = 1000\n",
        "        ball_vx = np.clip(self.game.dynamic_body.velocity[0] / max_velocity, -1, 1)\n",
        "        ball_vy = np.clip(self.game.dynamic_body.velocity[1] / max_velocity, -1, 1)\n",
        "\n",
        "        # Normalize platform position\n",
        "        platform_x = self.game.kinematic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
        "        platform_y = self.game.kinematic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
        "\n",
        "        # Normalize angular velocity (assuming max around 10)\n",
        "        max_angular_velocity = 10\n",
        "        platform_angular_velocity = np.clip(self.game.kinematic_body.angular_velocity / max_angular_velocity, -1, 1)\n",
        "\n",
        "        return np.array([\n",
        "            ball_x,\n",
        "            ball_y,\n",
        "            ball_vx,\n",
        "            ball_vy,\n",
        "            platform_x,\n",
        "            platform_y,\n",
        "            platform_angular_velocity\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step_state_based(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Take step in the game\n",
        "        _, step_reward, terminated = self.game.step(action)\n",
        "\n",
        "        # Get state-based observation\n",
        "        observation = self._get_state_based_observation()\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return observation, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset_state_based(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        self.game.reset()\n",
        "        observation = self._get_state_based_observation()\n",
        "\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment\"\"\"\n",
        "        return self.game.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        self.game.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgvyLDYKPlb"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebr3cvEiKPlc"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "def run_standalone_game(render_mode=\"human\", difficulty=\"medium\", capture_per_second=3):\n",
        "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "    window_x = 1000\n",
        "    window_y = 600\n",
        "    platform_shape = \"circle\"\n",
        "    platform_proportion = 0.333\n",
        "\n",
        "    game = BalancingBallGame(\n",
        "        render_mode = render_mode,\n",
        "        difficulty = difficulty,\n",
        "        window_x = window_x,\n",
        "        window_y = window_y,\n",
        "        platform_shape = platform_shape,\n",
        "        platform_proportion = platform_proportion,\n",
        "        fps = 30,\n",
        "        capture_per_second = 3,\n",
        "    )\n",
        "\n",
        "    game.run_standalone()\n",
        "\n",
        "def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
        "    \"\"\"Test the OpenAI Gym environment\"\"\"\n",
        "    import time\n",
        "    # from gym_env import BalancingBallEnv\n",
        "\n",
        "    fps = 30\n",
        "    env = BalancingBallEnv(\n",
        "        render_mode=\"human\",\n",
        "        difficulty=difficulty,\n",
        "        fps=fps,\n",
        "    )\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample a random action (for testing only)\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            # Render\n",
        "            env.render()\n",
        "\n",
        "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy  # MLP policy instead of CNN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "class Train:\n",
        "    def __init__(self,\n",
        "                 learning_rate=0.0003,\n",
        "                 n_steps=2048,\n",
        "                 batch_size=64,\n",
        "                 n_epochs=10,\n",
        "                 gamma=0.99,\n",
        "                 gae_lambda=0.95,\n",
        "                 ent_coef=0.01,\n",
        "                 vf_coef=0.5,\n",
        "                 max_grad_norm=0.5,\n",
        "                 policy_kwargs=None,\n",
        "                 n_envs=4,\n",
        "                 difficulty=\"medium\",\n",
        "                 load_model=None,\n",
        "                 log_dir=\"./logs/\",\n",
        "                 model_dir=\"./models/\",\n",
        "                 obs_type=\"game_screen\",\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        self.log_dir = log_dir\n",
        "        self.model_dir = model_dir\n",
        "        self.n_envs = n_envs\n",
        "        self.obs_type = obs_type\n",
        "\n",
        "        # Setup environments\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=obs_type),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        self.env = env  # No need for VecTransposeImage with state-based observations\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = eval_env  # No need for VecTransposeImage\n",
        "\n",
        "        # Define policy kwargs if not provided\n",
        "        if policy_kwargs is None:\n",
        "            policy_kwargs = {\n",
        "                \"net_arch\": [256, 256]  # MLP architecture\n",
        "            }\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=log_dir,\n",
        "            )\n",
        "        else:\n",
        "            hyper_param = {\n",
        "                'learning_rate': 0.0003,\n",
        "                'gamma': 0.99,\n",
        "                'clip_range': 0.2,\n",
        "                'gae_lambda': 0.95,\n",
        "                'ent_coef': 0.01,\n",
        "                'vf_coef': 0.5,\n",
        "            }\n",
        "            policy = ActorCriticPolicy if obs_type == \"game_screen\" else ActorCriticCnnPolicy\n",
        "            print(\"obs type: \", self.obs_type)\n",
        "            print(\"policy: \", policy)\n",
        "            # MLP policy for state-based observations, CNN policy for image-based observations\n",
        "            self.model = PPO(\n",
        "                policy=policy,\n",
        "                env=self.env,\n",
        "                learning_rate=hyper_param[\"learning_rate\"],\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=hyper_param[\"gamma\"],\n",
        "                clip_range=hyper_param[\"clip_range\"],\n",
        "                gae_lambda=hyper_param[\"gae_lambda\"],\n",
        "                ent_coef=hyper_param[\"ent_coef\"],\n",
        "                vf_coef=hyper_param[\"vf_coef\"],\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=log_dir,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=1,\n",
        "            )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty, obs_type=obs_type)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self,\n",
        "                  total_timesteps=1000000,\n",
        "                  save_freq=10000,\n",
        "                  eval_freq=10000,\n",
        "                  eval_episodes=5,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball_\" + str(self.obs_type),\n",
        "        )\n",
        "\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=eval_freq // self.n_envs,\n",
        "            n_eval_episodes=eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=[checkpoint_callback, eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final_\" + str(self.obs_type))\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10, difficulty=\"medium\"):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         difficulty=args.difficulty,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#             difficulty=args.difficulty\n",
        "#         )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpEVcsjfs45Q"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4gwCvLVs45Q"
      },
      "outputs": [],
      "source": [
        "class Optuna_optimize:\n",
        "    def __init__(self, obs_type=\"game_screen\"):\n",
        "        self.obs_type = obs_type\n",
        "        self.env = None\n",
        "        self.model = None\n",
        "        pass\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty, obs_type=obs_type)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def optuna_parameter_tuning(self, n_trials):\n",
        "        print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "        pruner = optuna.pruners.HyperbandPruner(\n",
        "            min_resource=100,        # 最小资源量\n",
        "            max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "            reduction_factor=3     # 折减因子 (eta)\n",
        "        )\n",
        "\n",
        "        # 建立 study 物件，並指定剪枝器\n",
        "        study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "        # 執行優化\n",
        "        study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "        # 分析結果\n",
        "        print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "        print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "        import pandas as pd\n",
        "        df = study.trials_dataframe()\n",
        "        print(df.head())\n",
        "\n",
        "    def objective(self, trial):\n",
        "        import gc\n",
        "\n",
        "        # 1. 建議超參數\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 0.9, 0.999)\n",
        "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "        gae_lambda = trial.suggest_float('gae_lambda', 0.5, 2)\n",
        "        ent_coef = trial.suggest_float('ent_coef', 0.005, 0.05)\n",
        "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "        features_dim = trial.suggest_categorical('features_dim', [32, 64, 128, 256, 512])\n",
        "        policy_kwargs = {\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "            \"net_arch\": [256, 256],  # MLP architecture\n",
        "        }\n",
        "\n",
        "        n_steps=2048\n",
        "        batch_size=64\n",
        "        n_epochs=10\n",
        "        # gamma=0.99\n",
        "        # gae_lambda=0.95\n",
        "        # ent_coef=0.01\n",
        "        # vf_coef=0.5\n",
        "        max_grad_norm=0.5\n",
        "        # policy_kwargs = {\n",
        "        #     \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "        # }\n",
        "\n",
        "        # 2. 建立環境\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=self.obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "\n",
        "        policy = ActorCriticCnnPolicy if self.obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "        print(\"obs type: \", self.obs_type)\n",
        "        print(\"policy: \", policy)\n",
        "        # 3. 建立模型\n",
        "        model = PPO(\n",
        "                policy=policy,  # MLP policy for state-based observations\n",
        "                env=env,\n",
        "                learning_rate=learning_rate,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=gamma,\n",
        "                clip_range=clip_range,\n",
        "                gae_lambda=gae_lambda,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=None,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=0,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # 4. 訓練模型\n",
        "            model.learn(total_timesteps=7000)\n",
        "            # 5. 評估模型\n",
        "            mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "        finally:\n",
        "            # Always cleanup\n",
        "            env.close()\n",
        "            del model\n",
        "            del env\n",
        "            gc.collect()\n",
        "\n",
        "            if TPU_AVAILABLE:\n",
        "                import torch_xla.core.xla_model as xm\n",
        "                xm.mark_step()\n",
        "\n",
        "        return mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ndr9hGp2CZzF"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Memory-optimized training setup\n",
        "def get_tpu_memory_info():\n",
        "    \"\"\"Get memory information from TPU device if available\"\"\"\n",
        "    if TPU_AVAILABLE:\n",
        "        try:\n",
        "            # This is just for diagnostic purposes\n",
        "            import subprocess\n",
        "            result = subprocess.run(['python3', '-c', 'import torch_xla; print(torch_xla._XLAC._xla_get_memory_info(torch_xla._XLAC._xla_get_default_device()))'],\n",
        "                                   stdout=subprocess.PIPE, text=True)\n",
        "            print(f\"TPU Memory Info: {result.stdout}\")\n",
        "        except:\n",
        "            print(\"Could not get detailed TPU memory info\")\n",
        "\n",
        "# Display memory information\n",
        "get_tpu_memory_info()\n",
        "\n",
        "n_envs = 1\n",
        "batch_size = 64\n",
        "n_steps = 2048\n",
        "\n",
        "# Policy kwargs for MLP (state-based observations)\n",
        "policy_kwargs = {\n",
        "    \"net_arch\": [256, 256]  # Simpler MLP architecture\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = True\n",
        "\n",
        "if do_optimization:\n",
        "    optuna_optimizer = Optuna_optimize(obs_type=\"game_screen\")\n",
        "    # Force TPU memory cleanup before starting\n",
        "    if TPU_AVAILABLE:\n",
        "        gc.collect()\n",
        "        xm.mark_step()\n",
        "\n",
        "    n_trials = 10\n",
        "    best_trial = optuna_optimizer.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    print(f\"best_trial found: {best_trial}\")\n",
        "else:\n",
        "    # Create trainer\n",
        "    training = Train(\n",
        "        n_steps=n_steps,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=n_envs,\n",
        "        load_model=None,  # Start fresh with state-based env\n",
        "        policy_kwargs=policy_kwargs,\n",
        "    )\n",
        "    # Run training with memory-optimized settings\n",
        "    # Use fewer total timesteps for TPU to avoid memory issues\n",
        "    total_timesteps = 500000\n",
        "\n",
        "    model = training.train_ppo(\n",
        "        total_timesteps=total_timesteps,\n",
        "        eval_episodes=3,  # Fewer eval episodes on TPU\n",
        "        save_freq=5000,\n",
        "        eval_freq=5000\n",
        "    )\n",
        "\n",
        "    # Force memory cleanup after training\n",
        "    if TPU_AVAILABLE:\n",
        "        del model\n",
        "        gc.collect()\n",
        "        xm.mark_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "outputs": [],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "outputs": [],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        n_steps=1024,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=1  # Use 1 env for evaluation\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "        difficulty=\"medium\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4h3jBARKPld"
      },
      "outputs": [],
      "source": [
        "# run_standalone_game(difficulty=\"medium\")\n",
        "# test_gym_env(difficulty=\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCFm3AoWs45R"
      },
      "outputs": [],
      "source": [
        "# Example of creating the environment with grayscale images\n",
        "env = BalancingBallEnv(\n",
        "    render_mode=\"rgb_array\",\n",
        "    difficulty=\"medium\",\n",
        "    fps=30,\n",
        "    obs_type=\"game_screen\",\n",
        "    image_size=(84, 84)  # Standard size for many RL frameworks\n",
        ")\n",
        "\n",
        "# Reset environment to get initial observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Print observation shape to verify\n",
        "print(f\"Observation shape: {obs.shape}\")  # Should be (84, 84, 3) for grayscale with 3 stacked frames\n",
        "\n",
        "# Display a sample observation (first frame only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(obs[:,:,0], cmap='gray')\n",
        "plt.title(\"Grayscale Observation (First Frame)\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}