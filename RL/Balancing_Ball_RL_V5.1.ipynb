{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V4.2 Update: in readme.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "a026f8e7-310f-41f5-e2c4-ab43e87c33bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch XLA not found, will attempt to install\n",
            "TPU support installed. Please restart the runtime now.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will attempt to install\")\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install pygame-ce pymunk stable-baselines3 stable-baselines3[extra] shimmy>=2.0 optuna\n",
        "!pip install -q cloud-tpu-client\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "94c03862-6ebf-4973-93c0-4e1b93c9930a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'=2.0'\t        logs\t\t\t\t\t\t sample_data\n",
            " capture        models\n",
            " game_history   ppo_balancing_ball_game_screen_25000_steps.zip\n"
          ]
        }
      ],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6bbqyvjZ1PZu"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhEqO-xFu4AI"
      },
      "source": [
        "## Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gJwfQb_Yuz1I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "class Recorder:\n",
        "\n",
        "    def __init__(self, task: str = \"game_history_record\"):\n",
        "        \"\"\"\n",
        "        tasks:\n",
        "        1. game_history_record\n",
        "        2. temp_memory\n",
        "        \"\"\"\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \"\"\n",
        "        if task == \"game_history_record\":\n",
        "            collection_name = self.get_newest_record_name()\n",
        "            self.json_file_path = CURRENT_DIR + \"./game_history/\" + collection_name + \".json\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)\n",
        "\n",
        "        if os.path.exists(self.json_file_path):\n",
        "            print(\"Loading the json memory file\")\n",
        "            self.memory = self.load(self.json_file_path)\n",
        "        else:\n",
        "            print(\"The json memory file does not exist. Creating new file.\")\n",
        "            self.memory = {\"game_records\": []}  # Direct dictionary instead of json.loads\n",
        "            with open(self.json_file_path, \"w\") as f:\n",
        "                json.dump(self.memory, f)\n",
        "\n",
        "    def get(self):\n",
        "        print(\"Getting the json memory\")\n",
        "        return self.memory\n",
        "\n",
        "    def add_no_limit(self, data: float, ):\n",
        "        \"\"\"\n",
        "        Add a records.\n",
        "\n",
        "        Args:\n",
        "            role: The role of the sender (e.g., 'user', 'assistant')\n",
        "            message: The message content\n",
        "        \"\"\"\n",
        "        self.memory[\"game_records\"].append({\n",
        "            \"game_total_duration\": data,\n",
        "            \"timestamp\": str(datetime.datetime.now())\n",
        "        })\n",
        "\n",
        "        self.save(self.json_file_path)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump(self.memory, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving memory to {file_path}: {e}\")\n",
        "\n",
        "    def load(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading memory from {file_path}: {e}\")\n",
        "            return {\"game_records\": []}\n",
        "\n",
        "    def get_newest_record_name(self) -> str:\n",
        "        \"\"\"\n",
        "        傳回最新的對話歷史資料和集的名稱 (game_YYYY_MM)\n",
        "            - 例如: \"game_2022-01\"\n",
        "        \"\"\"\n",
        "\n",
        "        this_month = datetime.datetime.now().strftime(\"%Y-%m\")\n",
        "        return \"record_\" + this_month"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnA8wZtosmeN"
      },
      "source": [
        "## Shapes & Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qL9XJcPV03-M"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Shape:\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape: Optional[pymunk.Shape] = None,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a physical shape with associated body.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the body\n",
        "            velocity: Initial velocity (vx, vy) of the body\n",
        "            body: The pymunk Body to attach to this shape\n",
        "            shape: The pymunk Shape for collision detection\n",
        "        \"\"\"\n",
        "\n",
        "        self.body = body\n",
        "        self.default_position = position\n",
        "        self.default_velocity = velocity\n",
        "        self.body.position = position\n",
        "        self.body.velocity = velocity\n",
        "        self.default_angular_velocity = 0\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the body to its default position, velocity and angular velocity.\"\"\"\n",
        "        self.body.position = self.default_position\n",
        "        self.body.velocity = self.default_velocity\n",
        "        self.body.angular_velocity = self.default_angular_velocity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dR0LWM9r03-N"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "\n",
        "# from shapes.shape import Shape\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Circle(Shape):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape_radio: float = 20,\n",
        "                shape_mass: float = 1,\n",
        "                shape_friction: float = 0.1,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a circular physics object.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the circle\n",
        "            velocity: Initial velocity (vx, vy) of the circle\n",
        "            body: The pymunk Body to attach this circle to\n",
        "            shape_radio: Radius of the circle in pixels\n",
        "            shape_mass: Mass of the circle\n",
        "            shape_friction: Friction coefficient for the circle\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(position, velocity, body)\n",
        "        self.shape_radio = shape_radio\n",
        "        self.shape = pymunk.Circle(self.body, shape_radio)\n",
        "        self.shape.mass = shape_mass\n",
        "        self.shape.friction = shape_friction\n",
        "        self.shape.elasticity = 0.8  # Add some bounce to make the simulation more interesting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vECK-IK203-N"
      },
      "source": [
        "## Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o-tue0N03-N"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pymunk\n",
        "import pygame\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from shapes.circle import Circle\n",
        "\n",
        "def get_level(level: int, space):\n",
        "    \"\"\"\n",
        "    Get the level object based on the level number.\n",
        "    \"\"\"\n",
        "    if level == 1:\n",
        "        return Level1(space)\n",
        "    elif level == 2:\n",
        "        return Level2(space)\n",
        "    elif level == 3:\n",
        "        return Level3(space)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid level number\")\n",
        "\n",
        "class Levels:\n",
        "    def __init__(self, space, window_x: int = 1000, window_y: int = 600):\n",
        "        self.space = space\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "\n",
        "    def create_player(self,\n",
        "                      default_player_position: tuple = None,\n",
        "                      ball_color = (255, 213, 79),  # Bright yellow ball\n",
        "                      window_x: int = 1000,\n",
        "                      window_y: int = 600,\n",
        "                     ):\n",
        "        \"\"\"\n",
        "        Create the ball with physics properties\n",
        "        default_player_position: Initial position of the player\n",
        "            default: (window_x / 2, window_y / 5)\n",
        "        \"\"\"\n",
        "        if default_player_position is None:\n",
        "            default_player_position = (window_x / 2, window_y / 5)\n",
        "        dynamic_body = pymunk.Body()  # Ball body\n",
        "        ball_radius = int(window_x / 67)\n",
        "        player = Circle(\n",
        "            position=default_player_position,\n",
        "            velocity=(0, 0),\n",
        "            body=dynamic_body,\n",
        "            shape_radio=ball_radius,\n",
        "            shape_friction=100,\n",
        "        )\n",
        "        # Store initial values for reset\n",
        "        return {\n",
        "            \"type\": \"player\",\n",
        "            \"shape\": player,\n",
        "            \"default_position\": default_player_position,\n",
        "            \"body\": dynamic_body,\n",
        "            \"ball_radius\": ball_radius,\n",
        "            \"ball_color\": ball_color\n",
        "        }\n",
        "\n",
        "    def create_platform(self,\n",
        "                        platform_shape: str = \"circle\",\n",
        "                        platform_proportion: float = 0.4,\n",
        "                        window_x: int = 1000,\n",
        "                        window_y: int = 600,\n",
        "                       ):\n",
        "        \"\"\"\n",
        "        Create the platform with physics properties\n",
        "        platform_shape: circle, rectangle\n",
        "        platform_length: Length of a rectangle or Diameter of a circle\n",
        "        \"\"\"\n",
        "        platform_length = int(window_x * platform_proportion)\n",
        "\n",
        "        # Create game bodies\n",
        "        kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
        "        kinematic_body.position = (window_x / 2, (window_y / 3) * 2)\n",
        "        default_kinematic_position = kinematic_body.position\n",
        "\n",
        "        if platform_shape == \"circle\":\n",
        "            platform_length = platform_length / 2 # radius\n",
        "            platform = pymunk.Circle(kinematic_body, platform_length)\n",
        "            platform.mass = 1  # 质量对 Kinematic 物体无意义，但需要避免除以零错误\n",
        "            platform.friction = 0.7\n",
        "\n",
        "        elif platform_shape == \"rectangle\":\n",
        "            platform_length = platform_length\n",
        "            vs = [(-platform_length/2, -10),\n",
        "                (platform_length/2, -10),\n",
        "                (platform_length/2, 10),\n",
        "                (-platform_length/2, 10)]\n",
        "\n",
        "            platform = pymunk.Poly(kinematic_body, vs)\n",
        "        platform.friction = 0.7\n",
        "        platform.rotation = 0\n",
        "\n",
        "        return {\n",
        "            \"type\": \"platform\",\n",
        "            \"platform_shape\": platform_shape,\n",
        "            \"shape\": platform,\n",
        "            \"default_position\": default_kinematic_position,\n",
        "            \"body\": kinematic_body,\n",
        "            \"platform_length\": platform_length,\n",
        "        }\n",
        "\n",
        "# TODO not use for now\n",
        "    def _draw_indie_style(self):\n",
        "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
        "        # # Draw platform with gradient and glow\n",
        "        # platform_points = []\n",
        "        # for v in self.platform.get_vertices():\n",
        "        #     x, y = v.rotated(self.kinematic_body.angle) + self.kinematic_body.position\n",
        "        #     platform_points.append((int(x), int(y)))\n",
        "\n",
        "        # pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
        "        # pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
        "\n",
        "        platform_pos = (int(self.kinematic_body.position[0]), int(self.kinematic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
        "\n",
        "        # Draw rotation direction indicator\n",
        "        self._draw_rotation_indicator(platform_pos, self.platform_length, self.kinematic_body.angular_velocity)\n",
        "\n",
        "        # Draw ball with gradient and glow\n",
        "        ball_pos = (int(self.dynamic_body.position[0]), int(self.dynamic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.BALL_COLOR, ball_pos, self.ball_radius)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
        "\n",
        "# TODO not use for now\n",
        "    def _draw_rotation_indicator(self, position, radius, angular_velocity):\n",
        "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
        "        # Only draw the indicator if there's some rotation\n",
        "        if abs(angular_velocity) < 0.1:\n",
        "            return\n",
        "\n",
        "        # Calculate indicator properties based on angular velocity\n",
        "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
        "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
        "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
        "\n",
        "        # Draw arrow indicators along the platform's circumference\n",
        "        start_angle = self.kinematic_body.angle\n",
        "\n",
        "        for i in range(num_arrows):\n",
        "            # Calculate arrow position\n",
        "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
        "\n",
        "            # Calculate arrow start and end points\n",
        "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
        "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
        "\n",
        "            # Determine arrow direction based on angular velocity\n",
        "            if angular_velocity > 0:  # Clockwise\n",
        "                arrow_end_angle = arrow_angle + 0.3\n",
        "            else:  # Counter-clockwise\n",
        "                arrow_end_angle = arrow_angle - 0.3\n",
        "\n",
        "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
        "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
        "\n",
        "            # Draw arrow line\n",
        "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
        "\n",
        "            # Draw arrowhead\n",
        "            arrowhead_size = 7\n",
        "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU2cMKZZ03-N"
      },
      "source": [
        "### Level1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMgnftJ703-N"
      },
      "outputs": [],
      "source": [
        "class Level1(Levels):\n",
        "    \"\"\"\n",
        "    Level 1: Basic setup with a dynamic body and a static kinematic body.\n",
        "    \"\"\"\n",
        "    def __init__(self, space):\n",
        "        super().__init__(space)\n",
        "        self.space = space\n",
        "\n",
        "\n",
        "    def setup(self, window_x, window_y):\n",
        "        player = super().create_player(window_x=window_x, window_y=window_y)\n",
        "        platform = super().create_platform(window_x=window_x, window_y=window_y)\n",
        "        self.space.add(player[\"body\"], player[\"shape\"].shape)\n",
        "        self.space.add(platform[\"body\"], platform[\"shape\"])\n",
        "        self.dynamic_body = player[\"body\"]\n",
        "        self.kinematic_body = platform[\"body\"]\n",
        "        self.default_player_position = player[\"default_position\"]\n",
        "\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        return (player, ), (platform, )\n",
        "\n",
        "    def action(self):\n",
        "        \"\"\"\n",
        "        shape state changes in the game\n",
        "        \"\"\"\n",
        "        # Noting to do in this level\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the level to its initial state.\n",
        "        \"\"\"\n",
        "        self.dynamic_body.position = self.default_player_position\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        self.space.reindex_shapes_for_body(self.dynamic_body)\n",
        "        self.space.reindex_shapes_for_body(self.kinematic_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utdqc0xf03-O"
      },
      "source": [
        "### Level2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW5YaemX03-O"
      },
      "outputs": [],
      "source": [
        "class Level2(Levels):\n",
        "    \"\"\"\n",
        "    Level 1: Basic setup with a dynamic body and a static kinematic body.\n",
        "    \"\"\"\n",
        "    def __init__(self, space):\n",
        "        super().__init__(space)\n",
        "        self.space = space\n",
        "        self.last_angular_velocity_change_time = time.time()\n",
        "        self.angular_velocity_change_timeout = 5 # sec\n",
        "\n",
        "\n",
        "    def setup(self, window_x, window_y):\n",
        "        player = super().create_player(window_x=window_x, window_y=window_y)\n",
        "        platform = super().create_platform(window_x=window_x, window_y=window_y)\n",
        "        self.space.add(player[\"body\"], player[\"shape\"].shape)\n",
        "        self.space.add(platform[\"body\"], platform[\"shape\"])\n",
        "        self.dynamic_body = player[\"body\"]\n",
        "        self.kinematic_body = platform[\"body\"]\n",
        "        self.default_player_position = player[\"default_position\"]\n",
        "        \n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        return (player, ), (platform, )\n",
        "\n",
        "    def action(self):\n",
        "        \"\"\"\n",
        "        shape state changes in the game\n",
        "        \"\"\"\n",
        "\n",
        "        if time.time() - self.last_angular_velocity_change_time > self.angular_velocity_change_timeout:\n",
        "            self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "            self.last_angular_velocity_change_time = time.time()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the level to its initial state.\n",
        "        \"\"\"\n",
        "        self.dynamic_body.position = self.default_player_position\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "        self.last_angular_velocity_change_time = time.time()\n",
        "\n",
        "        self.space.reindex_shapes_for_body(self.dynamic_body)\n",
        "        self.space.reindex_shapes_for_body(self.kinematic_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5GpDMbr03-O"
      },
      "source": [
        "### Level3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JZ3BX0p03-O"
      },
      "outputs": [],
      "source": [
        "# Two players\n",
        "# NOTE: 連續動作空間和對抗式訓練\n",
        "class Level3(Levels):\n",
        "    \"\"\"\n",
        "    Level 1: Basic setup with a dynamic body and a static kinematic body.\n",
        "    \"\"\"\n",
        "    def __init__(self, space):\n",
        "        super().__init__(space)\n",
        "        self.space = space\n",
        "\n",
        "\n",
        "    def setup(self, window_x, window_y):\n",
        "        x = window_x / 5\n",
        "        player1 = super().create_player(window_x=window_x, \n",
        "                                        window_y=window_y, \n",
        "                                        default_player_position=(x*2, window_y / 5)\n",
        "                                       )\n",
        "        player2 = super().create_player(window_x=window_x, \n",
        "                                        window_y=window_y, \n",
        "                                        ball_color=(194, 238, 84), \n",
        "                                        default_player_position=(x*3, window_y / 5)\n",
        "                                       )\n",
        "        platform = super().create_platform(platform_shape=\"rectangle\", platform_proportion=0.8, window_x=window_x, window_y=window_y)\n",
        "        self.space.add(player1[\"body\"], player1[\"shape\"].shape)\n",
        "        self.space.add(player2[\"body\"], player2[\"shape\"].shape)\n",
        "        self.space.add(platform[\"body\"], platform[\"shape\"])\n",
        "        self.dynamic_body1 = player1[\"body\"]\n",
        "        self.dynamic_body2 = player2[\"body\"]\n",
        "        self.kinematic_body = platform[\"body\"]\n",
        "        self.default_player_position1 = player1[\"default_position\"]\n",
        "        self.default_player_position2 = player2[\"default_position\"]\n",
        "\n",
        "        return (player1, player2), (platform, )\n",
        "\n",
        "    def action(self):\n",
        "        \"\"\"\n",
        "        shape state changes in the game\n",
        "        \"\"\"\n",
        "        # Noting to do in this level\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the level to its initial state.\n",
        "        \"\"\"\n",
        "        self.dynamic_body1.position = self.default_player_position1\n",
        "        self.dynamic_body1.angular_velocity = 0\n",
        "        self.dynamic_body1.velocity = (0, 0)\n",
        "        self.dynamic_body2.position = self.default_player_position2\n",
        "        self.dynamic_body2.angular_velocity = 0\n",
        "        self.dynamic_body2.velocity = (0, 0)\n",
        "        self.last_angular_velocity_change_time = time.time()\n",
        "\n",
        "        self.space.reindex_shapes_for_body(self.dynamic_body1)\n",
        "        self.space.reindex_shapes_for_body(self.dynamic_body2)\n",
        "        self.space.reindex_shapes_for_body(self.kinematic_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj7VhE-K03-O"
      },
      "source": [
        "### --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PTCyvj_J03-O"
      },
      "outputs": [],
      "source": [
        "def get_level(level: int, space):\n",
        "    \"\"\"\n",
        "    Get the level object based on the level number.\n",
        "    \"\"\"\n",
        "    if level == 1:\n",
        "        return Level1(space)\n",
        "    elif level == 2:\n",
        "        return Level2(space)\n",
        "    elif level == 3:\n",
        "        return Level3()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid level number\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8d5fKltI62"
      },
      "source": [
        "## Game class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wiw5Rjks-xw",
        "outputId": "32e68cd6-9722-48a1-b5e5-f61e8f96d6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame-ce 2.5.3 (SDL 2.30.12, Python 3.11.12)\n"
          ]
        }
      ],
      "source": [
        "import pymunk\n",
        "import pygame\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "# import IPython.display as ipd\n",
        "\n",
        "from typing import Dict, Tuple, Optional\n",
        "# from IPython.display import display, Image, clear_output\n",
        "from io import BytesIO\n",
        "from record import Recorder\n",
        "from levels.levels import get_level\n",
        "\n",
        "class BalancingBallGame:\n",
        "    \"\"\"\n",
        "    A physics-based balancing ball game that can run standalone or be used as a Gym environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Game constants\n",
        "\n",
        "\n",
        "    # Visual settings for indie style\n",
        "    BACKGROUND_COLOR = (41, 50, 65)  # Dark blue background\n",
        "    BALL_COLOR = (255, 213, 79)  # Bright yellow ball\n",
        "    PLATFORM_COLOR = (235, 64, 52)  # Red platform\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode: str = \"human\",\n",
        "                 sound_enabled: bool = True,\n",
        "                 difficulty: str = \"medium\",\n",
        "                 window_x: int = 1000,\n",
        "                 window_y: int = 600,\n",
        "                 max_step: int = 30000,\n",
        "                 player_ball_speed: int = 5,\n",
        "                 reward_staying_alive: float = 0.1,\n",
        "                 reward_ball_centered: float = 0.2,\n",
        "                 penalty_falling: float = -10.0,\n",
        "                 level: int = 2,\n",
        "                 fps: int = 120,\n",
        "                 platform_shape: str = \"circle\",\n",
        "                 platform_proportion: int = 0.4,\n",
        "                 capture_per_second: int = None,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialize the balancing ball game.\n",
        "\n",
        "        Args:\n",
        "            render_mode: \"human\" for visible window, \"rgb_array\" for gym env, \"headless\" for no rendering\n",
        "            sound_enabled: Whether to enable sound effects\n",
        "            difficulty: Game difficulty level (\"easy\", \"medium\", \"hard\")\n",
        "            max_step: 1 step = 1/fps, if fps = 120, 1 step = 1/120\n",
        "            reward_staying_alive: float = 0.1,\n",
        "            reward_ball_centered: float = 0.2,\n",
        "            penalty_falling: float = -10.0,\n",
        "            fps: frame per second\n",
        "            platform_proportion: platform_length = window_x * platform_proportion\n",
        "            capture_per_second: save game screen as a image every second, None means no capture\n",
        "        \"\"\"\n",
        "        # Game parameters\n",
        "        self.max_step = max_step\n",
        "        self.reward_staying_alive = reward_staying_alive\n",
        "        self.reward_ball_centered = reward_ball_centered\n",
        "        self.penalty_falling = penalty_falling\n",
        "        self.fps = fps\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "        self.player_ball_speed = player_ball_speed\n",
        "\n",
        "        self.recorder = Recorder(\"game_history_record\")\n",
        "        self.render_mode = render_mode\n",
        "        self.sound_enabled = sound_enabled\n",
        "        self.difficulty = difficulty\n",
        "\n",
        "        platform_length = int(window_x * platform_proportion)\n",
        "        self._get_x_axis_max_reward_rate(platform_length)\n",
        "\n",
        "        # Initialize physics space\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = (0, 9810)\n",
        "        self.space.damping = 0.9\n",
        "\n",
        "        self.level = get_level(level, self.space)\n",
        "        players, platforms = self.level.setup(self.window_x, self.window_y)\n",
        "        self.dynamic_body_players = []\n",
        "        self.kinematic_body_platforms = []\n",
        "        self.players_color = []\n",
        "        for player in players:\n",
        "            self.dynamic_body_players.append(player[\"body\"])\n",
        "            self.players_color.append(player[\"ball_color\"])\n",
        "        for platform in platforms:\n",
        "            self.kinematic_body_platforms.append(platform[\"body\"])\n",
        "            if platform[\"platform_shape\"] == \"rectangle\":\n",
        "                self.platform_shape = platform[\"shape\"]\n",
        "\n",
        "        self.ball_radius = player[\"ball_radius\"]\n",
        "        self.platform_length = platform[\"platform_length\"]\n",
        "\n",
        "        # Game state tracking\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "\n",
        "        # Initialize Pygame if needed\n",
        "        if self.render_mode in [\"human\", \"rgb_array\", \"rgb_array_and_human\", \"rgb_array_and_human_in_colab\"]:\n",
        "            self._setup_pygame()\n",
        "        else:\n",
        "            print(\"render_mode is not human or rgb_array, so no pygame setup.\")\n",
        "\n",
        "        # Set difficulty parameters\n",
        "        self._apply_difficulty()\n",
        "        self.capture_per_second = capture_per_second\n",
        "\n",
        "        # Create folders for captures if needed\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \".\"\n",
        "        os.makedirs(os.path.dirname(CURRENT_DIR + \"/capture/\"), exist_ok=True)\n",
        "\n",
        "    def _setup_pygame(self):\n",
        "        \"\"\"Set up PyGame for rendering\"\"\"\n",
        "        pygame.init()\n",
        "        self.frame_count = 0\n",
        "\n",
        "        if self.sound_enabled:\n",
        "            self._load_sounds()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.screen = pygame.display.set_mode((self.window_x, self.window_y))\n",
        "            pygame.display.set_caption(\"Balancing Ball - Indie Game\")\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\": # todo\n",
        "            from pymunk.pygame_util import DrawOptions\n",
        "\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))  # Create hidden surface\n",
        "\n",
        "            # Set up display in Colab\n",
        "            self.draw_options = DrawOptions(self.screen)\n",
        "            html_display = ipd.HTML('''\n",
        "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
        "                </div>\n",
        "            ''')\n",
        "            self.display_handle = display(html_display, display_id='pygame_display')\n",
        "\n",
        "            self.last_update_time = time.time()\n",
        "            self.update_interval = 1.0 / 15  # Update display at 15 FPS to avoid overwhelming Colab\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid render mode. Using headless mode.\")\n",
        "\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Create custom draw options for indie style\n",
        "\n",
        "    def _load_sounds(self):\n",
        "        \"\"\"Load game sound effects\"\"\"\n",
        "        try:\n",
        "            pygame.mixer.init()\n",
        "            self.sound_bounce = pygame.mixer.Sound(\"assets/bounce.wav\") if os.path.exists(\"assets/bounce.wav\") else None\n",
        "            self.sound_fall = pygame.mixer.Sound(\"assets/fall.wav\") if os.path.exists(\"assets/fall.wav\") else None\n",
        "        except Exception:\n",
        "            print(\"Sound loading error\")\n",
        "            self.sound_enabled = False\n",
        "            pass\n",
        "\n",
        "    def _apply_difficulty(self):\n",
        "        \"\"\"Apply difficulty settings to the game\"\"\"\n",
        "        if self.difficulty == \"easy\":\n",
        "            self.max_platform_speed = 1.5\n",
        "            self.ball_elasticity = 0.5\n",
        "        elif self.difficulty == \"medium\":\n",
        "            self.max_platform_speed = 2.5\n",
        "            self.ball_elasticity = 0.7\n",
        "        else:  # hard\n",
        "            self.max_platform_speed = 3.5\n",
        "            self.ball_elasticity = 0.9\n",
        "\n",
        "        # self.circle.shape.elasticity = self.ball_elasticity\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"Reset the game state and return the initial observation\"\"\"\n",
        "        # Reset physics objects\n",
        "        self.level.reset()\n",
        "\n",
        "        # Reset game state\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "\n",
        "        # Return initial observation\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, paction: list = []) -> Tuple[np.ndarray, float, bool, Dict]:\n",
        "        \"\"\"\n",
        "        Take a step in the game using the given action.\n",
        "\n",
        "        Args:\n",
        "            action: Float value between -1.0 and 1.0 controlling platform rotation\n",
        "\n",
        "        Returns:\n",
        "            observation: Game state observation\n",
        "            reward: Reward for this step\n",
        "            terminated: Whether episode is done\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        # Apply action to platform rotation\n",
        "\n",
        "        # self.dynamic_body_players[0].angular_velocity += p1action\n",
        "        # self.dynamic_body_players[1].angular_velocity += p1action\n",
        "        # 施加力在平台的當前位置 (質心)\n",
        "        for i in range(len(self.dynamic_body_players)):\n",
        "            force_vector = pymunk.Vec2d(paction[i], 0)\n",
        "            self.dynamic_body_players[i].apply_force_at_world_point(force_vector, self.dynamic_body_players[0].position)\n",
        "\n",
        "\n",
        "        self.level.action()\n",
        "\n",
        "        # Step the physics simulation\n",
        "        self.space.step(1/self.fps)\n",
        "\n",
        "        # Check game state\n",
        "        self.steps += 1\n",
        "        terminated = False\n",
        "\n",
        "        # Check if ball falls off screen\n",
        "        player_index = 1\n",
        "        for player in self.dynamic_body_players:\n",
        "            ball_x = player.position[0]\n",
        "            # ball_y = player.position[1]\n",
        "            if (player.position[1] > self.kinematic_body_platforms[0].position[1] or\n",
        "                ball_x < 0 or\n",
        "                ball_x > self.window_x or\n",
        "                self.steps >= self.max_step\n",
        "                ):\n",
        "\n",
        "                print(\"Score: \", self.score)\n",
        "                terminated = True\n",
        "                reward = self.penalty_falling if self.steps < self.max_step else 0\n",
        "                self.game_over = True\n",
        "                self.won_player = 1 if player_index != 1 else 2\n",
        "\n",
        "                result = {\n",
        "                    \"game_total_duration\": f\"{time.time() - self.start_time:.2f}\",\n",
        "                    \"score\": self.score,\n",
        "                }\n",
        "                self.recorder.add_no_limit(result)\n",
        "\n",
        "                if self.sound_enabled and self.sound_fall:\n",
        "                    self.sound_fall.play()\n",
        "            else:\n",
        "                player_index += 1\n",
        "\n",
        "        step_reward = self._reward_calculator(ball_x)\n",
        "        self.score += step_reward\n",
        "        # print(\"ball_x: \", ball_x, \", self.score: \", self.score)\n",
        "        return self._get_observation(), step_reward, terminated\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Convert game state to observation for RL agent\"\"\"\n",
        "        screen_data = self.render() # 获取数据\n",
        "\n",
        "        if self.capture_per_second is not None and self.frame_count % self.capture_per_second == 0:  # Every second at 60 FPS\n",
        "            pygame.image.save(self.screen, f\"capture/frame_{self.frame_count/60}.png\")\n",
        "\n",
        "        self.frame_count += 1\n",
        "        return screen_data\n",
        "\n",
        "    def render(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Render the current game state\"\"\"\n",
        "        if self.render_mode == \"headless\":\n",
        "            return None\n",
        "\n",
        "        # Clear screen with background color\n",
        "        self.screen.fill(self.BACKGROUND_COLOR)\n",
        "\n",
        "        # Custom drawing (for indie style)\n",
        "        self._draw_indie_style()\n",
        "\n",
        "\n",
        "        # Update display if in human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            # Draw game information\n",
        "            self._draw_game_info()\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.fps)\n",
        "            return None\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            # Return RGB array for gym environment\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\":\n",
        "            self.space.debug_draw(self.draw_options)\n",
        "            current_time = time.time()\n",
        "            if current_time - self.last_update_time >= self.update_interval:\n",
        "                # Convert Pygame surface to an image that can be displayed in Colab\n",
        "                buffer = BytesIO()\n",
        "                pygame.image.save(self.screen, buffer, 'PNG')\n",
        "                buffer.seek(0)\n",
        "                img_data = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "\n",
        "                # Update the HTML image\n",
        "                self.display_handle.update(ipd.HTML(f'''\n",
        "                    <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                        <img id=\"pygame-img\" src=\"data:image/png;base64,{img_data}\" style=\"width:100%;\">\n",
        "                    </div>\n",
        "                '''))\n",
        "\n",
        "                self.last_update_time = current_time\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def _draw_indie_style(self):\n",
        "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
        "        # # Draw platform with gradient and glow\n",
        "        for i in range(len(self.dynamic_body_players)):\n",
        "            ball_pos = (int(self.dynamic_body_players[i].position[0]), int(self.dynamic_body_players[i].position[1]))\n",
        "            pygame.draw.circle(self.screen, self.players_color[i], ball_pos, self.ball_radius)\n",
        "            pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
        "\n",
        "        for platform in self.kinematic_body_platforms:\n",
        "            if (platform[\"platform_shape\"] == \"rectangle\"): # TODO 變數名不清晰\n",
        "                platform_points = []\n",
        "                for v in self.platform_shape.get_vertices():\n",
        "                    x, y = v.rotated(platform.angle) + platform.position\n",
        "                    platform_points.append((int(x), int(y)))\n",
        "\n",
        "                pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
        "                pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
        "            else:  # Circle platform\n",
        "                platform_pos = (int(platform.position[0]), int(platform.position[1]))\n",
        "                pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
        "                pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
        "\n",
        "            # Draw rotation direction indicator\n",
        "            self._draw_rotation_indicator(platform_pos, self.platform_length, platform.angular_velocity, platform)\n",
        "\n",
        "    def _draw_rotation_indicator(self, position, radius, angular_velocity, body):\n",
        "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
        "        # Only draw the indicator if there's some rotation\n",
        "        if abs(angular_velocity) < 0.1:\n",
        "            return\n",
        "\n",
        "        # Calculate indicator properties based on angular velocity\n",
        "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
        "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
        "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
        "\n",
        "        # Draw arrow indicators along the platform's circumference\n",
        "        start_angle = body.angle\n",
        "\n",
        "        for i in range(num_arrows):\n",
        "            # Calculate arrow position\n",
        "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
        "\n",
        "            # Calculate arrow start and end points\n",
        "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
        "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
        "\n",
        "            # Determine arrow direction based on angular velocity\n",
        "            if angular_velocity > 0:  # Clockwise\n",
        "                arrow_end_angle = arrow_angle + 0.3\n",
        "            else:  # Counter-clockwise\n",
        "                arrow_end_angle = arrow_angle - 0.3\n",
        "\n",
        "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
        "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
        "\n",
        "            # Draw arrow line\n",
        "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
        "\n",
        "            # Draw arrowhead\n",
        "            arrowhead_size = 7\n",
        "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)\n",
        "\n",
        "    def _draw_game_info(self):\n",
        "        \"\"\"Draw game information on screen\"\"\"\n",
        "        # Create texts\n",
        "        time_text = f\"Time: {time.time() - self.start_time:.1f}\"\n",
        "        score_text = f\"Score: {self.score}\"\n",
        "\n",
        "        # Render texts\n",
        "        time_surface = self.font.render(time_text, True, (255, 255, 255))\n",
        "        score_surface = self.font.render(score_text, True, (255, 255, 255))\n",
        "\n",
        "        # Draw text backgrounds\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (5, 5, time_surface.get_width() + 10, time_surface.get_height() + 5))\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (self.window_x - score_surface.get_width() - 15, 5,\n",
        "                         score_surface.get_width() + 10, score_surface.get_height() + 5))\n",
        "\n",
        "        # Draw texts\n",
        "        self.screen.blit(time_surface, (10, 10))\n",
        "        self.screen.blit(score_surface, (self.window_x - score_surface.get_width() - 10, 10))\n",
        "\n",
        "        # Draw game over screen\n",
        "        if self.game_over:\n",
        "            game_over_text = f\"WINNER: Player {self.won_player} - Press R to restart\"\n",
        "            game_over_surface = self.font.render(game_over_text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw semi-transparent background\n",
        "            overlay = pygame.Surface((self.window_x, self.window_y), pygame.SRCALPHA)\n",
        "            overlay.fill((0, 0, 0, 128))\n",
        "            self.screen.blit(overlay, (0, 0))\n",
        "\n",
        "            # Draw text\n",
        "            self.screen.blit(game_over_surface,\n",
        "                           (self.window_x/2 - game_over_surface.get_width()/2,\n",
        "                            self.window_y/2 - game_over_surface.get_height()/2))\n",
        "\n",
        "    def _get_x_axis_max_reward_rate(self, platform_length):\n",
        "        \"\"\"\n",
        "        ((self.platform_length / 2) - 5) for calculate the distance to the\n",
        "        center of game window coordinates. The closer you are, the higher the reward.\n",
        "\n",
        "        When the ball is to be 10 points away from the center coordinates,\n",
        "        it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        \"\"\"\n",
        "        self.reward_width = (platform_length / 2) - 5\n",
        "        self.x_axis_max_reward_rate = 2 / self.reward_width\n",
        "        print(\"self.x_axis_max_reward_rate: \", self.x_axis_max_reward_rate)\n",
        "\n",
        "    def _reward_calculator(self, ball_x):\n",
        "        # score & reward\n",
        "        step_reward = 1/100\n",
        "\n",
        "        rw = abs(ball_x - self.window_x/2)\n",
        "        if rw < self.reward_width:\n",
        "            x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
        "            step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
        "\n",
        "            if self.steps % 500 == 0:\n",
        "                step_reward += self.steps/100\n",
        "                print(\"check point: \", self.steps/500)\n",
        "\n",
        "            return step_reward\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the game and clean up resources\"\"\"\n",
        "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
        "            pygame.quit()\n",
        "\n",
        "    def run_standalone(self):\n",
        "        \"\"\"Run the game in standalone mode with keyboard controls\"\"\"\n",
        "        if self.render_mode not in [\"human\", \"rgb_array_and_human_in_colab\"]:\n",
        "            raise ValueError(\"Standalone mode requires render_mode='human' or 'rgb_array_and_human_in_colab'\")\n",
        "\n",
        "        running = True\n",
        "        while running:\n",
        "            # Handle events\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.QUIT:\n",
        "                    running = False\n",
        "                elif event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_r and self.game_over:\n",
        "                        self.reset()\n",
        "\n",
        "            # Process keyboard controls\n",
        "            keys = pygame.key.get_pressed()\n",
        "            # In order to fit the model action space, the model can currently only output 0 and 1, so 2 is no action\n",
        "            paction = []\n",
        "            # Player 1 controls\n",
        "            if keys[pygame.K_LEFT]:\n",
        "                paction.append((0 - self.player_ball_speed) * 10000)\n",
        "            elif keys[pygame.K_RIGHT]:\n",
        "                paction.append(self.player_ball_speed * 10000)\n",
        "            else:\n",
        "                paction.append(0)\n",
        "\n",
        "            # Player 2 controls\n",
        "            if keys[pygame.K_a]:\n",
        "                paction.append((0 - self.player_ball_speed) * 10000)\n",
        "            elif keys[pygame.K_d]:\n",
        "                paction.append(self.player_ball_speed * 10000)\n",
        "            else:\n",
        "                paction.append(0)\n",
        "\n",
        "            # Take game step\n",
        "            if not self.game_over:\n",
        "                self.step(paction)\n",
        "\n",
        "            # Render\n",
        "            self.render()\n",
        "\n",
        "        self.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjobL-nozI81"
      },
      "source": [
        "## GYM env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MBzvHTN1zJu1"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "class BalancingBallEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for the Balancing Ball game\n",
        "    \"\"\"\n",
        "    metadata = {'render_modes': ['human', 'rgb_array', 'rgb_array_and_human_in_colab']}\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode=\"rgb_array\",\n",
        "                 difficulty=\"medium\",\n",
        "                 level=2,\n",
        "                 fps=30,\n",
        "                 obs_type=\"game_screen\",\n",
        "                 image_size=(84, 84),\n",
        "                ):\n",
        "        \"\"\"\n",
        "        render_mode: how to render the environment\n",
        "            Example: \"human\" or \"rgb_array\"\n",
        "        fps: Frames per second,\n",
        "            Example: 30\n",
        "        obs_type: type of observation\n",
        "            Example: \"game_screen\" or \"state_based\"\n",
        "        image_size: Size to resize images to (height, width)\n",
        "            Example: (84, 84) - standard for many RL implementations\n",
        "        \"\"\"\n",
        "\n",
        "        super(BalancingBallEnv, self).__init__()\n",
        "\n",
        "        # Action space: discrete - 0: left, 1: right\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Initialize game\n",
        "        self.window_x = 300\n",
        "        self.window_y = 180\n",
        "        self.platform_shape = \"circle\"\n",
        "        self.platform_proportion = 0.333\n",
        "\n",
        "        # Image preprocessing settings\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.stack_size = 3  # Number of frames to stack\n",
        "        self.observation_stack = []  # Initialize the stack\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.game = BalancingBallGame(\n",
        "            render_mode=render_mode,\n",
        "            sound_enabled=(render_mode == \"human\"),\n",
        "            difficulty=difficulty,\n",
        "            window_x = self.window_x,\n",
        "            window_y = self.window_y,\n",
        "            level = level,\n",
        "            fps = fps,\n",
        "            platform_shape = self.platform_shape,\n",
        "            platform_proportion = self.platform_proportion,\n",
        "        )\n",
        "\n",
        "        if obs_type == \"game_screen\":\n",
        "            channels = 1\n",
        "\n",
        "            # Image observation space with stacked frames\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=0, high=255,\n",
        "                shape=(self.image_size[0], self.image_size[1], channels * self.stack_size),\n",
        "                dtype=np.uint8,\n",
        "            )\n",
        "            self.step = self.step_game_screen\n",
        "            self.reset = self.reset_game_screen\n",
        "        elif obs_type == \"state_based\":\n",
        "            # State-based observation space: [ball_x, ball_y, ball_vx, ball_vy, platform_x, platform_y, platform_angular_velocity]\n",
        "            # Normalize values to be between -1 and 1\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]),\n",
        "                high=np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
        "                dtype=np.float32\n",
        "            )\n",
        "            self.step = self.step_state_based\n",
        "            self.reset = self.reset_state_based\n",
        "        else:\n",
        "            raise ValueError(\"obs_type must be 'game_screen' or 'state_based'\")\n",
        "\n",
        "        # Platform_length /= 2 when for calculate the distance to the\n",
        "        # center of game window coordinates. The closer you are, the higher the reward.\n",
        "        self.platform_reward_length = (self.game.platform_length / 2) - 5\n",
        "\n",
        "        # When the ball is to be 10 points away from the center coordinates,\n",
        "        # it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        self.x_axis_max_reward_rate = 0.5 / self.platform_reward_length\n",
        "\n",
        "    def _preprocess_observation(self, observation):\n",
        "        \"\"\"Process raw game observation for RL training\n",
        "\n",
        "        Args:\n",
        "            observation: RGB image from the game\n",
        "\n",
        "        Returns:\n",
        "            Processed observation ready for RL\n",
        "        \"\"\"\n",
        "        observation = np.transpose(observation, (1, 0, 2))\n",
        "\n",
        "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = np.expand_dims(observation, axis=-1)  # Add channel dimension back\n",
        "\n",
        "        # Resize to target size\n",
        "        if observation.shape[0] != self.image_size[0] or observation.shape[1] != self.image_size[1]:\n",
        "            # For grayscale, temporarily remove the channel dimension for cv2.resize\n",
        "            observation = cv2.resize(\n",
        "                observation.squeeze(-1),\n",
        "                (self.image_size[1], self.image_size[0]),\n",
        "                interpolation=cv2.INTER_AREA\n",
        "            )\n",
        "            observation = np.expand_dims(observation, axis=-1)  # Add channel dimension back\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step_game_screen(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Take step in the game\n",
        "        obs, step_reward, terminated = self.game.step(action)\n",
        "\n",
        "        # Preprocess the observation\n",
        "        obs = self._preprocess_observation(obs)\n",
        "\n",
        "        # Stack the frames\n",
        "        self.observation_stack.append(obs)\n",
        "        if len(self.observation_stack) > self.stack_size:\n",
        "            self.observation_stack.pop(0)  # Remove the oldest frame\n",
        "\n",
        "        # If the stack isn't full yet, pad it with the current frame\n",
        "        while len(self.observation_stack) < self.stack_size:\n",
        "            self.observation_stack.insert(0, obs)  # Pad with current frame at the beginning\n",
        "\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return stacked_obs, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset_game_screen(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        observation = self.game.reset()\n",
        "\n",
        "        # Preprocess the observation\n",
        "        observation = self._preprocess_observation(observation)\n",
        "\n",
        "        # Reset the observation stack\n",
        "        self.observation_stack = []\n",
        "\n",
        "        # Fill the stack with the initial observation\n",
        "        for _ in range(self.stack_size):\n",
        "            self.observation_stack.append(observation)\n",
        "\n",
        "        # Create stacked observation\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        info = {}\n",
        "        return stacked_obs, info\n",
        "\n",
        "    def _get_state_based_observation(self):\n",
        "        \"\"\"Convert game state to state-based observation for RL agent\"\"\"\n",
        "        # Normalize positions by window dimensions\n",
        "        ball_x = self.game.dynamic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
        "        ball_y = self.game.dynamic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
        "\n",
        "        # Normalize velocities (assuming max velocity around 1000)\n",
        "        max_velocity = 1000\n",
        "        ball_vx = np.clip(self.game.dynamic_body.velocity[0] / max_velocity, -1, 1)\n",
        "        ball_vy = np.clip(self.game.dynamic_body.velocity[1] / max_velocity, -1, 1)\n",
        "\n",
        "        # Normalize platform position\n",
        "        platform_x = self.game.kinematic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
        "        platform_y = self.game.kinematic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
        "\n",
        "        # Normalize angular velocity (assuming max around 10)\n",
        "        max_angular_velocity = 10\n",
        "        platform_angular_velocity = np.clip(self.game.kinematic_body.angular_velocity / max_angular_velocity, -1, 1)\n",
        "\n",
        "        return np.array([\n",
        "            ball_x,\n",
        "            ball_y,\n",
        "            ball_vx,\n",
        "            ball_vy,\n",
        "            platform_x,\n",
        "            platform_y,\n",
        "            platform_angular_velocity\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step_state_based(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # Take step in the game\n",
        "        _, step_reward, terminated = self.game.step(action)\n",
        "\n",
        "        # Get state-based observation\n",
        "        observation = self._get_state_based_observation()\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return observation, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset_state_based(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        self.game.reset()\n",
        "        observation = self._get_state_based_observation()\n",
        "\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment\"\"\"\n",
        "        return self.game.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        self.game.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgvyLDYKPlb"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ebr3cvEiKPlc"
      },
      "outputs": [],
      "source": [
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "def run_standalone_game(render_mode=\"human\", difficulty=\"medium\", capture_per_second=3, window_x=1000, window_y=600, level=2):\n",
        "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "\n",
        "    platform_shape = \"circle\"\n",
        "    platform_proportion = 0.333\n",
        "\n",
        "    game = BalancingBallGame(\n",
        "        render_mode = render_mode,\n",
        "        difficulty = difficulty,\n",
        "        window_x = window_x,\n",
        "        window_y = window_y,\n",
        "        platform_shape = platform_shape,\n",
        "        platform_proportion = platform_proportion,\n",
        "        level = level,\n",
        "        fps = 30,\n",
        "        capture_per_second = 3,\n",
        "    )\n",
        "\n",
        "    game.run_standalone()\n",
        "\n",
        "def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
        "    \"\"\"Test the OpenAI Gym environment\"\"\"\n",
        "    import time\n",
        "    # from gym_env import BalancingBallEnv\n",
        "\n",
        "    fps = 30\n",
        "    env = BalancingBallEnv(\n",
        "        render_mode=\"rgb_array_and_human_in_colab\",\n",
        "        difficulty=difficulty,\n",
        "        fps=fps,\n",
        "    )\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample a random action (for testing only)\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            # Render\n",
        "            env.render()\n",
        "\n",
        "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy  # MLP policy instead of CNN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "class Train:\n",
        "    def __init__(self,\n",
        "                 learning_rate=0.0003,\n",
        "                 n_steps=2048,\n",
        "                 batch_size=64,\n",
        "                 n_epochs=10,\n",
        "                 gamma=0.99,\n",
        "                 gae_lambda=0.95,\n",
        "                 ent_coef=0.01,\n",
        "                 vf_coef=0.5,\n",
        "                 max_grad_norm=0.5,\n",
        "                 policy_kwargs=None,\n",
        "                 n_envs=4,\n",
        "                 difficulty=\"medium\",\n",
        "                 level=2,\n",
        "                 load_model=None,\n",
        "                 log_dir=\"./logs/\",\n",
        "                 model_dir=\"./models/\",\n",
        "                 obs_type=\"game_screen\",\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        self.log_dir = log_dir\n",
        "        self.model_dir = model_dir\n",
        "        self.n_envs = n_envs\n",
        "        self.obs_type = obs_type\n",
        "        self.level = level\n",
        "\n",
        "        # Setup environments\n",
        "        env = make_vec_env( # rgb_array_and_human_in_colab\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=obs_type),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        self.env = env  # No need for VecTransposeImage with state-based observations\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env( # rgb_array_and_human_in_colab\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = eval_env  # No need for VecTransposeImage\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=log_dir,\n",
        "            )\n",
        "        else:\n",
        "            # hyper_param = {\n",
        "            #     'learning_rate': 0.0003,\n",
        "            #     'gamma': 0.99,\n",
        "            #     'clip_range': 0.2,\n",
        "            #     'gae_lambda': 0.95,\n",
        "            #     'ent_coef': 0.01,\n",
        "            #     'vf_coef': 0.5,\n",
        "            # }\n",
        "            hyper_param = {\n",
        "                'learning_rate': 9.610262132782771e-05,\n",
        "                'gamma': 0.9354902732764846,\n",
        "                'clip_range': 0.10781723398003445,\n",
        "                'gae_lambda': 0.7317659739640645,\n",
        "                'ent_coef': 0.04688706780862235,\n",
        "                'vf_coef': 0.9117913569235723,\n",
        "            }\n",
        "            policy_kwargs = {\n",
        "                \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "                \"net_arch\": [256, 256],  # MLP architecture\n",
        "            }\n",
        "\n",
        "            policy = ActorCriticCnnPolicy if obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "            print(\"obs type: \", self.obs_type)\n",
        "            print(\"policy: \", policy)\n",
        "            # MLP policy for state-based observations, CNN policy for image-based observations\n",
        "            self.model = PPO(\n",
        "                policy=policy,\n",
        "                env=self.env,\n",
        "                learning_rate=hyper_param[\"learning_rate\"],\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=hyper_param[\"gamma\"],\n",
        "                clip_range=hyper_param[\"clip_range\"],\n",
        "                gae_lambda=hyper_param[\"gae_lambda\"],\n",
        "                ent_coef=hyper_param[\"ent_coef\"],\n",
        "                vf_coef=hyper_param[\"vf_coef\"],\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=log_dir,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=1,\n",
        "            )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty, level=self.level, obs_type=obs_type)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self,\n",
        "                  total_timesteps=1000000,\n",
        "                  save_freq=10000,\n",
        "                  eval_freq=10000,\n",
        "                  eval_episodes=5,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball_\" + str(self.obs_type),\n",
        "        )\n",
        "\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=eval_freq // self.n_envs,\n",
        "            n_eval_episodes=eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=[checkpoint_callback, eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final_\" + str(self.obs_type))\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10, difficulty=\"medium\"):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         difficulty=args.difficulty,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#             difficulty=args.difficulty\n",
        "#         )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpEVcsjfs45Q"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "B4gwCvLVs45Q"
      },
      "outputs": [],
      "source": [
        "class Optuna_optimize:\n",
        "    def __init__(self, obs_type=\"game_screen\"):\n",
        "        self.obs_type = obs_type\n",
        "        self.env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=self.obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty, obs_type=obs_type)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def optuna_parameter_tuning(self, n_trials):\n",
        "        print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "        pruner = optuna.pruners.HyperbandPruner(\n",
        "            min_resource=100,        # 最小资源量\n",
        "            max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "            reduction_factor=3     # 折减因子 (eta)\n",
        "        )\n",
        "\n",
        "        # 建立 study 物件，並指定剪枝器\n",
        "        study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "        # 執行優化\n",
        "        try:\n",
        "            study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "            # 分析結果\n",
        "            print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "            print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "            import pandas as pd\n",
        "            df = study.trials_dataframe()\n",
        "            print(df.head())\n",
        "        finally:\n",
        "            self.env.close()\n",
        "            del self.env\n",
        "\n",
        "\n",
        "    def objective(self, trial):\n",
        "        import gc\n",
        "\n",
        "        # 1. 建議超參數\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 0.9, 0.999)\n",
        "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "        gae_lambda = trial.suggest_float('gae_lambda', 0.5, 2)\n",
        "        ent_coef = trial.suggest_float('ent_coef', 0.005, 0.05)\n",
        "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "        features_dim = trial.suggest_categorical('features_dim', [32, 64, 128, 256, 512])\n",
        "        policy_kwargs = {\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "            \"net_arch\": [256, 256],  # MLP architecture\n",
        "        }\n",
        "\n",
        "        n_steps=2048\n",
        "        batch_size=64\n",
        "        n_epochs=10\n",
        "        # gamma=0.99\n",
        "        # gae_lambda=0.95\n",
        "        # ent_coef=0.01\n",
        "        # vf_coef=0.5\n",
        "        max_grad_norm=0.5\n",
        "\n",
        "        # 2. 建立環境\n",
        "\n",
        "\n",
        "        policy = ActorCriticCnnPolicy if self.obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "        print(\"obs type: \", self.obs_type)\n",
        "        print(\"policy: \", policy)\n",
        "        # 3. 建立模型\n",
        "        model = PPO(\n",
        "                policy=policy,  # MLP policy for state-based observations\n",
        "                env=self.env,\n",
        "                learning_rate=learning_rate,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=gamma,\n",
        "                clip_range=clip_range,\n",
        "                gae_lambda=gae_lambda,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=None,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=0,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # 4. 訓練模型\n",
        "            model.learn(total_timesteps=30000)\n",
        "            # 5. 評估模型\n",
        "            mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "        finally:\n",
        "            # Always cleanup\n",
        "            del model\n",
        "            gc.collect()\n",
        "\n",
        "            if TPU_AVAILABLE:\n",
        "                import torch_xla.core.xla_model as xm\n",
        "                xm.mark_step()\n",
        "\n",
        "        return mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Ndr9hGp2CZzF",
        "outputId": "24805b6b-67f4-43fa-8fd7-5b9b914abe5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Loading model from /content/ppo_balancing_ball_game_screen_25000_steps\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Starting training...\n",
            "Logging to ./logs/PPO_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7ba6dddb8250> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7ba778cab010>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m串流輸出內容已截斷至最後 5000 行。\u001b[0m\n",
            "Score:  8.115964755003926\n",
            "Score:  17.967437066264516\n",
            "Score:  7.767765536823051\n",
            "Score:  26.162625816568625\n",
            "Score:  32.19970259350479\n",
            "Score:  12.072994127210984\n",
            "Score:  6.69362659539393\n",
            "Score:  7.251867351413152\n",
            "Score:  37.05262841551069\n",
            "Score:  90.10922839361608\n",
            "Score:  12.526311055649995\n",
            "Score:  7.677713105324747\n",
            "Score:  20.618119351331792\n",
            "Score:  19.780252278286017\n",
            "Score:  19.571909466476377\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 52.6     |\n",
            "|    ep_rew_mean     | 27.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 303      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Score:  7.593673362908703\n",
            "Score:  26.68564009228585\n",
            "Score:  26.392934545186623\n",
            "Score:  93.81343434422688\n",
            "Score:  14.000492947940385\n",
            "Score:  10.575737958499321\n",
            "Score:  19.185595343019134\n",
            "Score:  38.839141730162446\n",
            "Score:  18.20123474372213\n",
            "Score:  8.723377240523176\n",
            "Score:  12.217512021133551\n",
            "Score:  37.66536316192395\n",
            "Score:  168.35761022221686\n",
            "Score:  6.81623177337327\n",
            "Score:  214.60401097770938\n",
            "Score:  20.27464109346486\n",
            "Score:  34.713318007214795\n",
            "Score:  28.385065854513176\n",
            "Score:  15.907628099805127\n",
            "Score:  24.814229260336543\n",
            "Score:  30.771162134792277\n",
            "Score:  5.765791740254094\n",
            "Score:  12.886186599343292\n",
            "Score:  9.759971829459362\n",
            "Score:  10.032819028877341\n",
            "Score:  29.269952173347736\n",
            "Score:  38.99421529101785\n",
            "Score:  33.22158960306812\n",
            "Score:  8.620561723236298\n",
            "Score:  28.827865378741247\n",
            "Score:  37.15129133341123\n",
            "Score:  11.295703291298889\n",
            "Score:  7.835124171848449\n",
            "Score:  7.597132136281579\n",
            "Score:  34.21497481145752\n",
            "Score:  13.10016180929334\n",
            "Score:  26.745037129898666\n",
            "Score:  10.662727392050233\n",
            "Score:  115.01639349722755\n",
            "Score:  71.19097035964697\n",
            "Score:  40.72143801227348\n",
            "Score:  134.873165104715\n",
            "Eval num_timesteps=35000, episode_reward=82.26 +/- 39.23\n",
            "Episode length: 90.00 +/- 18.99\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 90           |\n",
            "|    mean_reward          | 82.3         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 35000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027712984 |\n",
            "|    clip_fraction        | 0.102        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.65        |\n",
            "|    explained_variance   | 0.91         |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 0.786        |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.0049      |\n",
            "|    value_loss           | 0.992        |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Score:  66.27250254513088\n",
            "Score:  61.32651083299603\n",
            "Score:  5.758165285821708\n",
            "Score:  18.07399360889257\n",
            "Score:  12.894461111382286\n",
            "Score:  29.231309993166832\n",
            "Score:  52.72142905804857\n",
            "Score:  26.682281514809905\n",
            "Score:  29.77962677510191\n",
            "Score:  35.04256311868439\n",
            "Score:  9.033859451535823\n",
            "Score:  7.427535705061225\n",
            "Score:  65.6358444637549\n",
            "Score:  22.106105057956167\n",
            "Score:  10.61324178542937\n",
            "Score:  61.81182389813465\n",
            "Score:  8.902331288874826\n",
            "Score:  17.782947300842018\n",
            "Score:  6.302588751742072\n",
            "Score:  45.34933835551665\n",
            "Score:  59.846807294895356\n",
            "Score:  8.150361824829552\n",
            "Score:  6.156465465005135\n",
            "Score:  64.91699409102692\n",
            "Score:  46.11297013517889\n",
            "Score:  53.17880291450427\n",
            "Score:  18.25501803601815\n",
            "Score:  41.170433696993946\n",
            "Score:  25.33629611682642\n",
            "Score:  42.478791659591494\n",
            "Score:  26.129978391594932\n",
            "Score:  32.96971864772129\n",
            "Score:  19.52646399561268\n",
            "Score:  32.18961080599804\n",
            "Score:  41.13512198664428\n",
            "Score:  76.91489141572085\n",
            "Score:  34.404874813231906\n",
            "Score:  26.71847159728893\n",
            "Score:  12.22867288253622\n",
            "Score:  31.530430380176277\n",
            "Score:  12.993092253382168\n",
            "Score:  7.401959161810285\n",
            "Score:  11.27879675531959\n",
            "Score:  81.22701502796156\n",
            "Score:  28.854225473332896\n",
            "Score:  129.429318479708\n",
            "Score:  14.272592498868503\n",
            "Score:  41.35077459784764\n",
            "Score:  29.147376531434727\n",
            "Score:  59.21346570705141\n",
            "Score:  47.56608340043751\n",
            "Score:  21.81040364080875\n",
            "Score:  6.49004259166949\n",
            "Score:  5.293714007854152\n",
            "Score:  18.781980584737653\n",
            "Score:  37.10255304790915\n",
            "Score:  46.21206508929076\n",
            "Score:  116.85590162458055\n",
            "Score:  14.793040726138193\n",
            "Score:  54.47802494505984\n",
            "Score:  168.62326540737797\n",
            "Score:  30.35511301708405\n",
            "Score:  49.26588704238948\n",
            "Score:  10.584027157960538\n",
            "Score:  12.362024900731946\n",
            "Score:  11.715604835794911\n",
            "Score:  184.91799390425118\n",
            "Score:  30.67597670445064\n",
            "Score:  19.523782327446845\n",
            "Score:  18.631086768923755\n",
            "Score:  8.783724614416592\n",
            "Score:  28.18821373554295\n",
            "Score:  26.89346861781133\n",
            "Score:  20.20931036281537\n",
            "Score:  27.22439214534851\n",
            "Score:  52.11908136364961\n",
            "Score:  29.09118698625466\n",
            "Score:  17.163513388808994\n",
            "Score:  14.481026732988267\n",
            "Score:  73.54415592429028\n",
            "Score:  103.05926822772099\n",
            "Score:  23.953114469262523\n",
            "Score:  14.730867377226929\n",
            "Score:  42.29759675264741\n",
            "Score:  8.159629152263042\n",
            "Score:  88.7068997605562\n",
            "Score:  88.7068997605562\n",
            "Eval num_timesteps=40000, episode_reward=61.86 +/- 37.97\n",
            "Episode length: 76.33 +/- 26.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 76.3     |\n",
            "|    mean_reward     | 61.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 40000    |\n",
            "---------------------------------\n",
            "Score:  16.234051607734386\n",
            "Score:  33.610700309946004\n",
            "Score:  31.913687016010503\n",
            "Score:  54.90172590974448\n",
            "Score:  6.147459562698521\n",
            "Score:  9.715457319526667\n",
            "Score:  10.012144231466786\n",
            "Score:  19.94933746614445\n",
            "Score:  20.014344103245175\n",
            "Score:  9.436386781234264\n",
            "Score:  51.986392937722606\n",
            "Score:  21.441671110839746\n",
            "Score:  6.766041384488357\n",
            "Score:  28.33559918122376\n",
            "Score:  20.400085482603885\n",
            "Score:  10.651610599876175\n",
            "Score:  12.192816773267202\n",
            "Score:  92.61117882761442\n",
            "Score:  6.518018731864526\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 57.8     |\n",
            "|    ep_rew_mean     | 34.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 296      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 138      |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Score:  13.443623715604\n",
            "Score:  7.923672007494861\n",
            "Score:  24.248219531086704\n",
            "Score:  23.24437800629745\n",
            "Score:  10.57977581940943\n",
            "Score:  31.754679506857915\n",
            "Score:  25.741829405738894\n",
            "Score:  44.418119197381856\n",
            "Score:  6.077285818013184\n",
            "Score:  7.293634558515376\n",
            "Score:  10.581187852980833\n",
            "Score:  32.43670403352874\n",
            "Score:  102.43232321499907\n",
            "Score:  6.5686994742740215\n",
            "Score:  50.79566028943906\n",
            "Score:  97.79210583008964\n",
            "Score:  143.49151409124397\n",
            "Score:  11.540406679965423\n",
            "Score:  6.783598482494666\n",
            "Score:  22.11316479468855\n",
            "Score:  46.94545205153811\n",
            "Score:  230.70818292030776\n",
            "Score:  43.25790229419718\n",
            "Score:  99.40918487834571\n",
            "Score:  66.25179568314041\n",
            "Score:  23.81899515354447\n",
            "Score:  95.7869966433936\n",
            "Score:  72.56962687720751\n",
            "Score:  7.629444520068495\n",
            "Score:  13.271931996191197\n",
            "Score:  26.168952283926643\n",
            "Score:  30.05334786513969\n",
            "Score:  25.57715888539146\n",
            "Score:  19.63065155012342\n",
            "Score:  22.709705770538424\n",
            "Score:  86.03987892292471\n",
            "Score:  30.955309477298158\n",
            "Score:  62.70657780863568\n",
            "Score:  12.964477090334023\n",
            "Score:  7.704385329228009\n",
            "Score:  33.44823287808872\n",
            "Score:  21.832608378029118\n",
            "Score:  27.939336440357085\n",
            "Score:  9.001638726530775\n",
            "Score:  48.99983647223654\n",
            "Score:  20.640295149815735\n",
            "Score:  27.07668161580427\n",
            "Score:  37.754953654531135\n",
            "Score:  186.52901827391904\n",
            "Score:  21.89489533298297\n",
            "Score:  15.375608134292966\n",
            "Score:  28.97193003911802\n",
            "Score:  82.58741724515211\n",
            "Score:  154.97326739887257\n",
            "Score:  24.10567214497765\n",
            "Score:  23.029099882167294\n",
            "Score:  37.27951992411225\n",
            "Score:  20.4441812642207\n",
            "Score:  34.526007256512536\n",
            "Score:  8.909907679200318\n",
            "Score:  34.18630861243039\n",
            "Score:  74.18799997683307\n",
            "Score:  13.882783808764124\n",
            "Score:  33.90197935349697\n",
            "Score:  7.830938601354732\n",
            "Score:  7.830938601354732\n",
            "Score:  18.036456751726266\n",
            "Eval num_timesteps=45000, episode_reward=11.23 +/- 4.81\n",
            "Episode length: 42.00 +/- 7.07\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 42          |\n",
            "|    mean_reward          | 11.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 45000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002340515 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.637      |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 0.913       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00408    |\n",
            "|    value_loss           | 1.22        |\n",
            "-----------------------------------------\n",
            "Score:  43.27589600725948\n",
            "Score:  92.59393154112828\n",
            "Score:  25.465946444914927\n",
            "Score:  255.19354448384553\n",
            "Score:  19.18831262520219\n",
            "Score:  41.36162927578111\n",
            "Score:  99.21474853660223\n",
            "Score:  18.097613975993145\n",
            "Score:  12.690266844599124\n",
            "Score:  8.326535756379378\n",
            "Score:  11.448141905438819\n",
            "Score:  13.136074511310406\n",
            "Score:  151.07343954346996\n",
            "Score:  10.81739948260326\n",
            "Score:  27.528417883065824\n",
            "Score:  16.728460621582638\n",
            "Score:  7.701853083258621\n",
            "Score:  17.269994265381825\n",
            "Score:  29.064230197019796\n",
            "Score:  93.5285709336208\n",
            "Score:  5.961235159668983\n",
            "Score:  57.640547133716\n",
            "Score:  30.50113616788807\n",
            "Score:  112.29607739128532\n",
            "Score:  16.600583183896532\n",
            "Score:  9.532617788997621\n",
            "Score:  22.325030991130937\n",
            "Score:  22.199945113395053\n",
            "Score:  70.30239565358438\n",
            "Score:  29.722832766883375\n",
            "Score:  22.846192458469826\n",
            "Score:  9.231599592156842\n",
            "Score:  107.39495471578535\n",
            "Score:  57.38556671917118\n",
            "Score:  49.90092442542163\n",
            "Score:  14.126298430623452\n",
            "Score:  29.855504843453183\n",
            "Score:  60.957758212134756\n",
            "Score:  60.83126492335455\n",
            "Score:  14.768975480933662\n",
            "Score:  7.750943823985322\n",
            "Score:  126.44471685542162\n",
            "Score:  8.579426432701416\n",
            "Score:  5.879196026480172\n",
            "Score:  8.365566458126434\n",
            "Score:  8.772067992442365\n",
            "Score:  19.02922210233821\n",
            "Score:  123.84776289162171\n",
            "Score:  25.127205297472294\n",
            "Score:  16.803019940886813\n",
            "Score:  9.645793531400463\n",
            "Score:  7.319200529655485\n",
            "Score:  17.832318759517683\n",
            "Score:  40.83074867685276\n",
            "Score:  9.071983349638076\n",
            "Score:  7.217814197751931\n",
            "Score:  49.16167722476313\n",
            "Score:  59.48775391757636\n",
            "Score:  34.16308574705053\n",
            "Score:  29.596816431027587\n",
            "Score:  17.156705473268758\n",
            "Score:  8.077907185216866\n",
            "Score:  6.399808567199661\n",
            "Score:  9.187889022364706\n",
            "Score:  20.076046353050057\n",
            "Score:  16.7391319928911\n",
            "Score:  32.78169881240536\n",
            "Score:  82.52419320322078\n",
            "Score:  25.152810179462907\n",
            "Score:  25.749040509136787\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 60.3     |\n",
            "|    ep_rew_mean     | 39       |\n",
            "| time/              |          |\n",
            "|    fps             | 294      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Score:  63.66470646100424\n",
            "Score:  48.132175965000656\n",
            "Score:  20.22300883072235\n",
            "Score:  6.973457687993854\n",
            "Score:  87.3723556310085\n",
            "Score:  213.24093409489836\n",
            "Score:  11.329602263044462\n",
            "Score:  56.75668428535595\n",
            "Score:  16.138560844821964\n",
            "Score:  118.64578709098916\n",
            "Score:  27.07666305250159\n",
            "Score:  18.972515111208207\n",
            "Score:  13.07428363499211\n",
            "Score:  62.59415441649764\n",
            "Score:  14.692256893459891\n",
            "Score:  8.455189247561583\n",
            "Eval num_timesteps=50000, episode_reward=28.58 +/- 24.19\n",
            "Episode length: 56.00 +/- 19.61\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 56           |\n",
            "|    mean_reward          | 28.6         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028029291 |\n",
            "|    clip_fraction        | 0.0977       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.627       |\n",
            "|    explained_variance   | 0.908        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 1.28         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00389     |\n",
            "|    value_loss           | 1.55         |\n",
            "------------------------------------------\n",
            "Score:  8.38897620209922\n",
            "Score:  78.38905273602782\n",
            "Score:  5.712514975076915\n",
            "Score:  5.491875925996615\n",
            "Score:  30.591495468388594\n",
            "Score:  26.660185192755513\n",
            "Score:  13.768870555565803\n",
            "Score:  17.090666818866364\n",
            "Score:  185.7707953064586\n",
            "Score:  26.482799067366948\n",
            "Score:  43.42978005177936\n",
            "Score:  7.656505772150977\n",
            "Score:  224.14582240311248\n",
            "Score:  22.898699329387934\n",
            "Score:  14.40347253390926\n",
            "Score:  25.96878203836348\n",
            "Score:  24.638619333123277\n",
            "Score:  73.50516563922069\n",
            "Score:  117.7486328915534\n",
            "Score:  156.57606056692777\n",
            "Score:  8.36883004011337\n",
            "Score:  70.90030830386691\n",
            "Score:  27.73657833035498\n",
            "Score:  12.383502510479836\n",
            "Score:  25.240912236796422\n",
            "Score:  60.40546596979456\n",
            "Score:  17.156831373220292\n",
            "Score:  8.304731339708667\n",
            "Score:  10.30909192177165\n",
            "Score:  88.56311373858493\n",
            "Score:  40.37598339173126\n",
            "Score:  43.70615151398827\n",
            "Score:  7.1541732838414545\n",
            "Score:  60.12900604983253\n",
            "Score:  82.57941090922546\n",
            "Score:  9.031033893564654\n",
            "Score:  8.047028288930624\n",
            "Score:  69.56822680720741\n",
            "Score:  17.61085944999813\n",
            "Score:  34.78528861961576\n",
            "Score:  104.08002281698941\n",
            "Score:  13.090844910409723\n",
            "Score:  17.26082462155425\n",
            "Score:  44.61631451302968\n",
            "Score:  80.4788783649859\n",
            "Score:  89.20204509795143\n",
            "Score:  19.202395159803597\n",
            "Score:  10.779759273810862\n",
            "Score:  41.8951398497134\n",
            "Score:  21.6674965982821\n",
            "Score:  15.072746453935887\n",
            "Score:  33.89065090942636\n",
            "Score:  92.50139344772381\n",
            "Score:  28.65295298520721\n",
            "Score:  13.84350066687345\n",
            "Score:  26.804470200294954\n",
            "Score:  85.70525993531477\n",
            "Score:  88.89495024322132\n",
            "Score:  5.975575094615466\n",
            "Score:  10.271112130658798\n",
            "Score:  19.81079646207937\n",
            "Score:  8.79880194927025\n",
            "Score:  7.367377112135061\n",
            "Score:  18.806831036674556\n",
            "Score:  40.484939982527536\n",
            "Score:  65.62871623006441\n",
            "Score:  7.827807388370898\n",
            "Score:  14.99293652802412\n",
            "Score:  13.693166551112078\n",
            "Score:  44.72195498900354\n",
            "Score:  14.225625799766418\n",
            "Score:  6.609338225771916\n",
            "Score:  25.252347059479295\n",
            "Score:  67.76693633900103\n",
            "Score:  10.831871938290757\n",
            "Score:  14.608279686582616\n",
            "Score:  44.76457535949573\n",
            "Score:  102.81985882506189\n",
            "Score:  24.526904077898156\n",
            "Score:  33.571851813884116\n",
            "Score:  13.40861627520321\n",
            "Score:  6.550284367424024\n",
            "Score:  36.1342761109216\n",
            "Score:  14.698235893390569\n",
            "Score:  8.159629152263042\n",
            "Score:  7.844064323435008\n",
            "Eval num_timesteps=55000, episode_reward=10.23 +/- 3.16\n",
            "Episode length: 42.00 +/- 5.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 42       |\n",
            "|    mean_reward     | 10.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 55000    |\n",
            "---------------------------------\n",
            "Score:  7.134879266133558\n",
            "Score:  5.957730431901013\n",
            "Score:  32.54247287287631\n",
            "Score:  51.57735616482061\n",
            "Score:  36.043727729358054\n",
            "Score:  13.93170411100436\n",
            "Score:  62.59340319202835\n",
            "Score:  113.17170880723333\n",
            "Score:  46.032239413041836\n",
            "Score:  21.09096147608236\n",
            "Score:  77.28516387030876\n",
            "Score:  148.2783987385156\n",
            "Score:  13.95299796741058\n",
            "Score:  48.225628114728515\n",
            "Score:  59.46424943491101\n",
            "Score:  49.975181275443354\n",
            "Score:  27.540284935540505\n",
            "Score:  24.617270199052456\n",
            "Score:  99.1912003153401\n",
            "Score:  8.859430080921948\n",
            "Score:  42.19686411819552\n",
            "Score:  80.15415880191377\n",
            "Score:  9.365518299657209\n",
            "Score:  13.741441607421965\n",
            "Score:  29.094140451233518\n",
            "Score:  18.582277060788645\n",
            "Score:  62.179491169590065\n",
            "Score:  83.54279703550957\n",
            "Score:  5.024245598390213\n",
            "Score:  9.141996951378271\n",
            "Score:  18.002460838998566\n",
            "Score:  8.211932464533561\n",
            "Score:  44.61925749472427\n",
            "Score:  43.734985541132396\n",
            "Score:  9.903417203835394\n",
            "Score:  10.30036744353617\n",
            "Score:  44.60498100821787\n",
            "Score:  79.31805661363619\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 59.9     |\n",
            "|    ep_rew_mean     | 37.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 292      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 196      |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Score:  6.651353945748898\n",
            "Score:  75.96286319515218\n",
            "Score:  22.327409681613094\n",
            "Score:  33.47165108383161\n",
            "Score:  21.201771798172924\n",
            "Score:  16.97038533451524\n",
            "Score:  17.75950481349596\n",
            "Score:  9.022949068871785\n",
            "Score:  26.454949192855974\n",
            "Score:  46.64028332186611\n",
            "Score:  98.74833197305527\n",
            "Score:  17.704967670513483\n",
            "Score:  5.702139970672701\n",
            "Score:  42.253611853303624\n",
            "Score:  7.0455860031762665\n",
            "Score:  7.805279434517532\n",
            "Score:  14.423116707703828\n",
            "Score:  20.58933114429707\n",
            "Score:  8.351066103760475\n",
            "Score:  93.42211097311936\n",
            "Score:  55.41396695397507\n",
            "Score:  30.284468143846112\n",
            "Score:  9.11504712655061\n",
            "Score:  50.34308886863117\n",
            "Score:  7.3644737844313966\n",
            "Score:  18.36932079090807\n",
            "Score:  28.083184328802805\n",
            "Score:  19.132591399635967\n",
            "Score:  165.09188535091238\n",
            "Score:  16.671839362758124\n",
            "Score:  10.034960570622058\n",
            "Score:  9.120232055866893\n",
            "Score:  54.38747659927388\n",
            "Score:  26.369095268555824\n",
            "Score:  98.78438278554374\n",
            "Score:  57.004971052367694\n",
            "Score:  77.62602076615583\n",
            "Score:  18.086967388921344\n",
            "Score:  47.317539827638264\n",
            "Score:  35.900715882959936\n",
            "Score:  41.35677214009021\n",
            "Score:  42.39339257542072\n",
            "Score:  11.084646535375576\n",
            "Score:  76.15032610101062\n",
            "Score:  108.24657367897414\n",
            "Score:  49.85043458052843\n",
            "Score:  40.05568937775585\n",
            "Eval num_timesteps=60000, episode_reward=66.05 +/- 30.10\n",
            "Episode length: 83.67 +/- 15.92\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 83.7         |\n",
            "|    mean_reward          | 66.1         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 60000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041003963 |\n",
            "|    clip_fraction        | 0.132        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.604       |\n",
            "|    explained_variance   | 0.92         |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 0.666        |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00615     |\n",
            "|    value_loss           | 1.47         |\n",
            "------------------------------------------\n",
            "Score:  35.28531270128946\n",
            "Score:  19.741206562396982\n",
            "Score:  10.951986788805836\n",
            "Score:  47.961715519890014\n",
            "Score:  45.121686333145796\n",
            "Score:  9.190603154428143\n",
            "Score:  59.81893681173126\n",
            "Score:  34.913205860375896\n",
            "Score:  47.44264354160028\n",
            "Score:  33.076856260152574\n",
            "Score:  14.175015985006262\n",
            "Score:  14.769993390235621\n",
            "Score:  28.90572973808295\n",
            "Score:  31.425689719922403\n",
            "Score:  49.47221363832824\n",
            "Score:  444.76922335501666\n",
            "Score:  59.429595501238154\n",
            "Score:  23.7255746191396\n",
            "Score:  8.555029967399074\n",
            "Score:  164.69808935431945\n",
            "Score:  51.013926746797836\n",
            "Score:  32.02586745879966\n",
            "Score:  74.15665679356503\n",
            "Score:  12.845659997486337\n",
            "Score:  10.613136149309499\n",
            "Score:  43.17560538797774\n",
            "Score:  260.84237344180286\n",
            "Score:  20.18488301142894\n",
            "Score:  213.3342510773807\n",
            "Score:  25.634924480495158\n",
            "Score:  184.2538911702714\n",
            "Score:  24.88598592092166\n",
            "Score:  14.925678687639609\n",
            "Score:  249.6185719304717\n",
            "Score:  8.366979211537998\n",
            "Score:  277.56374448539714\n",
            "Score:  41.700258866067216\n",
            "Score:  43.88795307918946\n",
            "Score:  6.641730771499494\n",
            "Score:  73.07494020363686\n",
            "Score:  68.90198173337986\n",
            "Score:  204.69545731024553\n",
            "Score:  19.665058438094892\n",
            "Score:  36.11867158223362\n",
            "Score:  49.34415093041979\n",
            "Score:  12.245748863918116\n",
            "Score:  19.158929145853264\n",
            "Score:  38.35756989363147\n",
            "Score:  88.58075255045496\n",
            "Score:  26.458068882273526\n",
            "Score:  47.57772142970457\n",
            "Score:  11.660660508506117\n",
            "Score:  14.089280913957001\n",
            "Score:  7.2729193380903565\n",
            "Score:  29.660888464840934\n",
            "Score:  10.716199756339503\n",
            "Score:  26.158476330198184\n",
            "Score:  166.31927086130742\n",
            "Score:  51.723197329112736\n",
            "Score:  37.24143200004243\n",
            "Score:  236.0436203885356\n",
            "Score:  30.555810331092527\n",
            "Score:  9.162475247648768\n",
            "Score:  65.05349008906764\n",
            "Score:  15.357179204647313\n",
            "Score:  57.21822174204906\n",
            "Score:  17.174122297691834\n",
            "Score:  12.703804876682184\n",
            "Score:  16.211471186515116\n",
            "Score:  61.72510808032017\n",
            "Score:  29.569851949618645\n",
            "Score:  33.528906231212886\n",
            "Score:  54.40485648049699\n",
            "Score:  79.33624557309588\n",
            "Score:  40.05568937775585\n",
            "Eval num_timesteps=65000, episode_reward=57.93 +/- 16.23\n",
            "Episode length: 81.33 +/- 9.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 81.3     |\n",
            "|    mean_reward     | 57.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 65000    |\n",
            "---------------------------------\n",
            "Score:  70.72190282078064\n",
            "Score:  11.977783789179073\n",
            "Score:  11.185975736355536\n",
            "Score:  80.74742322846744\n",
            "Score:  72.68041750321102\n",
            "Score:  86.8887116176646\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 68.8     |\n",
            "|    ep_rew_mean     | 56.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 290      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 225      |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Score:  44.45461668400946\n",
            "Score:  76.18326295736446\n",
            "Score:  14.083848230651268\n",
            "Score:  84.88327994717434\n",
            "Score:  8.860832021978135\n",
            "Score:  38.74357347209292\n",
            "Score:  26.235358208922495\n",
            "Score:  52.56420529040223\n",
            "Score:  44.80676416703575\n",
            "Score:  65.56251116803291\n",
            "Score:  147.43365297960747\n",
            "Score:  6.611507366164239\n",
            "Score:  32.07173146981359\n",
            "Score:  72.60256831184974\n",
            "Score:  144.3431602841752\n",
            "Score:  77.72903302075252\n",
            "Score:  65.77732297695249\n",
            "Score:  7.255902045115989\n",
            "Score:  27.288402267130728\n",
            "Score:  182.03998814620445\n",
            "Score:  61.074530057090406\n",
            "Score:  73.7113453544744\n",
            "Score:  25.96750983489896\n",
            "Score:  29.83219843398655\n",
            "Score:  215.8882126232267\n",
            "Score:  49.46666711659248\n",
            "Score:  39.136545662366906\n",
            "Score:  62.70758184016886\n",
            "Score:  69.08347419115601\n",
            "Score:  18.901265827772917\n",
            "Score:  27.799662524245136\n",
            "Score:  14.135600953402665\n",
            "Score:  6.333644388628611\n",
            "Score:  5.272992734073441\n",
            "Score:  146.36242231601986\n",
            "Score:  149.03736020571597\n",
            "Score:  26.61724574021254\n",
            "Score:  40.11222346784016\n",
            "Score:  94.34200441905237\n",
            "Score:  19.375856700958728\n",
            "Score:  21.30068309076506\n",
            "Score:  58.40150533647241\n",
            "Score:  17.31328508859998\n",
            "Score:  45.67081965635991\n",
            "Score:  136.2427091119644\n",
            "Score:  27.32730593550663\n",
            "Score:  17.198546083265104\n",
            "Score:  38.908272658542025\n",
            "Score:  88.3682600977894\n",
            "Score:  28.601903172640043\n",
            "Score:  4.847934127923033\n",
            "Score:  9.00761509004298\n",
            "Score:  15.57676090544628\n",
            "Score:  34.31674452618099\n",
            "Score:  13.321761946248968\n",
            "Score:  21.751848744792973\n",
            "Score:  105.32483516857232\n",
            "Score:  41.72230401289848\n",
            "Score:  133.42138992385944\n",
            "Score:  150.4109460864487\n",
            "Score:  9.414888377585479\n",
            "Score:  33.56657716886097\n",
            "Score:  21.15933886301163\n",
            "Score:  28.957097622161978\n",
            "Score:  10.453137085955355\n",
            "Score:  8.99738452478589\n",
            "Score:  140.35464024533755\n",
            "Score:  84.94454767465287\n",
            "Score:  115.33216976427424\n",
            "Eval num_timesteps=70000, episode_reward=113.54 +/- 22.66\n",
            "Episode length: 107.00 +/- 10.71\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 107          |\n",
            "|    mean_reward          | 114          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 70000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021654768 |\n",
            "|    clip_fraction        | 0.0835       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.593       |\n",
            "|    explained_variance   | 0.883        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 3.08         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00219     |\n",
            "|    value_loss           | 2.81         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Score:  26.499126712732057\n",
            "Score:  53.02846588753822\n",
            "Score:  155.11845212215133\n",
            "Score:  196.28037548573468\n",
            "Score:  69.3790803153907\n",
            "Score:  183.83327226563424\n",
            "Score:  168.64281591577767\n",
            "Score:  30.03024795692698\n",
            "Score:  34.90357042433645\n",
            "Score:  53.171776641648165\n",
            "Score:  55.298049086404184\n",
            "Score:  56.563861325434715\n",
            "Score:  157.0949547477587\n",
            "Score:  116.70642610204696\n",
            "Score:  25.106919757473307\n",
            "Score:  33.25176313629229\n",
            "Score:  70.39756879235077\n",
            "Score:  35.83515200292236\n",
            "Score:  460.5275747891792\n",
            "Score:  32.101454418287126\n",
            "Score:  11.846482671369598\n",
            "Score:  247.66118527775703\n",
            "Score:  256.88895278467203\n",
            "Score:  27.00976754269415\n",
            "Score:  53.08385918472899\n",
            "Score:  69.56024621801352\n",
            "Score:  133.91062891557684\n",
            "Score:  80.67279114272739\n",
            "Score:  77.44570880859105\n",
            "Score:  28.911261625289036\n",
            "Score:  74.41719592832065\n",
            "Score:  46.874641063643395\n",
            "Score:  54.421863621130505\n",
            "Score:  173.76271808585474\n",
            "Score:  6.663409250347319\n",
            "Score:  38.64861669788781\n",
            "Score:  25.376213109460483\n",
            "Score:  37.64646719938845\n",
            "Score:  33.35608680149085\n",
            "Score:  16.464997069824022\n",
            "Score:  45.13453859377423\n",
            "Score:  9.155522324910974\n",
            "Score:  30.761865806729123\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 75.3     |\n",
            "|    ep_rew_mean     | 67.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 288      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 255      |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Score:  138.63885985448613\n",
            "Score:  75.83168735301618\n",
            "Score:  73.985324280401\n",
            "Score:  30.853075156285712\n",
            "Score:  31.348368733848954\n",
            "Score:  26.443445008820884\n",
            "Score:  44.441990559864244\n",
            "Score:  30.298485971404812\n",
            "Score:  8.422843220355128\n",
            "Score:  36.04774008795988\n",
            "Score:  46.80298402477764\n",
            "Score:  66.32324620549376\n",
            "Score:  13.613352267348478\n",
            "Score:  267.9175204178655\n",
            "Score:  106.30697107731974\n",
            "Score:  53.12328281501162\n",
            "Score:  11.18232522508966\n",
            "Score:  110.68404983953823\n",
            "Score:  13.697572397843324\n",
            "Score:  47.12165547465625\n",
            "Score:  14.182626767554465\n",
            "Eval num_timesteps=75000, episode_reward=25.00 +/- 15.64\n",
            "Episode length: 56.33 +/- 13.91\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 56.3         |\n",
            "|    mean_reward          | 25           |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 75000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023343125 |\n",
            "|    clip_fraction        | 0.1          |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.566       |\n",
            "|    explained_variance   | 0.886        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 1.45         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00212     |\n",
            "|    value_loss           | 2.71         |\n",
            "------------------------------------------\n",
            "Score:  31.424520345175566\n",
            "Score:  33.34623742266561\n",
            "Score:  53.72250463424532\n",
            "Score:  13.900906726491032\n",
            "Score:  48.54395120952016\n",
            "Score:  121.23843453145446\n",
            "Score:  114.41112247852598\n",
            "Score:  34.65422131936141\n",
            "Score:  8.591193834844276\n",
            "Score:  199.68082186891453\n",
            "Score:  60.794771779359685\n",
            "Score:  130.19720063038818\n",
            "Score:  113.45719603453406\n",
            "Score:  25.734934533852016\n",
            "Score:  38.612941263474475\n",
            "Score:  54.67466547330305\n",
            "Score:  10.081781152411343\n",
            "Score:  49.71060779350221\n",
            "Score:  375.6646320990602\n",
            "Score:  125.39841437647259\n",
            "Score:  27.653284729332146\n",
            "Score:  10.323813169227597\n",
            "Score:  13.561248110379422\n",
            "Score:  10.844201590017063\n",
            "Score:  231.24458817884826\n",
            "Score:  149.71335643262262\n",
            "Score:  83.84897878725113\n",
            "Score:  22.72760570757809\n",
            "Score:  12.845948208478342\n",
            "Score:  63.83869774861757\n",
            "Score:  159.47462745148363\n",
            "Score:  67.11489411485624\n",
            "Score:  66.20343514769775\n",
            "Score:  16.532266611249764\n",
            "Score:  9.807033345468524\n",
            "Score:  42.80377972430508\n",
            "Score:  167.58549944351438\n",
            "Score:  25.64613890293268\n",
            "Score:  11.211530922455019\n",
            "Score:  55.550352742323504\n",
            "Score:  10.5449291882242\n",
            "Score:  45.996607097119885\n",
            "Score:  26.550012745447244\n",
            "Score:  7.56430829804906\n",
            "Score:  222.80626797360938\n",
            "Score:  22.276038140172997\n",
            "Score:  145.91946289552322\n",
            "Score:  8.736077298165846\n",
            "Score:  11.133832106863215\n",
            "Score:  133.92889529068952\n",
            "Score:  86.70732621137235\n",
            "Score:  12.314095893197365\n",
            "Score:  33.95751618721156\n",
            "Score:  91.19502078603058\n",
            "Score:  195.92744253952486\n",
            "Score:  83.55647987053231\n",
            "Score:  42.6475630915949\n",
            "Score:  70.04397518548711\n",
            "Score:  30.150470609020715\n",
            "Score:  131.86211139341899\n",
            "Score:  8.262839836550974\n",
            "Score:  24.824693091166292\n",
            "Score:  52.71439120081654\n",
            "Score:  16.480310799734465\n",
            "Score:  18.741237744421515\n",
            "Score:  41.24346516941112\n",
            "Score:  146.02140928588085\n",
            "Score:  150.70014141671092\n",
            "Score:  88.59727593386795\n",
            "Eval num_timesteps=80000, episode_reward=128.44 +/- 28.24\n",
            "Episode length: 113.67 +/- 11.09\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 114      |\n",
            "|    mean_reward     | 128      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 80000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Score:  44.95179536664558\n",
            "Score:  73.67697913226313\n",
            "Score:  41.88952569112399\n",
            "Score:  8.736577143698362\n",
            "Score:  49.28358519108343\n",
            "Score:  110.18184517390524\n",
            "Score:  57.599601947658684\n",
            "Score:  11.466378773106973\n",
            "Score:  18.65482737273318\n",
            "Score:  43.40599727348594\n",
            "Score:  489.11870415808886\n",
            "Score:  31.65375365686987\n",
            "Score:  28.14735903943131\n",
            "Score:  12.625434035146116\n",
            "Score:  106.08412900565237\n",
            "Score:  230.11101659978974\n",
            "Score:  124.2281227738132\n",
            "Score:  11.818040844914057\n",
            "Score:  18.414826455520796\n",
            "Score:  112.26899318162772\n",
            "Score:  259.57148121775884\n",
            "Score:  22.726176135278646\n",
            "Score:  48.41002580727576\n",
            "Score:  79.9015431686652\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 76.9     |\n",
            "|    ep_rew_mean     | 72.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 287      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 285      |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Score:  187.2858588356013\n",
            "Score:  19.988339388191797\n",
            "Score:  63.780287281978566\n",
            "Score:  27.567947157276354\n",
            "Score:  41.22694234198141\n",
            "Score:  49.70229139888508\n",
            "Score:  6.607095568116594\n",
            "Score:  27.333465077048725\n",
            "Score:  23.764911996778213\n",
            "Score:  8.046146275352815\n",
            "Score:  6.663804465148457\n",
            "Score:  40.277108280693284\n",
            "Score:  46.647929518880055\n",
            "Score:  21.727124642559367\n",
            "Score:  51.43728415073498\n",
            "Score:  42.051368262252296\n",
            "Score:  545.9851889189291\n",
            "Score:  356.1903143430896\n",
            "Score:  68.66085309216935\n",
            "Score:  124.42230816495169\n",
            "Score:  106.46990699797306\n",
            "Score:  169.02707808441494\n",
            "Score:  60.52394209156907\n",
            "Score:  96.41579910469474\n",
            "Score:  22.284410124533103\n",
            "Score:  12.955419875855892\n",
            "Score:  29.086054635027136\n",
            "Score:  19.22480374176582\n",
            "Score:  132.92730601052662\n",
            "Score:  37.37638815521315\n",
            "Score:  49.45050509948645\n",
            "Score:  13.33806898900793\n",
            "Score:  151.7354078130491\n",
            "Score:  196.95991665876576\n",
            "Score:  151.11985373769423\n",
            "Score:  281.4938868911093\n",
            "Score:  133.45502813302954\n",
            "Score:  40.046450206697905\n",
            "Score:  115.33216976427424\n",
            "Score:  14.674393710167962\n",
            "Eval num_timesteps=85000, episode_reward=56.68 +/- 42.74\n",
            "Episode length: 75.33 +/- 26.11\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 75.3         |\n",
            "|    mean_reward          | 56.7         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 85000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017850008 |\n",
            "|    clip_fraction        | 0.0813       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.56        |\n",
            "|    explained_variance   | 0.885        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 2.37         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00204     |\n",
            "|    value_loss           | 3.18         |\n",
            "------------------------------------------\n",
            "Score:  50.00714129267616\n",
            "Score:  40.75985408666706\n",
            "Score:  15.954246046915495\n",
            "Score:  153.1702847319484\n",
            "Score:  82.92605727518892\n",
            "Score:  205.79148017954174\n",
            "Score:  56.91235513839335\n",
            "Score:  6.09695409694061\n",
            "Score:  12.65010914252889\n",
            "Score:  58.44391526062021\n",
            "Score:  49.448562469288504\n",
            "Score:  10.828949475111852\n",
            "Score:  28.516092110682816\n",
            "Score:  186.50161068483538\n",
            "Score:  23.155631544969797\n",
            "Score:  48.35403185643658\n",
            "Score:  7.176500700296861\n",
            "Score:  80.01652634489781\n",
            "Score:  8.316999267220389\n",
            "Score:  48.96155294637303\n",
            "Score:  101.29279365300174\n",
            "Score:  6.550872976469022\n",
            "Score:  77.39590962289077\n",
            "Score:  30.07625692873134\n",
            "Score:  185.27979789853077\n",
            "Score:  16.74280321156788\n",
            "Score:  42.041219184309924\n",
            "Score:  8.159096391269724\n",
            "Score:  20.231422576045055\n",
            "Score:  126.06489944107227\n",
            "Score:  50.455205230703505\n",
            "Score:  44.82999718010384\n",
            "Score:  30.392626231133534\n",
            "Score:  60.18872108838012\n",
            "Score:  47.811984441817806\n",
            "Score:  40.25214485784865\n",
            "Score:  85.71135638180247\n",
            "Score:  196.25230643400045\n",
            "Score:  147.8312383978734\n",
            "Score:  24.890955541904706\n",
            "Score:  41.25587367608422\n",
            "Score:  59.060017409089596\n",
            "Score:  217.750075246965\n",
            "Score:  8.888538875569\n",
            "Score:  71.44818235593226\n",
            "Score:  16.643608582974505\n",
            "Score:  234.40618936208358\n",
            "Score:  39.20667734289211\n",
            "Score:  155.31801018394646\n",
            "Score:  162.29393833275867\n",
            "Score:  17.71574255919093\n",
            "Score:  48.720868695941775\n",
            "Score:  73.42804391481134\n",
            "Score:  19.51439949019012\n",
            "Score:  245.0217306646409\n",
            "Score:  43.15415643833159\n",
            "Score:  315.0125190990816\n",
            "Score:  63.02639700637984\n",
            "Score:  89.37600428695414\n",
            "Score:  54.885516017787545\n",
            "Score:  38.80408615815992\n",
            "Score:  40.05568937775585\n",
            "Score:  252.37109476682238\n",
            "Score:  14.182626767554465\n",
            "Eval num_timesteps=90000, episode_reward=102.20 +/- 106.71\n",
            "Episode length: 90.33 +/- 46.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.3     |\n",
            "|    mean_reward     | 102      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90000    |\n",
            "---------------------------------\n",
            "Score:  152.84508226041507\n",
            "Score:  11.2247233208206\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 81.2     |\n",
            "|    ep_rew_mean     | 81.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 285      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 315      |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Score:  401.8240173250091\n",
            "Score:  7.634475884825114\n",
            "Score:  9.506747265107375\n",
            "Score:  231.62168524380078\n",
            "Score:  73.84826476509399\n",
            "Score:  335.853871352415\n",
            "Score:  81.39629627342589\n",
            "Score:  839.6675634964338\n",
            "Score:  59.09000560738769\n",
            "Score:  47.015525500817695\n",
            "Score:  18.792535622526064\n",
            "Score:  241.15509728462507\n",
            "Score:  38.32641115673524\n",
            "Score:  100.02467933009973\n",
            "Score:  205.09827929758225\n",
            "Score:  110.43233301505568\n",
            "Score:  75.0456979177321\n",
            "Score:  36.97118710917612\n",
            "Score:  14.168597040390289\n",
            "Score:  10.382882012190716\n",
            "Score:  8.520461831564072\n",
            "Score:  450.6853120306733\n",
            "Score:  418.05786315943124\n",
            "Score:  70.97473773896448\n",
            "Score:  33.19496672956075\n",
            "Score:  16.376694828685086\n",
            "Score:  90.17066823225973\n",
            "Score:  43.240753358752826\n",
            "Score:  66.93497534319579\n",
            "Score:  14.882903257374853\n",
            "Score:  28.06081097632216\n",
            "Score:  32.57375764384354\n",
            "Score:  226.0045711178031\n",
            "Score:  82.08732186659464\n",
            "Score:  91.51926768080756\n",
            "Score:  49.12990517692079\n",
            "Score:  32.91882588490529\n",
            "Score:  13.293320075093142\n",
            "Score:  51.56968274981959\n",
            "Score:  167.2488270276667\n",
            "Score:  99.41687462701621\n",
            "Score:  174.3909140322406\n",
            "Score:  7.8245586189356775\n",
            "Score:  116.18498704998173\n",
            "Score:  301.81994405601296\n",
            "Score:  13.547385573003215\n",
            "Score:  93.64521298243282\n",
            "Score:  143.0870168951948\n",
            "Score:  32.51586009720117\n",
            "Score:  184.14911240944608\n",
            "Score:  41.09856997581294\n",
            "Score:  48.60422119769964\n",
            "Score:  39.425082429863885\n",
            "Score:  11.237928946617503\n",
            "Score:  68.69054532325023\n",
            "Score:  14.223567001748417\n",
            "Score:  112.05554431424862\n",
            "Score:  121.07933120232893\n",
            "Eval num_timesteps=95000, episode_reward=82.45 +/- 48.39\n",
            "Episode length: 87.00 +/- 29.02\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 87           |\n",
            "|    mean_reward          | 82.5         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 95000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020501926 |\n",
            "|    clip_fraction        | 0.0943       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.553       |\n",
            "|    explained_variance   | 0.884        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 3.58         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00177     |\n",
            "|    value_loss           | 3.76         |\n",
            "------------------------------------------\n",
            "Score:  64.15696624305762\n",
            "Score:  21.371174658970503\n",
            "Score:  121.80282841436288\n",
            "Score:  55.269790110975606\n",
            "Score:  14.964043301086134\n",
            "Score:  80.19763212200782\n",
            "Score:  17.969824121812618\n",
            "Score:  60.90710272590485\n",
            "Score:  33.13972210523675\n",
            "Score:  22.460574119800352\n",
            "Score:  91.17453556262619\n",
            "Score:  459.1951435431936\n",
            "Score:  160.78654985369747\n",
            "Score:  162.14393771393014\n",
            "Score:  26.72197286280016\n",
            "Score:  41.005348952021656\n",
            "Score:  20.630935805618737\n",
            "Score:  27.460883316866187\n",
            "Score:  87.35429965589884\n",
            "Score:  289.26850023587207\n",
            "Score:  53.06572703660283\n",
            "Score:  49.508154644872796\n",
            "Score:  91.48509478907643\n",
            "Score:  349.7553359681195\n",
            "Score:  37.774810322961166\n",
            "Score:  82.70443759589772\n",
            "Score:  51.903058954426264\n",
            "Score:  11.83887394676222\n",
            "Score:  7.704428898526377\n",
            "Score:  12.742216778383128\n",
            "Score:  10.96838907096083\n",
            "Score:  87.2442099999164\n",
            "Score:  9.813602434095493\n",
            "Score:  46.98041137059058\n",
            "Score:  35.132578324877535\n",
            "Score:  11.784707049505538\n",
            "Score:  117.41956518957375\n",
            "Score:  295.1237807547752\n",
            "Score:  79.98830451309685\n",
            "Score:  101.47917998138138\n",
            "Score:  68.84351231987681\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 86.7     |\n",
            "|    ep_rew_mean     | 100      |\n",
            "| time/              |          |\n",
            "|    fps             | 285      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 344      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Score:  7.860404583061795\n",
            "Score:  38.20644616404401\n",
            "Score:  17.342782263613334\n",
            "Score:  51.88697303029167\n",
            "Score:  50.56620386083614\n",
            "Score:  18.85439847926805\n",
            "Score:  86.67334690619782\n",
            "Score:  52.8952641990191\n",
            "Score:  7.8728034288279325\n",
            "Score:  167.4224263897988\n",
            "Score:  132.31280692069885\n",
            "Score:  65.46003337229895\n",
            "Score:  71.39842204502175\n",
            "Score:  17.483961040154156\n",
            "Score:  48.395079293375964\n",
            "Score:  8.227771052079325\n",
            "Score:  51.115233462999306\n",
            "Score:  95.51568479440486\n",
            "Score:  90.67849134737016\n",
            "Score:  291.6488204425823\n",
            "Score:  103.74975886299356\n",
            "Score:  172.80823495460143\n",
            "Score:  8.155222173475646\n",
            "Eval num_timesteps=100000, episode_reward=94.90 +/- 67.51\n",
            "Episode length: 89.67 +/- 38.51\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 89.7         |\n",
            "|    mean_reward          | 94.9         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 100000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021161586 |\n",
            "|    clip_fraction        | 0.0812       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.547       |\n",
            "|    explained_variance   | 0.855        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 6.35         |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00123     |\n",
            "|    value_loss           | 5.85         |\n",
            "------------------------------------------\n",
            "Score:  75.58372223782669\n",
            "Score:  76.45760508623536\n",
            "Score:  104.02165387007373\n",
            "Score:  26.10522755581656\n",
            "Score:  71.18574418996573\n",
            "Score:  73.76701300305334\n",
            "Score:  175.68524663484334\n",
            "Score:  63.02140702624145\n",
            "Score:  270.0247285673639\n",
            "Score:  89.81068170411818\n",
            "Score:  88.09280106897263\n",
            "Score:  13.20388975998911\n",
            "Score:  267.6845768175786\n",
            "Score:  340.8479750680346\n",
            "Score:  208.25011698321177\n",
            "Score:  1223.4767823619125\n",
            "Score:  42.412933172551604\n",
            "Score:  37.01011381613197\n",
            "Score:  71.57120905580818\n",
            "Score:  133.0568917848146\n",
            "Score:  7.1282621811879165\n",
            "Score:  9.993756417209806\n",
            "Score:  25.298285633143525\n",
            "Score:  71.1244916153163\n",
            "Score:  6.440838951162617\n",
            "Score:  95.83837783258342\n",
            "Score:  338.4257948185864\n",
            "Score:  21.421509702019804\n",
            "Score:  156.17691865364452\n",
            "Score:  39.2953424799285\n",
            "Score:  48.605267208154025\n",
            "Score:  63.680689655682286\n",
            "Score:  101.90325989131446\n",
            "Score:  50.013956928598\n",
            "Score:  298.55842248540944\n",
            "Score:  110.96117075589295\n",
            "Score:  50.72075277406097\n",
            "Score:  80.83883885717717\n",
            "Score:  114.2888599972169\n",
            "Score:  354.64391460654565\n",
            "Score:  106.3534210317748\n",
            "Score:  51.50081509997257\n",
            "Score:  102.9916416258426\n",
            "Score:  13.534063261021721\n",
            "Score:  2979.29931719862\n",
            "Score:  13.035947373453256\n",
            "Score:  78.84096760766032\n",
            "Score:  88.47770226846946\n",
            "Score:  28.453606987179146\n",
            "Score:  79.3523836940609\n",
            "Score:  400.24243477382294\n",
            "Score:  8.159629152263038\n",
            "Eval num_timesteps=105000, episode_reward=162.58 +/- 170.54\n",
            "Episode length: 104.67 +/- 60.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 105      |\n",
            "|    mean_reward     | 163      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 105000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Score:  41.48102764014782\n",
            "Score:  109.34773923504841\n",
            "Score:  83.7816451061263\n",
            "Score:  89.57120635698357\n",
            "Score:  10.639840492124147\n",
            "Score:  55.58374525999712\n",
            "Score:  231.27738018258677\n",
            "Score:  215.44938199906554\n",
            "Score:  48.81757159092385\n",
            "Score:  76.42361915045946\n",
            "Score:  103.75187445709776\n",
            "Score:  122.41303645340253\n",
            "Score:  23.15286636865103\n",
            "Score:  104.04499139022299\n",
            "Score:  137.31057107392374\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 91.4     |\n",
            "|    ep_rew_mean     | 128      |\n",
            "| time/              |          |\n",
            "|    fps             | 284      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 374      |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n",
            "Score:  869.2708820196771\n",
            "Score:  181.40304155820328\n",
            "Score:  442.04562798483573\n",
            "Score:  14.167545677167901\n",
            "Score:  36.26174314776334\n",
            "Score:  338.0866769420889\n",
            "Score:  25.49165672334951\n",
            "Score:  258.466636224937\n",
            "Score:  1388.1468396814594\n",
            "Score:  29.325100192310437\n",
            "Score:  9.99274787880843\n",
            "Score:  188.83776407764742\n",
            "Score:  72.17476485175553\n",
            "Score:  225.4818582254931\n",
            "Score:  114.2090287132696\n",
            "Score:  40.3568356845041\n",
            "Score:  21.951366255704293\n",
            "Score:  411.4713625546045\n",
            "Score:  14.695436674805531\n",
            "Score:  136.83312320259571\n",
            "Score:  424.5572664421941\n",
            "Score:  292.6785680173769\n",
            "Score:  30.674261630317083\n",
            "Score:  11.455330136924356\n",
            "Score:  135.5854766108305\n",
            "Score:  167.63848475039165\n",
            "Score:  61.11808143295363\n",
            "Score:  8.133775103451432\n",
            "Score:  80.89055471014193\n",
            "Score:  16.290472621412004\n",
            "Score:  8.420907867564347\n",
            "Score:  16.215828841974446\n",
            "Score:  8.155222173475646\n",
            "Score:  8.159629152263038\n",
            "Score:  307.5880210414153\n",
            "Eval num_timesteps=110000, episode_reward=107.97 +/- 141.15\n",
            "Episode length: 80.33 +/- 60.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 80.3        |\n",
            "|    mean_reward          | 108         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 110000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001899352 |\n",
            "|    clip_fraction        | 0.0922      |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.538      |\n",
            "|    explained_variance   | 0.638       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 20.6        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.000575   |\n",
            "|    value_loss           | 22.7        |\n",
            "-----------------------------------------\n",
            "Score:  118.64589055581743\n",
            "Score:  327.04255910622203\n",
            "Score:  55.49037122812983\n",
            "Score:  44.72265310045844\n",
            "Score:  656.5791170590348\n",
            "Score:  177.4994359928018\n",
            "Score:  25.77468695830904\n",
            "Score:  257.73742210245183\n",
            "Score:  94.8577569116625\n",
            "Score:  62.603755399223054\n",
            "Score:  74.03300815918402\n",
            "Score:  11.553706114829737\n",
            "Score:  198.89770349537332\n",
            "Score:  29.967356006339095\n",
            "Score:  61.19020029416073\n",
            "Score:  11.279181996642498\n",
            "Score:  28.257122337722123\n",
            "Score:  78.47527850565824\n",
            "Score:  237.90552656748892\n",
            "Score:  32.741336625956805\n",
            "Score:  288.590836801484\n",
            "Score:  106.67731612593388\n",
            "Score:  4.808755707040712\n",
            "Score:  274.2119253175821\n",
            "Score:  17.5350543693889\n",
            "Score:  334.09370610332263\n",
            "Score:  45.34399864869599\n",
            "Score:  34.49097868948792\n",
            "Score:  39.66547365259433\n",
            "Score:  39.79681223513695\n",
            "Score:  18.573564128105684\n",
            "Score:  480.73337799344426\n",
            "Score:  108.8535250435654\n",
            "Score:  142.8257724581713\n",
            "Score:  70.81513156879822\n",
            "Score:  12.740921212701075\n",
            "Score:  21.136512933400663\n",
            "Score:  162.38931920503023\n",
            "Score:  6.553390059602654\n",
            "Score:  19.355015191617515\n",
            "Score:  173.50296447780804\n",
            "Score:  110.65740197843894\n",
            "Score:  6.558144755330191\n",
            "Score:  23.70532502072844\n",
            "Score:  119.40150702342515\n",
            "Score:  48.18185540131387\n",
            "Score:  34.21622346978163\n",
            "Score:  242.65390402470072\n",
            "Score:  345.19903269408377\n",
            "Score:  37.37307851577501\n",
            "Score:  11.023026105975053\n",
            "Score:  77.71342618348827\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 96.8     |\n",
            "|    ep_rew_mean     | 136      |\n",
            "| time/              |          |\n",
            "|    fps             | 284      |\n",
            "|    iterations      | 14       |\n",
            "|    time_elapsed    | 403      |\n",
            "|    total_timesteps | 114688   |\n",
            "---------------------------------\n",
            "Score:  41.843646576065254\n",
            "Score:  117.73500002294375\n",
            "Score:  5.988671803130294\n",
            "Score:  8.155222173475646\n",
            "Score:  7.852784068221043\n",
            "Score:  8.155222173475646\n",
            "Eval num_timesteps=115000, episode_reward=8.05 +/- 0.14\n",
            "Episode length: 37.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 37           |\n",
            "|    mean_reward          | 8.05         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 115000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017942847 |\n",
            "|    clip_fraction        | 0.0811       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.522       |\n",
            "|    explained_variance   | 0.807        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 4.95         |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | 0.000237     |\n",
            "|    value_loss           | 10.1         |\n",
            "------------------------------------------\n",
            "Score:  581.9179529327076\n",
            "Score:  66.56030608886176\n",
            "Score:  589.5832120796944\n",
            "Score:  242.8030710681103\n",
            "Score:  15.287331748465409\n",
            "Score:  11.405994293150945\n",
            "Score:  23.841965364332363\n",
            "Score:  112.90020809283193\n",
            "Score:  312.6637310603074\n",
            "Score:  88.53412306475371\n",
            "Score:  588.4069931599468\n",
            "Score:  1261.171905237043\n",
            "Score:  62.308344033413206\n",
            "Score:  257.1816831561138\n",
            "Score:  222.53995616017784\n",
            "Score:  630.6958813939676\n",
            "Score:  209.33273724816019\n",
            "Score:  65.46044143778194\n",
            "Score:  10.448118353416687\n",
            "Score:  11.229140899288767\n",
            "Score:  18.500418371066946\n",
            "Score:  39.23001324104188\n",
            "Score:  40.943641970055936\n",
            "Score:  177.05212365802953\n",
            "Score:  898.9396451404276\n",
            "Score:  6.9662199542556005\n",
            "Score:  63.20421611132797\n",
            "Score:  8.135767484658718\n",
            "Score:  131.11372351588216\n",
            "Score:  18.467298999640267\n",
            "Score:  48.23149034565659\n",
            "Score:  395.309064910819\n",
            "Score:  24.053899194377337\n",
            "Score:  523.6885334265417\n",
            "Score:  86.00174179286432\n",
            "Score:  329.8065550288452\n",
            "Score:  12.275220809844152\n",
            "Score:  272.2533464722315\n",
            "Score:  47.01736179838894\n",
            "Score:  571.4229690365835\n",
            "Score:  539.7019845094761\n",
            "Score:  8.155222173475646\n",
            "Score:  123.84791371682122\n",
            "Score:  7.852784068221043\n",
            "Eval num_timesteps=120000, episode_reward=46.62 +/- 54.61\n",
            "Episode length: 61.67 +/- 34.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61.7     |\n",
            "|    mean_reward     | 46.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 120000   |\n",
            "---------------------------------\n",
            "Score:  1341.253969892357\n",
            "Score:  53.57091546660711\n",
            "Score:  168.46094816073162\n",
            "Score:  72.09957880758353\n",
            "Score:  30.823886683738426\n",
            "Score:  406.3029850226028\n",
            "Score:  423.3671034976091\n",
            "Score:  483.97525347922385\n",
            "Score:  179.86296368277584\n",
            "Score:  21.972006998463005\n",
            "Score:  727.503364102314\n",
            "Score:  1162.6726355689045\n",
            "Score:  40.58196140139399\n",
            "Score:  218.19563204212866\n",
            "Score:  553.6116206907506\n",
            "Score:  13.459340873921294\n",
            "Score:  191.5036907003311\n",
            "Score:  133.2232562254359\n",
            "Score:  7.766871242707835\n",
            "Score:  17.59583159401825\n",
            "Score:  39.04085776492809\n",
            "Score:  35.03976873716262\n",
            "Score:  156.8245583497593\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 111      |\n",
            "|    ep_rew_mean     | 198      |\n",
            "| time/              |          |\n",
            "|    fps             | 284      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 432      |\n",
            "|    total_timesteps | 122880   |\n",
            "---------------------------------\n",
            "Score:  146.6237141159324\n",
            "Score:  35.88287631040727\n",
            "Score:  60.0143820439607\n",
            "Score:  475.16381705136916\n",
            "Score:  84.52832129678448\n",
            "Score:  710.6459563899517\n",
            "Score:  36.02714775456821\n",
            "Score:  180.57496471996203\n",
            "Score:  189.19611549361656\n",
            "Score:  57.95652702072988\n",
            "Score:  58.75222538237696\n",
            "Score:  17.1426825029276\n",
            "Score:  16.374026625759623\n",
            "Score:  58.56952013694053\n",
            "Score:  20.155087259009576\n",
            "Score:  794.2531536187581\n",
            "Score:  185.98770492097472\n",
            "Score:  34.40668504183872\n",
            "Score:  89.66697239043334\n",
            "Score:  140.68681509324657\n",
            "Score:  14.674393710167962\n",
            "Score:  194.29393128495462\n",
            "Eval num_timesteps=125000, episode_reward=116.55 +/- 75.29\n",
            "Episode length: 100.67 +/- 38.58\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 101          |\n",
            "|    mean_reward          | 117          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 125000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023597442 |\n",
            "|    clip_fraction        | 0.104        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.527       |\n",
            "|    explained_variance   | 0.758        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 11.2         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | 0.00104      |\n",
            "|    value_loss           | 16.2         |\n",
            "------------------------------------------\n",
            "Score:  261.4621473293323\n",
            "Score:  187.4600332852654\n",
            "Score:  44.2292924904059\n",
            "Score:  31.795236064866117\n",
            "Score:  227.34825847536067\n",
            "Score:  627.0776952287454\n",
            "Score:  22.1351071113688\n",
            "Score:  117.81221108536673\n",
            "Score:  275.7938094450549\n",
            "Score:  270.20487664082395\n",
            "Score:  58.52848510223056\n",
            "Score:  78.63855547958812\n",
            "Score:  48.083424697261385\n",
            "Score:  83.25515766260182\n",
            "Score:  147.18238079555837\n",
            "Score:  101.19994171144334\n",
            "Score:  235.4220685387798\n",
            "Score:  151.9303409826083\n",
            "Score:  11.41258534307227\n",
            "Score:  23.79901646780915\n",
            "Score:  104.5960133374123\n",
            "Score:  6.605165770654675\n",
            "Score:  17.986112604540413\n",
            "Score:  144.55779483459492\n",
            "Score:  33.84006158066467\n",
            "Score:  167.98158723412772\n",
            "Score:  123.12376777889662\n",
            "Score:  49.189234864705234\n",
            "Score:  11.965509752948678\n",
            "Score:  171.31097365404608\n",
            "Score:  7.1796232941101135\n",
            "Score:  197.46099933541794\n",
            "Score:  176.68129290067637\n",
            "Score:  12.895622301213088\n",
            "Score:  30.255820818106034\n",
            "Score:  156.29050056252498\n",
            "Score:  335.46616594356783\n",
            "Score:  85.28660926733409\n",
            "Score:  19.19017278338115\n",
            "Score:  80.82613100992532\n",
            "Score:  20.610275815755976\n",
            "Score:  42.0466841232278\n",
            "Score:  575.1053891196228\n",
            "Score:  26.452155027301465\n",
            "Score:  282.73781907293585\n",
            "Score:  119.83604368406824\n",
            "Score:  8.544758724608936\n",
            "Score:  104.97341306252903\n",
            "Score:  8.675436908419952\n",
            "Score:  181.77501122197958\n",
            "Score:  13.080278622474832\n",
            "Score:  83.56821355274937\n",
            "Score:  872.3767130054696\n",
            "Score:  88.14726787737774\n",
            "Score:  228.9421128121536\n",
            "Score:  525.9547394326393\n",
            "Eval num_timesteps=130000, episode_reward=281.01 +/- 182.49\n",
            "Episode length: 152.33 +/- 49.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 152      |\n",
            "|    mean_reward     | 281      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 130000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Score:  9.470092262101407\n",
            "Score:  41.339980414069124\n",
            "Score:  417.1803345414168\n",
            "Score:  38.14952654080077\n",
            "Score:  175.46672930747476\n",
            "Score:  84.7875788699215\n",
            "Score:  121.24697325298374\n",
            "Score:  15.387104318074558\n",
            "Score:  7.568751002883425\n",
            "Score:  151.29645165895118\n",
            "Score:  167.67174120821642\n",
            "Score:  72.57022789469566\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 103      |\n",
            "|    ep_rew_mean     | 158      |\n",
            "| time/              |          |\n",
            "|    fps             | 283      |\n",
            "|    iterations      | 16       |\n",
            "|    time_elapsed    | 463      |\n",
            "|    total_timesteps | 131072   |\n",
            "---------------------------------\n",
            "Score:  40.393967252193264\n",
            "Score:  68.05090767374185\n",
            "Score:  203.4521514235766\n",
            "Score:  79.64524157051834\n",
            "Score:  26.107735835259458\n",
            "Score:  250.6453149695229\n",
            "Score:  37.39960137651276\n",
            "Score:  176.1241415255931\n",
            "Score:  63.82369680283403\n",
            "Score:  39.227078611051176\n",
            "Score:  227.8503098346237\n",
            "Score:  101.99642830517557\n",
            "Score:  145.4102764315568\n",
            "Score:  40.31106819047023\n",
            "Score:  51.1434092987951\n",
            "Score:  66.4850821209124\n",
            "Score:  10.700266208922526\n",
            "Score:  6.407014202903488\n",
            "Score:  56.50155324658399\n",
            "Score:  53.16227339737385\n",
            "Score:  90.6420260127452\n",
            "Score:  92.40928556033346\n",
            "Score:  10.009599627182952\n",
            "Score:  24.13428934826268\n",
            "Score:  7.436484468319617\n",
            "Score:  153.274848274202\n",
            "Score:  43.88749796387042\n",
            "Score:  60.87677967915166\n",
            "Score:  11.803623647007168\n",
            "Score:  39.95509592278095\n",
            "Score:  9.864149276264596\n",
            "Score:  76.16556663052518\n",
            "Score:  732.1291540215066\n",
            "Score:  329.77686397285146\n",
            "Score:  77.14628693346123\n",
            "Score:  51.06598955428486\n",
            "Score:  347.1619758388382\n",
            "Score:  27.758747877537854\n",
            "Score:  51.38356594598234\n",
            "Score:  72.6797285124896\n",
            "Score:  76.81951221311994\n",
            "Score:  214.96120475576632\n",
            "Score:  8.159629152263038\n",
            "Score:  8.159629152263042\n",
            "Eval num_timesteps=135000, episode_reward=77.09 +/- 97.49\n",
            "Episode length: 73.00 +/- 49.50\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 73           |\n",
            "|    mean_reward          | 77.1         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 135000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023568242 |\n",
            "|    clip_fraction        | 0.0897       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.534       |\n",
            "|    explained_variance   | 0.869        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 6.37         |\n",
            "|    n_updates            | 190          |\n",
            "|    policy_gradient_loss | 0.000608     |\n",
            "|    value_loss           | 8.95         |\n",
            "------------------------------------------\n",
            "Score:  468.9690846096543\n",
            "Score:  269.5540702735646\n",
            "Score:  218.3005936556829\n",
            "Score:  1245.835743429965\n",
            "Score:  142.48448912807453\n",
            "Score:  49.97723738460279\n",
            "Score:  41.348260062723504\n",
            "Score:  56.81641100968448\n",
            "Score:  227.48342638579615\n",
            "Score:  47.40886791351163\n",
            "Score:  39.61719314467733\n",
            "Score:  58.13786336901161\n",
            "Score:  360.0295160542912\n",
            "Score:  10.511130025022366\n",
            "Score:  1210.7692233963373\n",
            "Score:  28.74823947954406\n",
            "Score:  69.66687195410807\n",
            "Score:  25.841419020541725\n",
            "Score:  81.18319495874164\n",
            "Score:  155.3002772514219\n",
            "Score:  227.22742915428682\n",
            "Score:  140.00477325442716\n",
            "Score:  114.98583033669262\n",
            "Score:  8.644448430174279\n",
            "Score:  92.97151387104883\n",
            "Score:  14.027883781938547\n",
            "Score:  64.82802894316532\n",
            "Score:  36.309361094909235\n",
            "Score:  15.187277285090332\n",
            "Score:  289.2126519427187\n",
            "Score:  37.5741119447617\n",
            "Score:  17.031231653443538\n",
            "Score:  49.19762378631586\n",
            "Score:  69.26659898000683\n",
            "Score:  161.02311806584288\n",
            "Score:  238.828033719314\n",
            "Score:  102.73636036421254\n",
            "Score:  7.254956825414134\n",
            "Score:  37.1885949890924\n",
            "Score:  12.879719499399526\n",
            "Score:  19.02730068558205\n",
            "Score:  263.5312283444794\n",
            "Score:  112.72776341636114\n",
            "Score:  112.8676545665908\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 95.7     |\n",
            "|    ep_rew_mean     | 135      |\n",
            "| time/              |          |\n",
            "|    fps             | 282      |\n",
            "|    iterations      | 17       |\n",
            "|    time_elapsed    | 492      |\n",
            "|    total_timesteps | 139264   |\n",
            "---------------------------------\n",
            "Score:  217.71721591848038\n",
            "Score:  52.01628025640366\n",
            "Score:  106.94259835417215\n",
            "Score:  9.986039232039156\n",
            "Score:  337.0432556533035\n",
            "Score:  271.05979866451867\n",
            "Score:  381.46163686197525\n",
            "Score:  1290.3302557593202\n",
            "Score:  396.7845673449934\n",
            "Eval num_timesteps=140000, episode_reward=689.53 +/- 424.88\n",
            "Episode length: 235.67 +/- 63.17\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 236          |\n",
            "|    mean_reward          | 690          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 140000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020098453 |\n",
            "|    clip_fraction        | 0.0833       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.519       |\n",
            "|    explained_variance   | 0.846        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 4.25         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.000394    |\n",
            "|    value_loss           | 10.1         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Score:  102.08501218016197\n",
            "Score:  480.82307165382053\n",
            "Score:  70.78736398470305\n",
            "Score:  638.7984113632095\n",
            "Score:  35.953601491646225\n",
            "Score:  136.21289161069691\n",
            "Score:  46.75554784605114\n",
            "Score:  122.24046203528992\n",
            "Score:  50.476614668541664\n",
            "Score:  44.54630212848908\n",
            "Score:  182.9979907901349\n",
            "Score:  854.772233094288\n",
            "Score:  89.1105465466444\n",
            "Score:  79.00817011121963\n",
            "Score:  51.25930612017119\n",
            "Score:  259.8816172277823\n",
            "Score:  10.931387178762424\n",
            "Score:  62.12524159468731\n",
            "Score:  11.087780710503754\n",
            "Score:  7.622940606781211\n",
            "Score:  18.035775591859807\n",
            "Score:  9.3306885880909\n",
            "Score:  22.61796519368585\n",
            "Score:  154.18015717302634\n",
            "Score:  133.7549141588157\n",
            "Score:  327.09243142262767\n",
            "Score:  52.02707025040946\n",
            "Score:  19.209959235029334\n",
            "Score:  58.5026265048365\n",
            "Score:  6.191249300776351\n",
            "Score:  44.062998643964804\n",
            "Score:  162.39079358313802\n",
            "Score:  9.589475812881105\n",
            "Score:  461.45444705231233\n",
            "Score:  287.4838737206015\n",
            "Score:  177.19638591682374\n",
            "Score:  49.4199543644936\n",
            "Score:  222.11169991732825\n",
            "Score:  16.651954303798878\n",
            "Score:  43.2730116323227\n",
            "Score:  426.9233708875855\n",
            "Score:  41.22628050098136\n",
            "Score:  660.0770065358283\n",
            "Score:  35.71573986059397\n",
            "Score:  461.28017371181807\n",
            "Score:  34.49045394948231\n",
            "Score:  78.28157255594324\n",
            "Score:  53.466747968332335\n",
            "Score:  144.74929231642255\n",
            "Score:  46.30789678868013\n",
            "Score:  181.85447555038232\n",
            "Score:  304.7462890560919\n",
            "Score:  213.5384525726167\n",
            "Eval num_timesteps=145000, episode_reward=233.38 +/- 52.10\n",
            "Episode length: 146.00 +/- 14.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 146      |\n",
            "|    mean_reward     | 233      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 145000   |\n",
            "---------------------------------\n",
            "Score:  79.46427719070721\n",
            "Score:  97.87227758292715\n",
            "Score:  38.84311723411593\n",
            "Score:  277.7299042736523\n",
            "Score:  505.6050087508799\n",
            "Score:  52.3652505106475\n",
            "Score:  87.47399576290006\n",
            "Score:  11.248331705605073\n",
            "Score:  21.054482866997418\n",
            "Score:  12.815370415240329\n",
            "Score:  20.323957367199913\n",
            "Score:  42.94198315989056\n",
            "Score:  11.437195473658516\n",
            "Score:  30.632324052098607\n",
            "Score:  35.593035144088795\n",
            "Score:  850.3850783993433\n",
            "Score:  899.7801147073784\n",
            "Score:  87.3973593414162\n",
            "Score:  124.13452852671328\n",
            "Score:  28.69786934268057\n",
            "Score:  62.05810040578739\n",
            "Score:  192.37116693086355\n",
            "Score:  45.2680941341868\n",
            "Score:  248.4845746064165\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 98.4     |\n",
            "|    ep_rew_mean     | 142      |\n",
            "| time/              |          |\n",
            "|    fps             | 281      |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 524      |\n",
            "|    total_timesteps | 147456   |\n",
            "---------------------------------\n",
            "Score:  33.87392997542295\n",
            "Score:  916.3124014082803\n",
            "Score:  105.21182814920915\n",
            "Score:  137.94933399514898\n",
            "Score:  10.537398618036294\n",
            "Score:  134.65518947722418\n",
            "Score:  47.978714533370905\n",
            "Score:  74.13048795944223\n",
            "Score:  67.77552354932642\n",
            "Score:  276.56834436304587\n",
            "Score:  12.311716435064353\n",
            "Score:  403.1627128720376\n",
            "Score:  62.56483308580511\n",
            "Score:  246.8935262922152\n",
            "Score:  1305.5286600052125\n",
            "Score:  70.15527339443459\n",
            "Score:  10.75511400147437\n",
            "Score:  7.514642527026752\n",
            "Score:  1325.882504579905\n",
            "Score:  410.4801807885999\n",
            "Score:  33.040699978761275\n",
            "Score:  290.7825904110741\n",
            "Score:  355.63083240295964\n",
            "Score:  487.4312746016905\n",
            "Eval num_timesteps=150000, episode_reward=377.95 +/- 81.82\n",
            "Episode length: 184.33 +/- 18.62\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 184          |\n",
            "|    mean_reward          | 378          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 150000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023400902 |\n",
            "|    clip_fraction        | 0.0703       |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.526       |\n",
            "|    explained_variance   | 0.849        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 10.9         |\n",
            "|    n_updates            | 210          |\n",
            "|    policy_gradient_loss | -0.000725    |\n",
            "|    value_loss           | 11.2         |\n",
            "------------------------------------------\n",
            "Score:  68.24642636148326\n",
            "Score:  61.19243311776406\n",
            "Score:  24.80840801238427\n",
            "Score:  39.52781209534055\n",
            "Score:  823.4588851508845\n",
            "Score:  29.24778998916742\n",
            "Score:  457.3859253608884\n",
            "Score:  42.88240881518781\n",
            "Score:  7.875693171494144\n",
            "Score:  66.39751467039544\n",
            "Score:  227.60723386559744\n",
            "Score:  759.8102975546515\n",
            "Score:  508.9558673164373\n",
            "Score:  2016.893350070417\n",
            "Score:  11.491805842124624\n",
            "Score:  95.71516514235078\n",
            "Score:  159.66827213178516\n",
            "Score:  141.48726282344197\n",
            "Score:  47.83332217895458\n",
            "Score:  19.736360543570836\n",
            "Score:  62.15061859328967\n",
            "Score:  211.36953211969774\n",
            "Score:  350.41703597537503\n",
            "Score:  36.54156800598972\n",
            "Score:  76.62532286866737\n",
            "Score:  96.54550712792656\n",
            "Score:  43.462510796509854\n",
            "Score:  1062.900941733536\n",
            "Score:  56.88407644634748\n",
            "Score:  127.4376344679646\n",
            "Score:  106.12404914110304\n",
            "Score:  54.2495142949449\n",
            "Score:  165.2527578273655\n",
            "Score:  346.1039603277967\n",
            "Score:  12.301480266093643\n",
            "Score:  16.87027461065566\n",
            "Score:  44.599933599704514\n",
            "Score:  2971.2541628390723\n",
            "Score:  183.06648777239897\n",
            "Score:  10.158835061117598\n",
            "Score:  112.40625036727326\n",
            "Score:  8.155222173475646\n",
            "Score:  322.1122772465281\n",
            "Score:  455.1797548135672\n",
            "Eval num_timesteps=155000, episode_reward=261.82 +/- 187.41\n",
            "Episode length: 135.00 +/- 70.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 135      |\n",
            "|    mean_reward     | 262      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 155000   |\n",
            "---------------------------------\n",
            "Score:  55.57668234609682\n",
            "Score:  6.8761335092433535\n",
            "Score:  98.95056890021155\n",
            "Score:  297.0182806947735\n",
            "Score:  153.65517810069764\n",
            "Score:  899.5388091282126\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 117      |\n",
            "|    ep_rew_mean     | 243      |\n",
            "| time/              |          |\n",
            "|    fps             | 280      |\n",
            "|    iterations      | 19       |\n",
            "|    time_elapsed    | 555      |\n",
            "|    total_timesteps | 155648   |\n",
            "---------------------------------\n",
            "Score:  31.24772815277779\n",
            "Score:  12.645160955713642\n",
            "Score:  214.96969081354516\n",
            "Score:  501.8369745309781\n",
            "Score:  34.47924839283438\n",
            "Score:  234.76584795691062\n",
            "Score:  876.9306377431558\n",
            "Score:  18.910256435704472\n",
            "Score:  69.5637666423261\n",
            "Score:  108.78889126919397\n",
            "Score:  160.37769678933833\n",
            "Score:  94.24354502118591\n",
            "Score:  12.91711799938698\n",
            "Score:  114.68275954257135\n",
            "Score:  6.671816327007628\n",
            "Score:  198.28976477635777\n",
            "Score:  40.2932449739263\n",
            "Score:  227.55282141898746\n",
            "Score:  64.12739081276159\n",
            "Score:  13.381519742934984\n",
            "Score:  37.62232630845475\n",
            "Score:  336.5201246353929\n",
            "Score:  7.569903764143487\n",
            "Score:  168.44893287717093\n",
            "Score:  724.3525568831052\n",
            "Score:  10.333284783385631\n",
            "Score:  8.340847229555703\n",
            "Score:  13.645489445916443\n",
            "Score:  199.96249523208456\n",
            "Score:  521.9486019399907\n",
            "Score:  111.96298349068292\n",
            "Score:  186.8579590980567\n",
            "Score:  60.648727745350286\n",
            "Score:  264.26402858897814\n",
            "Score:  40.89697567943032\n",
            "Score:  11.836436751066953\n",
            "Score:  212.6340656626574\n",
            "Score:  289.3654554646773\n",
            "Score:  12.09638311318243\n",
            "Score:  366.6249000031757\n",
            "Score:  8.155222173475646\n",
            "Score:  103.74975886299356\n",
            "Score:  205.71323546389456\n",
            "Eval num_timesteps=160000, episode_reward=105.87 +/- 80.67\n",
            "Episode length: 94.67 +/- 43.77\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 94.7         |\n",
            "|    mean_reward          | 106          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 160000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023583875 |\n",
            "|    clip_fraction        | 0.101        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.51        |\n",
            "|    explained_variance   | 0.703        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 31.5         |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | 0.000405     |\n",
            "|    value_loss           | 34.6         |\n",
            "------------------------------------------\n",
            "Score:  7.653887908009992\n",
            "Score:  71.16597741225793\n",
            "Score:  170.33489255246008\n",
            "Score:  25.24504821751555\n",
            "Score:  36.8371551938975\n",
            "Score:  1180.2663299809537\n",
            "Score:  116.9237634282012\n",
            "Score:  193.52210864551455\n",
            "Score:  34.16566693152648\n",
            "Score:  55.91755435588439\n",
            "Score:  150.97517042560534\n",
            "Score:  6.261175832141234\n",
            "Score:  284.63462771319087\n",
            "Score:  68.63884002100241\n",
            "Score:  290.26374470046824\n",
            "Score:  1387.9231070279707\n",
            "Score:  1917.8478192896644\n",
            "Score:  13.759685149149089\n",
            "Score:  20.083723189295647\n",
            "Score:  145.2053855896056\n",
            "Score:  123.79981364079629\n",
            "Score:  154.093178318699\n",
            "Score:  38.72991774774591\n",
            "Score:  51.66759314719052\n",
            "Score:  554.2879008637577\n",
            "Score:  332.16125282818075\n",
            "Score:  90.76962666881144\n",
            "Score:  459.344307349394\n",
            "Score:  676.0005871619445\n",
            "Score:  60.596780404476895\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | 232      |\n",
            "| time/              |          |\n",
            "|    fps             | 280      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 584      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "Score:  80.27916536489641\n",
            "Score:  10.3297939117921\n",
            "Score:  598.4427570763756\n",
            "Score:  6.021752858543935\n",
            "Score:  517.4289142281729\n",
            "Score:  14.152129668060974\n",
            "Score:  1605.0072891640016\n",
            "Score:  185.86606986575677\n",
            "Score:  35.091622976625274\n",
            "Score:  1840.4853035956712\n",
            "Score:  14.582865067519869\n",
            "check point:  1.0\n",
            "Score:  5160.24618268234\n",
            "Eval num_timesteps=165000, episode_reward=2338.44 +/- 2130.01\n",
            "Episode length: 355.00 +/- 239.49\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 355         |\n",
            "|    mean_reward          | 2.34e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 165000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002851055 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.521      |\n",
            "|    explained_variance   | 0.803       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | 0.000332    |\n",
            "|    value_loss           | 18.1        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Score:  124.79923342768626\n",
            "Score:  57.59802512107167\n",
            "Score:  67.86583701606219\n",
            "Score:  350.398912284351\n",
            "Score:  148.36921305488107\n",
            "Score:  36.82591715828649\n",
            "Score:  423.2766812525118\n",
            "Score:  388.3368581604382\n",
            "Score:  19.77953373304071\n",
            "Score:  189.03340482576743\n",
            "Score:  265.3670079308235\n",
            "Score:  52.13914114155012\n",
            "Score:  207.83083521161365\n",
            "Score:  1277.8591161258769\n",
            "Score:  215.8471883611216\n",
            "Score:  12.715703829262171\n",
            "Score:  32.12650374994522\n",
            "Score:  138.86116893387708\n",
            "Score:  22.970251683690268\n",
            "Score:  162.82828391491094\n",
            "Score:  542.1184143511754\n",
            "Score:  31.237481300357665\n",
            "Score:  245.01920097443943\n",
            "Score:  97.01436170738424\n",
            "Score:  65.92248599785533\n",
            "Score:  351.786357658856\n",
            "Score:  151.5684428991917\n",
            "Score:  287.84036367409914\n",
            "Score:  91.92948329075489\n",
            "Score:  201.43108632549433\n",
            "Score:  698.9628462903668\n",
            "Score:  14.313177963673652\n",
            "Score:  227.9135176232817\n",
            "Score:  10.425839270472798\n",
            "Score:  266.25520437944704\n",
            "Score:  42.91766684037381\n",
            "Score:  50.15941731496834\n",
            "Score:  39.907848334992295\n",
            "Score:  516.9223705728675\n",
            "Score:  781.6134241979667\n",
            "check point:  1.0\n",
            "Score:  3470.9317624170494\n",
            "Score:  8.159629152263042\n",
            "Eval num_timesteps=170000, episode_reward=1420.23 +/- 1484.04\n",
            "Episode length: 274.67 +/- 198.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 275      |\n",
            "|    mean_reward     | 1.42e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 170000   |\n",
            "---------------------------------\n",
            "Score:  51.39426204038049\n",
            "Score:  1200.1923790721928\n",
            "Score:  674.4445132129649\n",
            "Score:  7.320693701204413\n",
            "Score:  251.4350957144974\n",
            "Score:  44.86929319809286\n",
            "Score:  237.22874711378384\n",
            "Score:  35.6193868976847\n",
            "Score:  2571.0094160725666\n",
            "Score:  10.726458485479434\n",
            "Score:  67.88379104634221\n",
            "Score:  216.79399368642214\n",
            "Score:  257.6595910386654\n",
            "Score:  564.7841342970283\n",
            "Score:  165.13413225906314\n",
            "Score:  58.96398624922252\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 127      |\n",
            "|    ep_rew_mean     | 272      |\n",
            "| time/              |          |\n",
            "|    fps             | 278      |\n",
            "|    iterations      | 21       |\n",
            "|    time_elapsed    | 617      |\n",
            "|    total_timesteps | 172032   |\n",
            "---------------------------------\n",
            "Score:  253.44939195739113\n",
            "Score:  135.5646190060012\n",
            "Score:  62.4771235210185\n",
            "Score:  363.5863718069547\n",
            "Score:  100.33947417193319\n",
            "Score:  140.31360524776966\n",
            "Score:  191.09677529530637\n",
            "Score:  72.37089662258248\n",
            "Score:  11.052430145196242\n",
            "Score:  71.25396103100572\n",
            "Score:  1212.9308937775913\n",
            "Score:  374.98922293374306\n",
            "Score:  646.344114896443\n",
            "Score:  35.71694830919095\n",
            "Score:  114.78330249831417\n",
            "Score:  1061.2221058168493\n",
            "Score:  189.5173578224524\n",
            "Score:  162.98817112876438\n",
            "Score:  212.42455802539942\n",
            "check point:  1.0\n",
            "Score:  3289.0638602697354\n",
            "Score:  1109.2600316111227\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "Score:  42417.36217608511\n",
            "Eval num_timesteps=175000, episode_reward=15605.23 +/- 18979.92\n",
            "Episode length: 860.67 +/- 646.97\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 861          |\n",
            "|    mean_reward          | 1.56e+04     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 175000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026844172 |\n",
            "|    clip_fraction        | 0.126        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.509       |\n",
            "|    explained_variance   | 0.764        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 21.6         |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | 0.00191      |\n",
            "|    value_loss           | 25.6         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Score:  1218.1383072051203\n",
            "Score:  200.42253503973998\n",
            "Score:  1087.4227422899219\n",
            "Score:  57.5504104537977\n",
            "Score:  2197.665104679366\n",
            "Score:  84.92498288622136\n",
            "Score:  299.8661025234426\n",
            "Score:  154.45665709003018\n",
            "Score:  82.3122280487121\n",
            "Score:  201.28541441135823\n",
            "Score:  148.5424283074003\n",
            "Score:  162.05215873925965\n",
            "Score:  52.13205075860199\n",
            "Score:  813.4782101680895\n",
            "Score:  138.16926893770201\n",
            "Score:  379.98914292234156\n",
            "Score:  388.04538319437387\n",
            "Score:  110.49113811761723\n",
            "Score:  109.89397602286229\n",
            "Score:  18.92851274360674\n",
            "Score:  277.19556932849144\n",
            "Score:  337.2102659551793\n",
            "Score:  1006.5439133064167\n",
            "Score:  670.622215350768\n",
            "Score:  34.6920228074265\n",
            "Score:  1181.3349539720937\n",
            "Score:  5.5300523558872925\n",
            "Score:  22.005500086263616\n",
            "Score:  159.67481929633038\n",
            "Score:  547.4328753031015\n",
            "Score:  16.027566056327874\n",
            "Score:  69.958095673345\n",
            "Score:  201.9810248280929\n",
            "Score:  2772.1754179548398\n",
            "Score:  95.33454359453908\n",
            "check point:  1.0\n",
            "Score:  7439.338688094337\n",
            "Eval num_timesteps=180000, episode_reward=3435.62 +/- 3034.66\n",
            "Episode length: 436.33 +/- 265.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 436      |\n",
            "|    mean_reward     | 3.44e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 180000   |\n",
            "---------------------------------\n",
            "Score:  587.013652533917\n",
            "Score:  278.433992185916\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 140      |\n",
            "|    ep_rew_mean     | 316      |\n",
            "| time/              |          |\n",
            "|    fps             | 274      |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 656      |\n",
            "|    total_timesteps | 180224   |\n",
            "---------------------------------\n",
            "Score:  825.0534611777668\n",
            "Score:  144.26401899924034\n",
            "Score:  587.9897188627021\n",
            "Score:  118.49859481598752\n",
            "Score:  66.53386554914891\n",
            "Score:  95.88323317643022\n",
            "Score:  140.0822954682957\n",
            "Score:  99.84488702552514\n",
            "Score:  33.644728204512504\n",
            "Score:  146.07414108355067\n",
            "Score:  12.392582361247516\n",
            "Score:  614.7991253472799\n",
            "Score:  295.89135677718014\n",
            "Score:  2830.501022492948\n",
            "Score:  161.1527578298497\n",
            "Score:  66.5027911603236\n",
            "Score:  93.5351746246414\n",
            "Score:  114.85462227731979\n",
            "Score:  310.18594232859977\n",
            "check point:  1.0\n",
            "Score:  10.972929818578255\n",
            "Score:  4219.349568718205\n",
            "Score:  12.82100634468385\n",
            "Score:  24.844119904885005\n",
            "Score:  42.28181141964323\n",
            "Score:  1205.5753088470576\n",
            "Score:  10.412252753946227\n",
            "Score:  7.548660508519484\n",
            "Score:  1435.2596615499735\n",
            "Score:  60.72605786481938\n",
            "Score:  10.30175463247594\n",
            "Score:  225.86706810976375\n",
            "Score:  1668.4391160123203\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "Score:  13895.208608836274\n",
            "Score:  72.88676496892889\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "Score:  15216.855510292964\n",
            "Eval num_timesteps=185000, episode_reward=9728.32 +/- 6848.71\n",
            "Episode length: 719.67 +/- 444.03\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 720         |\n",
            "|    mean_reward          | 9.73e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 185000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003199254 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.509      |\n",
            "|    explained_variance   | 0.776       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 15.9        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | 0.00205     |\n",
            "|    value_loss           | 24.1        |\n",
            "-----------------------------------------\n",
            "Score:  1289.3591907910352\n",
            "Score:  59.89721489751329\n",
            "Score:  76.00567425389413\n",
            "Score:  273.7477530905887\n",
            "Score:  33.450130426039784\n",
            "Score:  44.59590984839387\n",
            "Score:  92.71705467898461\n",
            "Score:  10.668118117429962\n",
            "Score:  141.27131845194396\n",
            "Score:  303.4907258426318\n",
            "Score:  388.28469928224507\n",
            "Score:  719.1949849429622\n",
            "Score:  78.71282285384594\n",
            "Score:  35.14749195439145\n",
            "Score:  270.0514322348846\n",
            "Score:  19.55775061704361\n",
            "Score:  695.672327137703\n",
            "Score:  155.76535465110393\n",
            "Score:  219.35610883003855\n",
            "Score:  185.15246760638675\n",
            "Score:  3173.370549099851\n",
            "Score:  38.95401593993801\n",
            "Score:  116.94468026892172\n",
            "Score:  16.35535540201823\n",
            "Score:  12.1044683898089\n",
            "Score:  73.46592095182196\n",
            "Score:  30.389295669528458\n",
            "Score:  451.5700039361782\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 146      |\n",
            "|    ep_rew_mean     | 397      |\n",
            "| time/              |          |\n",
            "|    fps             | 272      |\n",
            "|    iterations      | 23       |\n",
            "|    time_elapsed    | 690      |\n",
            "|    total_timesteps | 188416   |\n",
            "---------------------------------\n",
            "Score:  12.942440364616235\n",
            "Score:  158.8801095306969\n",
            "Score:  518.2877515528274\n",
            "Score:  153.55348813597274\n",
            "Score:  1466.4919070251392\n",
            "Score:  40.030282722163506\n",
            "Score:  2200.243389538539\n",
            "Score:  1130.4349126815416\n",
            "Score:  2849.789968020023\n",
            "Score:  13.31473308047106\n",
            "Eval num_timesteps=190000, episode_reward=1331.18 +/- 1166.65\n",
            "Episode length: 277.00 +/- 176.38\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 277          |\n",
            "|    mean_reward          | 1.33e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 190000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040123705 |\n",
            "|    clip_fraction        | 0.138        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.513       |\n",
            "|    explained_variance   | 0.691        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 30.6         |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | 0.00202      |\n",
            "|    value_loss           | 50.3         |\n",
            "------------------------------------------\n",
            "Score:  546.7546277261997\n",
            "Score:  88.24620506218761\n",
            "Score:  16.65177056025972\n",
            "Score:  263.9165068012147\n",
            "Score:  26.347119163031504\n",
            "Score:  174.37795553655744\n",
            "Score:  1905.1795132450031\n",
            "Score:  123.78510091504266\n",
            "Score:  33.49244996742533\n",
            "Score:  24.730289170684628\n",
            "Score:  62.13285059695836\n",
            "Score:  15.626738877362609\n",
            "Score:  16.67859277232536\n",
            "Score:  73.48553690856386\n",
            "Score:  10.465886013623019\n",
            "Score:  215.2902979404409\n",
            "Score:  65.83808033746222\n",
            "Score:  293.17648262830573\n",
            "Score:  2210.747291220056\n",
            "Score:  257.8383674900577\n",
            "Score:  493.2204063679104\n",
            "Score:  98.52926854268766\n",
            "Score:  8.375484880769802\n",
            "Score:  43.02826944997873\n",
            "Score:  5.922267715414891\n",
            "Score:  875.5686658647375\n",
            "Score:  871.4093649810225\n",
            "Score:  11.459995457052639\n",
            "Score:  161.01907099341284\n",
            "Score:  65.60560047813408\n",
            "Score:  774.4797257552145\n",
            "Score:  9.427291158106517\n",
            "Score:  43.85992362862815\n",
            "Score:  790.7343643399514\n",
            "Score:  965.7834360886723\n",
            "Score:  189.48989486373605\n",
            "check point:  1.0\n",
            "Score:  4661.208432208368\n",
            "Score:  8.159629152263042\n",
            "check point:  1.0\n",
            "Score:  3642.658320678733\n",
            "Eval num_timesteps=195000, episode_reward=2770.68 +/- 1997.16\n",
            "Episode length: 383.33 +/- 245.67\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 383      |\n",
            "|    mean_reward     | 2.77e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 195000   |\n",
            "---------------------------------\n",
            "Score:  1340.4235888491826\n",
            "Score:  10.099940854341837\n",
            "Score:  112.42046849147067\n",
            "Score:  762.7906582762548\n",
            "Score:  1387.5086237484722\n",
            "Score:  33.29689831126664\n",
            "Score:  502.3796172034349\n",
            "Score:  181.7110402094602\n",
            "Score:  232.5345850712443\n",
            "Score:  100.56398106639955\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 145      |\n",
            "|    ep_rew_mean     | 426      |\n",
            "| time/              |          |\n",
            "|    fps             | 271      |\n",
            "|    iterations      | 24       |\n",
            "|    time_elapsed    | 723      |\n",
            "|    total_timesteps | 196608   |\n",
            "---------------------------------\n",
            "Score:  1188.4313555790086\n",
            "Score:  312.53521804738983\n",
            "Score:  703.790017086325\n",
            "Score:  2525.0680333597943\n",
            "Score:  84.82029310982443\n",
            "Score:  21.015878739795788\n",
            "Score:  74.90150142524766\n",
            "Score:  162.10565056029137\n",
            "Score:  10.1814615316121\n",
            "Score:  37.483508281358574\n",
            "Score:  2476.2605916752677\n",
            "Score:  120.44127750791903\n",
            "Score:  873.8084949521734\n",
            "Score:  7.424018726624898\n",
            "Score:  191.65977537104288\n",
            "Score:  95.75171861870224\n",
            "Score:  142.3951111593975\n",
            "Score:  54.03287408917482\n",
            "Score:  271.28099503597343\n",
            "Score:  43.763706777961026\n",
            "Score:  922.1530130845207\n",
            "Score:  159.05447871354448\n",
            "Score:  14.695000065966976\n",
            "Score:  13.698852620415018\n",
            "Score:  31.211526868540226\n",
            "Score:  13.4612669301462\n",
            "Score:  10.741900836904286\n",
            "Score:  8.15522217347565\n",
            "Score:  7.852784068221046\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "Score:  21329.367530878906\n",
            "Eval num_timesteps=200000, episode_reward=7115.13 +/- 10050.99\n",
            "Episode length: 440.67 +/- 570.87\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 441          |\n",
            "|    mean_reward          | 7.12e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 200000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029018377 |\n",
            "|    clip_fraction        | 0.13         |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.51        |\n",
            "|    explained_variance   | 0.762        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 35.6         |\n",
            "|    n_updates            | 270          |\n",
            "|    policy_gradient_loss | 0.00265      |\n",
            "|    value_loss           | 35.6         |\n",
            "------------------------------------------\n",
            "Score:  8.052400131787401\n",
            "Score:  77.57864446355461\n",
            "Score:  94.53367403156601\n",
            "Score:  865.28538779236\n",
            "Score:  210.36414681299226\n",
            "Score:  90.05944783348549\n",
            "Score:  6.860045584155091\n",
            "Score:  1519.168832328884\n",
            "Score:  55.84609207205027\n",
            "Score:  54.58698278503477\n",
            "Score:  65.84879633201209\n",
            "Score:  50.39766749069933\n",
            "Score:  10.828416641221823\n",
            "Score:  14.844465155991468\n",
            "Score:  207.32307495504998\n",
            "Score:  10.661011798984633\n",
            "Score:  115.91520431822138\n",
            "Score:  93.48552247241687\n",
            "Score:  1499.9094059995032\n",
            "Score:  541.2692369707573\n",
            "Score:  7.099522226115923\n",
            "Score:  393.1432155278215\n",
            "Score:  867.0780206564315\n",
            "Score:  7.575566483417145\n",
            "Score:  592.8448769267973\n",
            "Score:  13.544886584491934\n",
            "Score:  193.32895398668148\n",
            "Score:  81.5495027000309\n",
            "Score:  71.95100521872814\n",
            "Score:  77.5151216749153\n",
            "Score:  10.349246377052616\n",
            "Score:  127.30197822454659\n",
            "Score:  14.359069314243609\n",
            "Score:  458.9371889531279\n",
            "Score:  154.99808261413745\n",
            "Score:  152.41417428617487\n",
            "Score:  162.8917311928507\n",
            "Score:  1040.3928789442473\n",
            "Score:  152.06338310440407\n",
            "Score:  147.32582392778158\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 135      |\n",
            "|    ep_rew_mean     | 341      |\n",
            "| time/              |          |\n",
            "|    fps             | 271      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 755      |\n",
            "|    total_timesteps | 204800   |\n",
            "---------------------------------\n",
            "Score:  6.698758778378811\n",
            "Score:  15.755491707783923\n",
            "Score:  8.15522217347565\n",
            "Score:  8.159629152263042\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "Score:  36169.06801707078\n",
            "Eval num_timesteps=205000, episode_reward=12061.79 +/- 17046.42\n",
            "Episode length: 569.33 +/- 751.42\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 569          |\n",
            "|    mean_reward          | 1.21e+04     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 205000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033628684 |\n",
            "|    clip_fraction        | 0.141        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.518       |\n",
            "|    explained_variance   | 0.805        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 22.6         |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | 0.00344      |\n",
            "|    value_loss           | 33.4         |\n",
            "------------------------------------------\n",
            "Score:  15.358004960241022\n",
            "Score:  177.59542698337034\n",
            "Score:  1955.1375196609772\n",
            "Score:  769.4992026599318\n",
            "Score:  16.832239738692294\n",
            "Score:  27.63714831343687\n",
            "Score:  54.07503315297801\n",
            "Score:  38.06091111604983\n",
            "Score:  1387.0674299787415\n",
            "Score:  174.54855243305258\n",
            "Score:  39.53629912655932\n",
            "Score:  12.815055335273987\n",
            "Score:  17.515866531115943\n",
            "Score:  10.508420732615653\n",
            "Score:  99.91032754659679\n",
            "Score:  5.645842289885922\n",
            "Score:  21.760879757566173\n",
            "Score:  41.821670773560015\n",
            "Score:  121.45833231954106\n",
            "Score:  27.256430841971856\n",
            "Score:  31.303152689044467\n",
            "Score:  63.37772938680661\n",
            "check point:  1.0\n",
            "Score:  850.8532037603719\n",
            "Score:  1388.7463322651286\n",
            "Score:  4594.074019614789\n",
            "Score:  458.36277614737696\n",
            "Score:  227.97876467132653\n",
            "Score:  115.11788672828686\n",
            "Score:  14.931489850830754\n",
            "Score:  8.275535676110511\n",
            "Score:  55.34497520091014\n",
            "Score:  110.1316537124402\n",
            "Score:  1602.5313788611727\n",
            "Score:  200.78141740197867\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "Score:  601404.5266368731\n",
            "Score:  202.61254503835713\n",
            "Eval num_timesteps=210000, episode_reward=200602.64 +/- 283409.73\n",
            "Episode length: 2246.67 +/- 2975.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.25e+03 |\n",
            "|    mean_reward     | 2.01e+05 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 210000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Score:  40.35158131377496\n",
            "Score:  142.93609333255918\n",
            "check point:  1.0\n",
            "Score:  4247.306860096839\n",
            "check point:  1.0\n",
            "Score:  182.7809363453255\n",
            "Score:  3809.508313285305\n",
            "Score:  160.37821245681042\n",
            "Score:  111.84572433616069\n",
            "Score:  645.4809838828141\n",
            "Score:  168.6796318592021\n",
            "Score:  36.53773842552041\n",
            "Score:  6.162805733595445\n",
            "Score:  10.175317918269462\n",
            "Score:  119.56711248782184\n",
            "Score:  52.01151204017623\n",
            "Score:  287.33515993889955\n",
            "Score:  485.18565723905994\n",
            "Score:  461.70620870142557\n",
            "Score:  20.760923837275605\n",
            "Score:  10.538344324243724\n",
            "Score:  9.388996734679418\n",
            "Score:  37.84318041436281\n",
            "Score:  121.78787986910768\n",
            "Score:  32.410299663921\n",
            "Score:  672.3247724188952\n",
            "Score:  184.0274465869916\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 130      |\n",
            "|    ep_rew_mean     | 369      |\n",
            "| time/              |          |\n",
            "|    fps             | 263      |\n",
            "|    iterations      | 26       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 212992   |\n",
            "---------------------------------\n",
            "Score:  257.56632958118786\n",
            "Score:  343.61697143686825\n",
            "Score:  20.17587130192662\n",
            "Score:  565.9538488684939\n",
            "Score:  818.233181796258\n",
            "Score:  20.994674919039987\n",
            "Score:  13.31473308047106\n",
            "Score:  393.25265374375624\n",
            "Score:  6.634231797640135\n",
            "Score:  143.77317862443502\n",
            "Score:  1864.564828161993\n",
            "Score:  181.3631930691673\n",
            "Score:  55.86427688620387\n",
            "Score:  14.253568570448328\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "Score:  26179.132907174848\n",
            "Eval num_timesteps=215000, episode_reward=8749.75 +/- 12324.45\n",
            "Episode length: 505.67 +/- 621.98\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 506         |\n",
            "|    mean_reward          | 8.75e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 215000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004205039 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.513      |\n",
            "|    explained_variance   | 0.699       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 58.1        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | 0.00362     |\n",
            "|    value_loss           | 71.6        |\n",
            "-----------------------------------------\n",
            "Score:  552.0929546253561\n",
            "Score:  269.57458414182656\n",
            "Score:  38.82942998238173\n",
            "Score:  20.479086227806718\n",
            "Score:  72.65594911937285\n",
            "Score:  1081.201703257786\n",
            "check point:  1.0\n",
            "Score:  8.821202705088343\n",
            "Score:  8.183458346834323\n",
            "Score:  4667.6951302721045\n",
            "Score:  1250.838558139954\n",
            "Score:  1398.0265492689098\n",
            "Score:  342.95369306006455\n",
            "Score:  15.952799977733566\n",
            "Score:  388.6493021083655\n",
            "Score:  13.517481919209256\n",
            "Score:  76.48822612817139\n",
            "Score:  116.11605998147805\n",
            "Score:  2600.957272729056\n",
            "Score:  10.031903599237976\n",
            "Score:  82.53295819245545\n",
            "Score:  1332.8984447384437\n",
            "Score:  18.26250675772016\n",
            "check point:  1.0\n",
            "Score:  3925.4290352766398\n",
            "Score:  86.65954123110511\n",
            "Score:  7.092412816932606\n",
            "Score:  1631.2024809399823\n",
            "Score:  1582.695995160078\n",
            "Score:  44.55599206560516\n",
            "Score:  191.74571852753863\n",
            "Score:  204.68528430120412\n",
            "Score:  200.45115266188458\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "Score:  62059.16289428468\n",
            "Eval num_timesteps=220000, episode_reward=20821.43 +/- 29159.48\n",
            "Episode length: 793.67 +/- 920.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 794      |\n",
            "|    mean_reward     | 2.08e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 220000   |\n",
            "---------------------------------\n",
            "Score:  43.12757299811557\n",
            "Score:  129.73227115614273\n",
            "Score:  1122.1445037173517\n",
            "Score:  509.62193185036085\n",
            "Score:  272.2176586747539\n",
            "Score:  175.52394853014204\n",
            "Score:  238.04662667808307\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 154      |\n",
            "|    ep_rew_mean     | 526      |\n",
            "| time/              |          |\n",
            "|    fps             | 261      |\n",
            "|    iterations      | 27       |\n",
            "|    time_elapsed    | 845      |\n",
            "|    total_timesteps | 221184   |\n",
            "---------------------------------\n",
            "Score:  150.15246694289462\n",
            "Score:  394.1616934885309\n",
            "Score:  83.63638601067481\n",
            "Score:  17.721663274539534\n",
            "Score:  1509.5928971315566\n",
            "Score:  33.063663547900525\n",
            "Score:  12.856515310386117\n",
            "Score:  1747.7774368108805\n",
            "Score:  161.10472065155363\n",
            "Score:  1565.0695981805143\n",
            "Score:  13.033326614050857\n",
            "Score:  15.454813704025307\n",
            "Score:  37.01370016558745\n",
            "Score:  669.290802645392\n",
            "Score:  232.46123136240485\n",
            "Score:  7.005012853280401\n",
            "Score:  9.530095204366198\n",
            "Score:  625.5844647343082\n",
            "Score:  64.53713531160291\n",
            "Score:  214.27089817174186\n",
            "Score:  968.5638076875489\n",
            "Score:  516.0619523552641\n",
            "Score:  13.790834980048537\n",
            "Score:  62.0644553785923\n",
            "Score:  11.93629231748474\n",
            "Score:  43.078346252374914\n",
            "Score:  900.6241155612673\n",
            "Score:  15.263924764836776\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "Score:  72300.610148033\n",
            "Score:  2079.0882037413467\n",
            "check point:  1.0\n",
            "Score:  5780.2746970667895\n",
            "Eval num_timesteps=225000, episode_reward=26719.99 +/- 32265.76\n",
            "Episode length: 1111.33 +/- 821.08\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.11e+03    |\n",
            "|    mean_reward          | 2.67e+04    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 225000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004847507 |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.508      |\n",
            "|    explained_variance   | 0.716       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 33.9        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | 0.00611     |\n",
            "|    value_loss           | 68.1        |\n",
            "-----------------------------------------\n",
            "Score:  156.41897852425964\n",
            "Score:  471.2352319995009\n",
            "Score:  370.42203068007433\n",
            "Score:  13.962315012843119\n",
            "Score:  13.720591044581745\n",
            "Score:  14.57255369046721\n",
            "Score:  8.55879509342574\n",
            "Score:  16.463263360414196\n",
            "Score:  17.29551163311701\n",
            "Score:  30.303340597106093\n",
            "Score:  1009.4640273602613\n",
            "Score:  13.93676264522868\n",
            "Score:  223.08254731649671\n",
            "Score:  102.79073549462053\n",
            "Score:  581.8327635375088\n",
            "Score:  37.489699099556944\n",
            "Score:  17.41992830236444\n",
            "Score:  14.301420393875876\n",
            "Score:  458.0538706887258\n",
            "Score:  122.59458765617437\n",
            "Score:  667.170828352007\n",
            "Score:  97.84370728325274\n",
            "Score:  57.828530285841985\n",
            "Score:  119.05865804012869\n",
            "Score:  7.160901741878974\n",
            "Score:  139.288734836607\n",
            "Score:  230.05488779973754\n",
            "Score:  37.444249158250415\n",
            "Score:  11.298772705538783\n",
            "Score:  16.26676474968\n",
            "Score:  19.48007072088057\n",
            "Score:  2330.6034702326797\n",
            "Score:  83.58631540065207\n",
            "Score:  237.12507481328646\n",
            "Score:  13.499517806356884\n",
            "Score:  50.22053454406671\n",
            "Score:  400.45403080083634\n",
            "Score:  37.58025187022644\n",
            "Score:  12.88700289264718\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 139      |\n",
            "|    ep_rew_mean     | 418      |\n",
            "| time/              |          |\n",
            "|    fps             | 259      |\n",
            "|    iterations      | 28       |\n",
            "|    time_elapsed    | 883      |\n",
            "|    total_timesteps | 229376   |\n",
            "---------------------------------\n",
            "Score:  676.6271458161666\n",
            "Score:  772.0933661203474\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13093374.257028522\n",
            "check point:  60.0\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "Score:  471156.3245957471\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "Score:  82322.71748834995\n",
            "Eval num_timesteps=230000, episode_reward=4549345.32 +/- 6044461.86\n",
            "Episode length: 12707.33 +/- 12301.75\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1.27e+04     |\n",
            "|    mean_reward          | 4.55e+06     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 230000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037243282 |\n",
            "|    clip_fraction        | 0.147        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.514       |\n",
            "|    explained_variance   | 0.846        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 24.4         |\n",
            "|    n_updates            | 310          |\n",
            "|    policy_gradient_loss | 0.00231      |\n",
            "|    value_loss           | 29           |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Score:  1609.563793026593\n",
            "Score:  53.95394662842435\n",
            "Score:  99.53983327608584\n",
            "Score:  1034.3980280299386\n",
            "Score:  1340.8036243083504\n",
            "Score:  185.10185724468593\n",
            "Score:  20.21891665917404\n",
            "Score:  601.9695046436182\n",
            "Score:  584.0506795943116\n",
            "Score:  33.75767983442084\n",
            "Score:  37.725783368985645\n",
            "Score:  423.8683441676662\n",
            "Score:  2100.5873427819142\n",
            "Score:  2207.6250301977743\n",
            "Score:  286.61307130651505\n",
            "Score:  30.113557065269116\n",
            "Score:  5.673524109042999\n",
            "Score:  14.28431704362603\n",
            "Score:  89.7461283526668\n",
            "Score:  57.440309556058594\n",
            "Score:  49.155236702664666\n",
            "Score:  576.5968727016619\n",
            "Score:  107.01502107287887\n",
            "Score:  10.4961490199413\n",
            "Score:  43.418780565958016\n",
            "Score:  7.054002930003751\n",
            "Score:  430.4440418689639\n",
            "Score:  203.59949754729232\n",
            "Score:  15.409267277695047\n",
            "Score:  61.159765013424476\n",
            "Score:  86.87974522665866\n",
            "Score:  8.814115483648227\n",
            "Score:  422.7634046535468\n",
            "Score:  6.319085651647042\n",
            "Score:  68.6271740591332\n",
            "Score:  111.71687608126172\n",
            "Score:  109.9950033038037\n",
            "Score:  129.98384115881956\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13115495.48954265\n",
            "check point:  60.0\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "Score:  300732.7861139815\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "Score:  28589.066692786644\n",
            "Eval num_timesteps=235000, episode_reward=4481994.36 +/- 6106642.26\n",
            "Episode length: 12013.00 +/- 12784.69\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.2e+04  |\n",
            "|    mean_reward     | 4.48e+06 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 235000   |\n",
            "---------------------------------\n",
            "Score:  9.996950651475254\n",
            "check point:  1.0\n",
            "Score:  379.2213273226315\n",
            "Score:  426.6050762005094\n",
            "Score:  246.2616258518365\n",
            "Score:  21.76331522775048\n",
            "Score:  98.63201492876956\n",
            "Score:  16.098778649158813\n",
            "Score:  74.98041122425714\n",
            "Score:  180.93808912100948\n",
            "Score:  40.420855987710176\n",
            "Score:  88.0324798367639\n",
            "Score:  9900.463512265384\n",
            "Score:  275.03630427301385\n",
            "Score:  394.95500386681016\n",
            "Score:  771.3758756837047\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 128      |\n",
            "|    ep_rew_mean     | 369      |\n",
            "| time/              |          |\n",
            "|    fps             | 212      |\n",
            "|    iterations      | 29       |\n",
            "|    time_elapsed    | 1115     |\n",
            "|    total_timesteps | 237568   |\n",
            "---------------------------------\n",
            "Score:  1284.6521114828279\n",
            "Score:  65.18782706874762\n",
            "Score:  1053.1120429218176\n",
            "Score:  37.629824600796084\n",
            "Score:  647.7195601509777\n",
            "Score:  9.242705063150774\n",
            "Score:  72.1242981383001\n",
            "Score:  154.33013679403902\n",
            "Score:  8.088320248052206\n",
            "Score:  148.40117300565936\n",
            "Score:  292.4548426931668\n",
            "Score:  372.710793759374\n",
            "Score:  101.36903855139998\n",
            "Score:  23.046596980185388\n",
            "Score:  1106.4165003063438\n",
            "Score:  9.18628857112979\n",
            "Score:  80.35167575285065\n",
            "Score:  191.5972358042475\n",
            "Score:  6.632236618634792\n",
            "Score:  83.63904832007717\n",
            "Score:  19.634760767363844\n",
            "Score:  2078.7936116761607\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "Score:  48883.44357128505\n",
            "Score:  280.93598497825843\n",
            "Eval num_timesteps=240000, episode_reward=17081.06 +/- 22499.66\n",
            "Episode length: 817.67 +/- 756.32\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 818          |\n",
            "|    mean_reward          | 1.71e+04     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 240000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071327873 |\n",
            "|    clip_fraction        | 0.215        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.484       |\n",
            "|    explained_variance   | 0.67         |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 109          |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | 0.00439      |\n",
            "|    value_loss           | 103          |\n",
            "------------------------------------------\n",
            "Score:  622.3046456266028\n",
            "Score:  22.17332037306013\n",
            "Score:  437.48873804058127\n",
            "Score:  39.02006930442841\n",
            "Score:  11.987924901588944\n",
            "Score:  348.0367322867321\n",
            "Score:  99.37924538509975\n",
            "Score:  17.588184940433496\n",
            "Score:  126.86363741449142\n",
            "Score:  464.8363116271213\n",
            "Score:  374.3504983912843\n",
            "Score:  455.14424985107206\n",
            "Score:  7.5696461004412\n",
            "Score:  130.24375873056997\n",
            "Score:  9.441407639140504\n",
            "Score:  277.84353931726423\n",
            "Score:  8.578874556607513\n",
            "Score:  333.37377192791473\n",
            "Score:  126.47849143234374\n",
            "Score:  362.52848165118826\n",
            "Score:  216.16868271064925\n",
            "Score:  10.14830575519025\n",
            "Score:  779.7076238736933\n",
            "Score:  12.008502713232662\n",
            "Score:  135.4792686059149\n",
            "Score:  11.926612356355548\n",
            "Score:  168.2084575104048\n",
            "Score:  70.83679169178478\n",
            "Score:  10.13184313890159\n",
            "Score:  193.95436743434894\n",
            "Score:  877.692188645541\n",
            "Score:  1484.9203531163648\n",
            "check point:  1.0\n",
            "Score:  1699.9803488023078\n",
            "Score:  1899.838010974291\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "Score:  72811.43433524079\n",
            "Score:  57.024919643606694\n",
            "Eval num_timesteps=245000, episode_reward=24922.77 +/- 33870.76\n",
            "Episode length: 911.67 +/- 965.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 912      |\n",
            "|    mean_reward     | 2.49e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 245000   |\n",
            "---------------------------------\n",
            "Score:  699.9946533672412\n",
            "Score:  286.0273727276594\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 132      |\n",
            "|    ep_rew_mean     | 374      |\n",
            "| time/              |          |\n",
            "|    fps             | 212      |\n",
            "|    iterations      | 30       |\n",
            "|    time_elapsed    | 1157     |\n",
            "|    total_timesteps | 245760   |\n",
            "---------------------------------\n",
            "Score:  1035.1527959537557\n",
            "Score:  2602.377619037468\n",
            "Score:  8.571045383463654\n",
            "Score:  12.154218178219928\n",
            "Score:  10711.16026860373\n",
            "Score:  27.13914392230047\n",
            "Score:  24.070707530627473\n",
            "Score:  451.7922810731964\n",
            "Score:  14.270792450212987\n",
            "Score:  281.05934089343884\n",
            "Score:  10.976460322611748\n",
            "Score:  14.353719151966748\n",
            "Score:  338.723423770767\n",
            "Score:  9.931294110306577\n",
            "Score:  12.835293815659522\n",
            "Score:  1751.1035078151683\n",
            "check point:  1.0\n",
            "Score:  1221.7467783682775\n",
            "Score:  28.563746068589804\n",
            "Score:  19.640797989970224\n",
            "Score:  9.734957369904754\n",
            "Score:  8.457465312828141\n",
            "Score:  8.309582235946452\n",
            "Score:  63.326800008338985\n",
            "Score:  7281.207171711952\n",
            "Score:  78.43961095035874\n",
            "Score:  776.4566282417187\n",
            "Score:  562.5444266203544\n",
            "Score:  35.05659420653652\n",
            "Score:  174.6527867775446\n",
            "Score:  67.12569277435486\n",
            "Score:  187.9726332193955\n",
            "Score:  17.12289234431311\n",
            "Score:  87.6160225689079\n",
            "Score:  6.413728297727261\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "Score:  254946.0012059606\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "Score:  43129.882679075825\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13078306.077095237\n",
            "check point:  60.0\n",
            "Eval num_timesteps=250000, episode_reward=4459186.67 +/- 6096084.13\n",
            "Episode length: 11997.33 +/- 12770.01\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.2e+04     |\n",
            "|    mean_reward          | 4.46e+06    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 250000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006173816 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.497      |\n",
            "|    explained_variance   | 0.721       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 62.7        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | 0.00527     |\n",
            "|    value_loss           | 100         |\n",
            "-----------------------------------------\n",
            "Score:  124.4463679028089\n",
            "Score:  386.5849505572446\n",
            "Score:  248.74104278419003\n",
            "Score:  281.1198715337132\n",
            "Score:  237.25682101836566\n",
            "Score:  14.971229391947714\n",
            "Score:  652.7445382701926\n",
            "Score:  136.42932112180355\n",
            "Score:  1345.068865541358\n",
            "Score:  57.9876957988519\n",
            "Score:  7.120631500512639\n",
            "Score:  136.8851724195746\n",
            "Score:  1915.7153681933755\n",
            "Score:  148.800410725216\n",
            "Score:  36.07045278347261\n",
            "Score:  713.0342063260653\n",
            "Score:  11.686659509205887\n",
            "Score:  163.83534149469196\n",
            "Score:  308.9529831022184\n",
            "Score:  11.104516910865918\n",
            "Score:  10.437542079786668\n",
            "check point:  1.0\n",
            "Score:  653.2499662719249\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 141      |\n",
            "|    ep_rew_mean     | 481      |\n",
            "| time/              |          |\n",
            "|    fps             | 197      |\n",
            "|    iterations      | 31       |\n",
            "|    time_elapsed    | 1284     |\n",
            "|    total_timesteps | 253952   |\n",
            "---------------------------------\n",
            "Score:  588.739623395942\n",
            "Score:  1265.549571479006\n",
            "Score:  12.798386566261723\n",
            "Score:  6.033492102324433\n",
            "Score:  655.4554753023007\n",
            "Score:  12.524241972777961\n",
            "Score:  10671.306195996403\n",
            "Score:  40.919141588773115\n",
            "Score:  6.585168093942668\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "Score:  272579.404644788\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13094861.705943296\n",
            "check point:  60.0\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "Score:  3513250.800852584\n",
            "Eval num_timesteps=255000, episode_reward=5627237.86 +/- 5444323.47\n",
            "Episode length: 16665.33 +/- 10489.90\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1.67e+04     |\n",
            "|    mean_reward          | 5.63e+06     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 255000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069874907 |\n",
            "|    clip_fraction        | 0.248        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.512       |\n",
            "|    explained_variance   | 0.732        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 157          |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | 0.00952      |\n",
            "|    value_loss           | 131          |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Score:  11.42029120403273\n",
            "Score:  56.03003417521316\n",
            "Score:  985.9566306127596\n",
            "Score:  173.28180978244197\n",
            "Score:  7.246117564248254\n",
            "Score:  13.284540857524833\n",
            "Score:  2218.8436527886765\n",
            "Score:  898.033623872424\n",
            "Score:  13.394420299971332\n",
            "Score:  112.12849886177213\n",
            "Score:  32.53371418458411\n",
            "Score:  638.7894492023116\n",
            "check point:  1.0\n",
            "Score:  605.43582578912\n",
            "Score:  3781.0682026556856\n",
            "Score:  63.817016030168006\n",
            "Score:  190.73906447869476\n",
            "Score:  1645.584194322284\n",
            "Score:  112.41137538490933\n",
            "Score:  380.58098936608786\n",
            "Score:  7.6244170803968965\n",
            "Score:  9.503132374259451\n",
            "Score:  22.571119314318242\n",
            "Score:  10.762570389713275\n",
            "Score:  110.16054752321038\n",
            "Score:  13.390654638580871\n",
            "Score:  53.4468807562102\n",
            "Score:  16.497919100301285\n",
            "Score:  88.70806747505343\n",
            "Score:  12.789274600732831\n",
            "Score:  88.50312450405535\n",
            "Score:  20.83182254488884\n",
            "Score:  183.17255022125607\n",
            "Score:  110.24577449979523\n",
            "Score:  960.1256802391899\n",
            "Score:  1916.7225153567545\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13091436.985289369\n",
            "check point:  60.0\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13107447.184776636\n",
            "check point:  60.0\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13105413.939047998\n",
            "check point:  60.0\n",
            "Eval num_timesteps=260000, episode_reward=13102617.20 +/- 7110.94\n",
            "Episode length: 30000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3e+04    |\n",
            "|    mean_reward     | 1.31e+07 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 260000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Score:  30.08758238670232\n",
            "Score:  2285.3173887743756\n",
            "Score:  7.062680199616653\n",
            "Score:  171.91419924184586\n",
            "Score:  485.8731547176628\n",
            "Score:  132.9918710792097\n",
            "Score:  1243.1411609854154\n",
            "Score:  16.157731240017643\n",
            "Score:  229.0236052639526\n",
            "Score:  945.0111712492912\n",
            "Score:  50.39610547531872\n",
            "Score:  1049.0716470518748\n",
            "Score:  9.874147040487344\n",
            "Score:  429.1073934564058\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 150      |\n",
            "|    ep_rew_mean     | 559      |\n",
            "| time/              |          |\n",
            "|    fps             | 154      |\n",
            "|    iterations      | 32       |\n",
            "|    time_elapsed    | 1696     |\n",
            "|    total_timesteps | 262144   |\n",
            "---------------------------------\n",
            "Score:  13.665015484580477\n",
            "Score:  573.1792699164193\n",
            "Score:  12.432986283446107\n",
            "Score:  4.75354852714908\n",
            "Score:  2524.865635719179\n",
            "Score:  7.197363353006487\n",
            "Score:  767.139522313329\n",
            "Score:  16.062143050991587\n",
            "Score:  834.5047316391027\n",
            "Score:  48.74048830737845\n",
            "Score:  8.219798670206139\n",
            "Score:  8.758789430195725\n",
            "check point:  1.0\n",
            "Score:  122.35793464792613\n",
            "Score:  12.181072242124127\n",
            "Score:  14.682659950895243\n",
            "Score:  466.94887045688574\n",
            "Score:  6366.9180408203265\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13158692.562730048\n",
            "check point:  60.0\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "Score:  10334435.81583638\n",
            "Score:  14.721457854206653\n",
            "Eval num_timesteps=265000, episode_reward=7831423.03 +/- 5656495.62\n",
            "Episode length: 18885.33 +/- 13391.80\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1.89e+04     |\n",
            "|    mean_reward          | 7.83e+06     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 265000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050841216 |\n",
            "|    clip_fraction        | 0.16         |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.515       |\n",
            "|    explained_variance   | 0.767        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 49.2         |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | 0.00493      |\n",
            "|    value_loss           | 107          |\n",
            "------------------------------------------\n",
            "Score:  134.12554040970642\n",
            "Score:  5.71261332340166\n",
            "Score:  100.39627182561225\n",
            "Score:  2963.2873218667723\n",
            "Score:  22.446025302554425\n",
            "Score:  46.34833123463573\n",
            "Score:  146.49374535429283\n",
            "Score:  26.532495395332937\n",
            "Score:  8.791790717313047\n",
            "Score:  68.71371991870237\n",
            "Score:  795.4009472657251\n",
            "Score:  215.76205571274286\n",
            "Score:  53.48749530721021\n",
            "Score:  17.855625434378812\n",
            "check point:  1.0\n",
            "Score:  158.0375277371619\n",
            "Score:  4596.556101880358\n",
            "Score:  170.22872377365246\n",
            "Score:  20.89290362812899\n",
            "Score:  248.56550590042093\n",
            "Score:  1047.7734242520846\n",
            "Score:  236.0243104359742\n",
            "Score:  714.1513812347948\n",
            "Score:  11.860250422674548\n",
            "Score:  526.8669964964154\n",
            "Score:  5.227740896745861\n",
            "Score:  30.549520863076523\n",
            "Score:  15.749850776061791\n",
            "Score:  977.7536348845902\n",
            "Score:  115.56244337521882\n",
            "Score:  239.4431651014886\n",
            "Score:  3305.9274552009956\n",
            "Score:  12.78595119726702\n",
            "Score:  2006.1886820223456\n",
            "Score:  7.795575922106718\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n",
            "check point:  47.0\n",
            "check point:  48.0\n",
            "check point:  49.0\n",
            "check point:  50.0\n",
            "check point:  51.0\n",
            "check point:  52.0\n",
            "check point:  53.0\n",
            "check point:  54.0\n",
            "check point:  55.0\n",
            "check point:  56.0\n",
            "check point:  57.0\n",
            "check point:  58.0\n",
            "check point:  59.0\n",
            "Score:  13161560.065280983\n",
            "check point:  60.0\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "Score:  366116.84632758395\n",
            "Eval num_timesteps=270000, episode_reward=4509626.91 +/- 6120511.55\n",
            "Episode length: 11690.00 +/- 13106.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.17e+04 |\n",
            "|    mean_reward     | 4.51e+06 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 270000   |\n",
            "---------------------------------\n",
            "Score:  138.05848496967485\n",
            "Score:  49.17055034834516\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 151      |\n",
            "|    ep_rew_mean     | 537      |\n",
            "| time/              |          |\n",
            "|    fps             | 137      |\n",
            "|    iterations      | 33       |\n",
            "|    time_elapsed    | 1971     |\n",
            "|    total_timesteps | 270336   |\n",
            "---------------------------------\n",
            "Score:  1003.475180340922\n",
            "Score:  32.931568738040696\n",
            "Score:  1766.4338216960602\n",
            "Score:  114.60860734093238\n",
            "Score:  65.29584905240002\n",
            "Score:  256.67541073462036\n",
            "Score:  198.99369815262125\n",
            "Score:  21.98942799417828\n",
            "Score:  15.332204937776446\n",
            "Score:  12.695987794865975\n",
            "Score:  17.99185442593195\n",
            "Score:  210.80021844471213\n",
            "Score:  987.2766098342051\n",
            "Score:  7.957792329487573\n",
            "Score:  6.584353115764364\n",
            "Score:  1467.423212049175\n",
            "Score:  162.91101684280324\n",
            "Score:  42.43126960975676\n",
            "Score:  80.13378003776046\n",
            "Score:  814.2832518505838\n",
            "Score:  13.197674462453193\n",
            "Score:  158.96759638788268\n",
            "Score:  884.7463976173876\n",
            "Score:  68.91025933534365\n",
            "Score:  11.590366300680458\n",
            "Score:  48.40983646612212\n",
            "Score:  177.32388847996788\n",
            "Score:  462.11429190987997\n",
            "Score:  76.2635132016587\n",
            "Score:  63.166064919670454\n",
            "Score:  675.507856651029\n",
            "Score:  24.047460345788423\n",
            "Score:  9.906028256308774\n",
            "Score:  44.949516247080375\n",
            "Score:  623.4518188525598\n",
            "Score:  7.67470720095751\n",
            "Score:  9.762413745255401\n",
            "Score:  442.85378947802553\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "Score:  274716.3422775633\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "Score:  569833.3275291653\n",
            "check point:  1.0\n",
            "Score:  13355.562527240481\n",
            "Eval num_timesteps=275000, episode_reward=285968.41 +/- 227320.38\n",
            "Episode length: 3896.00 +/- 2206.09\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 3.9e+03     |\n",
            "|    mean_reward          | 2.86e+05    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 275000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005755775 |\n",
            "|    clip_fraction        | 0.218       |\n",
            "|    clip_range           | 0.108       |\n",
            "|    entropy_loss         | -0.526      |\n",
            "|    explained_variance   | 0.786       |\n",
            "|    learning_rate        | 9.61e-05    |\n",
            "|    loss                 | 40.8        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | 0.00785     |\n",
            "|    value_loss           | 84.2        |\n",
            "-----------------------------------------\n",
            "Score:  84.49863733216146\n",
            "Score:  1901.1252402065581\n",
            "Score:  17.19436685129392\n",
            "Score:  8.18972261379987\n",
            "Score:  24.56534362008353\n",
            "Score:  29.022755452494394\n",
            "Score:  72.3007476147473\n",
            "Score:  15.282915330791196\n",
            "Score:  23.116403471866924\n",
            "Score:  14.375840486156939\n",
            "Score:  1889.4391271648867\n",
            "Score:  31.15255349413155\n",
            "Score:  194.8331119460551\n",
            "Score:  11.580700625480281\n",
            "Score:  544.187834535691\n",
            "Score:  667.6486892709672\n",
            "Score:  12.693092271722124\n",
            "Score:  120.3394026442101\n",
            "Score:  889.2835140564707\n",
            "Score:  942.7206786932013\n",
            "Score:  394.22436791223294\n",
            "Score:  105.35433579500534\n",
            "Score:  402.6206490098658\n",
            "Score:  178.50040083297063\n",
            "Score:  19.445551362530363\n",
            "Score:  3512.55054851382\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 145      |\n",
            "|    ep_rew_mean     | 488      |\n",
            "| time/              |          |\n",
            "|    fps             | 137      |\n",
            "|    iterations      | 34       |\n",
            "|    time_elapsed    | 2032     |\n",
            "|    total_timesteps | 278528   |\n",
            "---------------------------------\n",
            "Score:  67.63882475403368\n",
            "Score:  153.82329744197116\n",
            "Score:  547.0269100486901\n",
            "Score:  92.44445382562371\n",
            "Score:  80.395908610383\n",
            "Score:  13.332861247136583\n",
            "Score:  224.49618449570846\n",
            "Score:  58.50903091717655\n",
            "Score:  33.634772889675325\n",
            "Score:  378.6203979734896\n",
            "Score:  3395.1876955946877\n",
            "Score:  6.763248346275904\n",
            "Score:  11.142080404405434\n",
            "Score:  206.13350947551604\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "Score:  36420.83471012407\n",
            "Score:  5.927685602624458\n",
            "Eval num_timesteps=280000, episode_reward=12210.97 +/- 17119.16\n",
            "Episode length: 598.00 +/- 723.43\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 598          |\n",
            "|    mean_reward          | 1.22e+04     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 280000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043873135 |\n",
            "|    clip_fraction        | 0.191        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.529       |\n",
            "|    explained_variance   | 0.881        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 67.7         |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | 0.00311      |\n",
            "|    value_loss           | 44.8         |\n",
            "------------------------------------------\n",
            "Score:  7.587115128885736\n",
            "Score:  11.954756665442785\n",
            "Score:  107.6512852028785\n",
            "Score:  349.45672549903895\n",
            "Score:  617.5087277105878\n",
            "Score:  299.0165600711405\n",
            "Score:  1350.1173926296165\n",
            "Score:  6.983827224888506\n",
            "Score:  18.75480199928846\n",
            "Score:  91.62619685826341\n",
            "Score:  222.18903463915214\n",
            "Score:  37.39123167569001\n",
            "Score:  17.11311407147296\n",
            "Score:  233.78926720464978\n",
            "Score:  268.7543132972607\n",
            "Score:  271.83803083769067\n",
            "Score:  394.35606332462027\n",
            "Score:  473.8065634804487\n",
            "Score:  35.450261145584946\n",
            "Score:  364.71582923983755\n",
            "Score:  67.22876776408641\n",
            "Score:  682.808634568427\n",
            "Score:  118.023776905285\n",
            "Score:  371.55148063268143\n",
            "Score:  156.24578035499295\n",
            "Score:  27.854279746784332\n",
            "Score:  9.800093777978693\n",
            "Score:  23.270200522103703\n",
            "Score:  47.83523814371421\n",
            "Score:  117.04724905522325\n",
            "Score:  7.011489176789314\n",
            "Score:  565.166701321416\n",
            "Score:  21.815968389132696\n",
            "Score:  152.17118852591443\n",
            "Score:  101.69357462661023\n",
            "Score:  248.16528368684484\n",
            "Score:  175.79239850984368\n",
            "Score:  269.8638447411099\n",
            "Score:  12.884230139181605\n",
            "Score:  20.9284517660343\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "Score:  206208.3286598704\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "Score:  173103.29601537934\n",
            "Score:  8.734940252103426\n",
            "Eval num_timesteps=285000, episode_reward=126440.12 +/- 90416.29\n",
            "Episode length: 2431.67 +/- 1697.77\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.43e+03 |\n",
            "|    mean_reward     | 1.26e+05 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 285000   |\n",
            "---------------------------------\n",
            "Score:  488.78801012936293\n",
            "Score:  825.8749475647899\n",
            "Score:  44.23032831101378\n",
            "Score:  44.54893848733263\n",
            "Score:  15.450613023946781\n",
            "Score:  8.888475588689493\n",
            "Score:  109.19339026849688\n",
            "Score:  29.153506556271317\n",
            "Score:  23.858368832411724\n",
            "Score:  20.471196570794522\n",
            "Score:  2075.4434254187727\n",
            "Score:  16.718205711924746\n",
            "Score:  6.561705373883602\n",
            "Score:  11.148658597202719\n",
            "Score:  12.378765205600933\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 122      |\n",
            "|    ep_rew_mean     | 304      |\n",
            "| time/              |          |\n",
            "|    fps             | 137      |\n",
            "|    iterations      | 35       |\n",
            "|    time_elapsed    | 2084     |\n",
            "|    total_timesteps | 286720   |\n",
            "---------------------------------\n",
            "check point:  1.0\n",
            "Score:  196.81613481636367\n",
            "Score:  3691.007601861562\n",
            "Score:  33.87017477739934\n",
            "Score:  480.4544018875008\n",
            "Score:  365.90810297363515\n",
            "Score:  98.39319319442642\n",
            "Score:  274.50032376771907\n",
            "Score:  532.6306190778613\n",
            "Score:  83.29339385295484\n",
            "Score:  153.10862373078862\n",
            "Score:  248.34841287139554\n",
            "Score:  136.5397934542863\n",
            "Score:  6.907494945348628\n",
            "Score:  66.34039828398849\n",
            "Score:  126.22380849777313\n",
            "Score:  9.335488478575682\n",
            "Score:  204.29709469454198\n",
            "check point:  1.0\n",
            "Score:  100.52596731740205\n",
            "Score:  792.826856307634\n",
            "Score:  187.85689790771698\n",
            "Score:  37.245088906276564\n",
            "Score:  21.210702433908995\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "Score:  1071407.384866687\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "Score:  129860.26669816388\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "Score:  275820.0398072759\n",
            "Eval num_timesteps=290000, episode_reward=492362.56 +/- 413759.79\n",
            "Episode length: 5344.33 +/- 2384.88\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 5.34e+03     |\n",
            "|    mean_reward          | 4.92e+05     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 290000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044556507 |\n",
            "|    clip_fraction        | 0.173        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.514       |\n",
            "|    explained_variance   | 0.885        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 55.5         |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | 0.00252      |\n",
            "|    value_loss           | 42.7         |\n",
            "------------------------------------------\n",
            "Score:  223.25843590189137\n",
            "Score:  7511.203715911176\n",
            "Score:  114.6310355534824\n",
            "Score:  549.7211136665212\n",
            "Score:  66.72089758668982\n",
            "Score:  88.84571341320681\n",
            "Score:  43.62473990820655\n",
            "Score:  357.19671893452977\n",
            "Score:  572.0358869926367\n",
            "Score:  690.2776530839724\n",
            "Score:  87.27741706583033\n",
            "Score:  6.883135838283938\n",
            "Score:  128.72882277375\n",
            "Score:  10.486862864842097\n",
            "Score:  148.88107372501057\n",
            "Score:  8.038117194874047\n",
            "Score:  740.5508739983267\n",
            "Score:  19.401909516695483\n",
            "Score:  198.91040751968697\n",
            "Score:  38.51852227433518\n",
            "Score:  1436.67052946123\n",
            "Score:  424.9358372786779\n",
            "Score:  20.569377298744598\n",
            "Score:  47.017805721065066\n",
            "Score:  16.805604382765264\n",
            "Score:  1168.197827716949\n",
            "Score:  12.393464030696064\n",
            "Score:  80.7732885115069\n",
            "Score:  14.46236938732621\n",
            "Score:  96.82828509052248\n",
            "Score:  79.30056981694659\n",
            "Score:  476.2225527357878\n",
            "Score:  36.09646002767567\n",
            "Score:  20.542820786268685\n",
            "Score:  261.14852216970274\n",
            "Score:  145.7947132372121\n",
            "Score:  233.40965800045728\n",
            "Score:  11.702772876622074\n",
            "Score:  31.02287688988752\n",
            "Score:  161.35797293012797\n",
            "Score:  851.7730237058242\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 124      |\n",
            "|    ep_rew_mean     | 324      |\n",
            "| time/              |          |\n",
            "|    fps             | 136      |\n",
            "|    iterations      | 36       |\n",
            "|    time_elapsed    | 2154     |\n",
            "|    total_timesteps | 294912   |\n",
            "---------------------------------\n",
            "Score:  91.12551774106424\n",
            "Score:  41.06987619180081\n",
            "Score:  202.52972173524313\n",
            "Score:  14.721457854206653\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "Score:  2892075.0689844606\n",
            "Eval num_timesteps=295000, episode_reward=964097.44 +/- 1363286.06\n",
            "Episode length: 4788.33 +/- 6637.49\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 4.79e+03     |\n",
            "|    mean_reward          | 9.64e+05     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 295000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048517003 |\n",
            "|    clip_fraction        | 0.18         |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.508       |\n",
            "|    explained_variance   | 0.824        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 52.1         |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | 0.00413      |\n",
            "|    value_loss           | 74.2         |\n",
            "------------------------------------------\n",
            "Score:  21.746302187149734\n",
            "Score:  10.567541744269953\n",
            "Score:  1057.7069945044093\n",
            "Score:  179.82512469694575\n",
            "Score:  650.6189306171168\n",
            "Score:  1562.656565838304\n",
            "Score:  125.304513893313\n",
            "Score:  258.29908536816976\n",
            "Score:  164.77927292049762\n",
            "Score:  38.968343126217604\n",
            "Score:  47.23985006044509\n",
            "Score:  20.992846682683222\n",
            "Score:  11.603820471829518\n",
            "Score:  378.21254760969816\n",
            "Score:  103.18979619688047\n",
            "check point:  1.0\n",
            "Score:  103.37546796845105\n",
            "Score:  582.2171880482905\n",
            "Score:  4353.862750019945\n",
            "Score:  34.5631666574016\n",
            "Score:  561.0489111599103\n",
            "Score:  585.0901325461471\n",
            "Score:  136.56362784627314\n",
            "Score:  484.10336827679123\n",
            "Score:  17.073684506585693\n",
            "Score:  407.15415127102375\n",
            "Score:  12.186324138592193\n",
            "Score:  145.24466277726066\n",
            "Score:  42.440266765476\n",
            "Score:  1320.9057716341083\n",
            "Score:  1092.778986667226\n",
            "check point:  1.0\n",
            "Score:  11971.78853070457\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "Score:  543091.9958876339\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "Score:  19202.740866342276\n",
            "Eval num_timesteps=300000, episode_reward=191422.18 +/- 248685.64\n",
            "Episode length: 2763.33 +/- 2407.66\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.76e+03 |\n",
            "|    mean_reward     | 1.91e+05 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "Score:  778.5314252551722\n",
            "Score:  146.36101310801703\n",
            "Score:  168.80216561847038\n",
            "Score:  140.23751659543177\n",
            "Score:  96.61096630618505\n",
            "Score:  15.446600910696299\n",
            "Score:  3177.8273545755524\n",
            "Score:  1029.2821307363972\n",
            "Score:  11.852704911787123\n",
            "Score:  6.150159383822217\n",
            "Score:  426.07075009795125\n",
            "Score:  109.4252167713392\n",
            "Score:  318.8485948598703\n",
            "Score:  124.00432095112626\n",
            "Score:  20.868577960309523\n",
            "Score:  332.29034538349606\n",
            "Score:  2338.1473575638033\n",
            "Score:  95.62895122283398\n",
            "Score:  89.44004480733457\n",
            "Score:  113.43632751456961\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 143      |\n",
            "|    ep_rew_mean     | 428      |\n",
            "| time/              |          |\n",
            "|    fps             | 135      |\n",
            "|    iterations      | 37       |\n",
            "|    time_elapsed    | 2243     |\n",
            "|    total_timesteps | 303104   |\n",
            "---------------------------------\n",
            "Score:  251.02284442679573\n",
            "Score:  134.16890231848387\n",
            "Score:  1054.9619477035096\n",
            "Score:  816.6694125659384\n",
            "Score:  58.528855692036494\n",
            "check point:  1.0\n",
            "Score:  4077.507066278683\n",
            "Score:  383.60853342832627\n",
            "Score:  828.7940624024429\n",
            "Score:  448.58584341463745\n",
            "Score:  71.99671056552465\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "Score:  1151725.3353018356\n",
            "Score:  60.80598902426343\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "Score:  248122.2870222151\n",
            "Eval num_timesteps=305000, episode_reward=466636.14 +/- 494903.38\n",
            "Episode length: 4396.67 +/- 3619.63\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 4.4e+03      |\n",
            "|    mean_reward          | 4.67e+05     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 305000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047432184 |\n",
            "|    clip_fraction        | 0.175        |\n",
            "|    clip_range           | 0.108        |\n",
            "|    entropy_loss         | -0.498       |\n",
            "|    explained_variance   | 0.815        |\n",
            "|    learning_rate        | 9.61e-05     |\n",
            "|    loss                 | 33.2         |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | 0.00392      |\n",
            "|    value_loss           | 56.9         |\n",
            "------------------------------------------\n",
            "Score:  5.545326521536422\n",
            "Score:  947.7719778717924\n",
            "Score:  1287.7772419515275\n",
            "Score:  117.61405410916989\n",
            "Score:  1155.282498074723\n",
            "Score:  2407.336672874392\n",
            "Score:  257.91281536068647\n",
            "Score:  10.797686068270366\n",
            "Score:  24.194564158696867\n",
            "Score:  436.07853879242765\n",
            "Score:  195.14834187057116\n",
            "Score:  45.062296054261836\n",
            "Score:  7.150263059201741\n",
            "Score:  27.113103035462075\n",
            "Score:  6.521043622728701\n",
            "Score:  23.13829822203857\n",
            "Score:  16.12694929324727\n",
            "Score:  15.074270932412052\n",
            "Score:  329.2477722595098\n",
            "Score:  943.9264301134028\n",
            "Score:  114.01656668899103\n",
            "Score:  509.93531529387013\n",
            "Score:  8.336897042306148\n",
            "Score:  29.818489372161444\n",
            "Score:  527.0658661565514\n",
            "Score:  100.30921303230244\n",
            "Score:  19.71040884989483\n",
            "Score:  2537.4884364906734\n",
            "Score:  144.40450474171823\n",
            "Score:  2918.415305563744\n",
            "Score:  243.19592733113757\n",
            "Score:  11.159629954359728\n",
            "Score:  288.6761316273419\n",
            "Score:  27.314635956472696\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "Score:  170607.02481180138\n",
            "Score:  5.93137951505941\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "Score:  63989.69633239249\n",
            "Eval num_timesteps=310000, episode_reward=78200.88 +/- 70368.80\n",
            "Episode length: 1873.33 +/- 1411.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.87e+03 |\n",
            "|    mean_reward     | 7.82e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 310000   |\n",
            "---------------------------------\n",
            "Score:  109.46315781519972\n",
            "Score:  14.529760642620468\n",
            "Score:  86.331987938984\n",
            "Score:  2104.379516141838\n",
            "Score:  200.90989059851316\n",
            "Score:  185.33514801417445\n",
            "Score:  41.39605972946075\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 158      |\n",
            "|    ep_rew_mean     | 506      |\n",
            "| time/              |          |\n",
            "|    fps             | 134      |\n",
            "|    iterations      | 38       |\n",
            "|    time_elapsed    | 2321     |\n",
            "|    total_timesteps | 311296   |\n",
            "---------------------------------\n",
            "Score:  634.7905665351077\n",
            "Score:  1068.288710168027\n",
            "check point:  1.0\n",
            "Score:  3522.126242515332\n",
            "Score:  404.01004677641663\n",
            "Score:  18.38746196392036\n",
            "Score:  6.186190715226645\n",
            "Score:  55.74303438932409\n",
            "Score:  379.61368315646035\n",
            "Score:  7.241229334330067\n",
            "Score:  134.30631154433453\n",
            "Score:  134.5075992442428\n",
            "Score:  43.38661406763028\n",
            "Score:  70.810481896141\n",
            "Score:  11.335719037456235\n",
            "Score:  483.5262802895272\n",
            "Score:  975.6858555194743\n",
            "Score:  991.9742354633318\n",
            "Score:  20.619594881868313\n",
            "Score:  145.50291255192428\n",
            "Score:  1743.6946567169798\n",
            "Score:  773.402030295298\n",
            "Score:  5.14453226383511\n",
            "Score:  167.71172435434835\n",
            "Score:  7.268777478299236\n",
            "Score:  140.31398276663253\n",
            "check point:  1.0\n",
            "check point:  2.0\n",
            "check point:  3.0\n",
            "check point:  4.0\n",
            "check point:  5.0\n",
            "check point:  6.0\n",
            "check point:  7.0\n",
            "check point:  8.0\n",
            "check point:  9.0\n",
            "check point:  10.0\n",
            "check point:  11.0\n",
            "check point:  12.0\n",
            "check point:  13.0\n",
            "check point:  14.0\n",
            "check point:  15.0\n",
            "check point:  16.0\n",
            "check point:  17.0\n",
            "check point:  18.0\n",
            "check point:  19.0\n",
            "check point:  20.0\n",
            "check point:  21.0\n",
            "check point:  22.0\n",
            "check point:  23.0\n",
            "check point:  24.0\n",
            "check point:  25.0\n",
            "check point:  26.0\n",
            "check point:  27.0\n",
            "check point:  28.0\n",
            "check point:  29.0\n",
            "check point:  30.0\n",
            "check point:  31.0\n",
            "check point:  32.0\n",
            "check point:  33.0\n",
            "check point:  34.0\n",
            "check point:  35.0\n",
            "check point:  36.0\n",
            "check point:  37.0\n",
            "check point:  38.0\n",
            "check point:  39.0\n",
            "check point:  40.0\n",
            "check point:  41.0\n",
            "check point:  42.0\n",
            "check point:  43.0\n",
            "check point:  44.0\n",
            "check point:  45.0\n",
            "check point:  46.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-622b5d90668b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     model = training.train_ppo(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0meval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Fewer eval episodes on TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-6ca8023309f0>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(self, total_timesteps, save_freq, eval_freq, eval_episodes)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         self.model.learn(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Give access to local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Return False (stop training) if at least one callback returns False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_success_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             episode_rewards, episode_lengths = evaluate_policy(\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mepisode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode_counts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepisode_count_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         actions, states = model.predict(\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \"\"\"\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert to numpy, and reshape to the original action shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc, assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTaken\u001b[0m \u001b[0maction\u001b[0m \u001b[0maccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \"\"\"\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTorchObs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_features_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mlatent_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTorchObs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;31m# Here mean_actions are the flattened logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# Memory-optimized training setup\n",
        "def get_tpu_memory_info():\n",
        "    \"\"\"Get memory information from TPU device if available\"\"\"\n",
        "    # if TPU_AVAILABLE:\n",
        "    #     try:\n",
        "    #         # This is just for diagnostic purposes\n",
        "    #         import subprocess\n",
        "    #         result = subprocess.run(['python3', '-c', 'import torch_xla; print(torch_xla._XLAC._xla_get_memory_info(torch_xla._XLAC._xla_get_default_device()))'],\n",
        "    #                                stdout=subprocess.PIPE, text=True)\n",
        "    #         print(f\"TPU Memory Info: {result.stdout}\")\n",
        "    #     except:\n",
        "    #         print(\"Could not get detailed TPU memory info\")\n",
        "    pass\n",
        "\n",
        "# Display memory information\n",
        "get_tpu_memory_info()\n",
        "\n",
        "n_envs = 4\n",
        "batch_size = 64\n",
        "n_steps = 2048\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = False\n",
        "\n",
        "if do_optimization:\n",
        "    optuna_optimizer = Optuna_optimize(obs_type=\"game_screen\")\n",
        "    # # Force TPU memory cleanup before starting\n",
        "    # if TPU_AVAILABLE:\n",
        "    #     gc.collect()\n",
        "    #     xm.mark_step()\n",
        "\n",
        "    n_trials = 10\n",
        "    best_trial = optuna_optimizer.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    print(f\"best_trial found: {best_trial}\")\n",
        "else:\n",
        "    # Create trainer\n",
        "    training = Train(\n",
        "        n_steps=n_steps,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=n_envs,\n",
        "        level=1,\n",
        "        load_model=\"/content/ppo_balancing_ball_game_screen_25000_steps\",  # Start fresh with state-based env\n",
        "        obs_type='game_screen',\n",
        "    )\n",
        "    # Run training with memory-optimized settings\n",
        "    # Use fewer total timesteps for TPU to avoid memory issues\n",
        "    total_timesteps = 500000\n",
        "\n",
        "    model = training.train_ppo(\n",
        "        total_timesteps=total_timesteps,\n",
        "        eval_episodes=3,  # Fewer eval episodes on TPU\n",
        "        save_freq=5000,\n",
        "        eval_freq=5000\n",
        "    )\n",
        "\n",
        "    # # Force memory cleanup after training\n",
        "    # if TPU_AVAILABLE:\n",
        "    #     del model\n",
        "    #     gc.collect()\n",
        "    #     xm.mark_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "outputs": [],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "outputs": [],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        n_steps=1024,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=1  # Use 1 env for evaluation\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "        difficulty=\"medium\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4h3jBARKPld"
      },
      "outputs": [],
      "source": [
        "# run_standalone_game(render_mode=\"rgb_array_and_human_in_colab\", difficulty=\"medium\", window_x=1000, window_y=600)\n",
        "test_gym_env(difficulty=\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCFm3AoWs45R"
      },
      "outputs": [],
      "source": [
        "# Example of creating the environment with grayscale images\n",
        "env = BalancingBallEnv(\n",
        "    render_mode=\"rgb_array\",\n",
        "    difficulty=\"medium\",\n",
        "    fps=30,\n",
        "    obs_type=\"game_screen\",\n",
        "    image_size=(84, 84)  # Standard size for many RL frameworks\n",
        ")\n",
        "\n",
        "# Reset environment to get initial observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Print observation shape to verify\n",
        "print(f\"Observation shape: {obs.shape}\")  # Should be (84, 84, 3) for grayscale with 3 stacked frames\n",
        "\n",
        "# Display a sample observation (first frame only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(obs[:,:,0], cmap='gray')\n",
        "plt.title(\"Grayscale Observation (First Frame)\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
