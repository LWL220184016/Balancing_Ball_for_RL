{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V4.2 Update: in readme.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "e5a868df-a483-41d0-aaca-85f1ccd2ed01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch XLA not found, will attempt to install\n",
            "TPU support installed. Please restart the runtime now.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will attempt to install\")\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install pygame-ce pymunk stable-baselines3 stable-baselines3[extra] shimmy>=2.0 optuna\n",
        "!pip install -q cloud-tpu-client\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "b38e48c4-02cb-44a0-eb71-aa16092c0bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'=2.0'\t logs   models\t sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbqyvjZ1PZu",
        "outputId": "0f78a0ed-c9ca-439f-c362-bf4df4750ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/capture': No such file or directory\n",
            "rm: cannot remove '/content/game_history': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhEqO-xFu4AI"
      },
      "source": [
        "## Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gJwfQb_Yuz1I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "class Recorder:\n",
        "\n",
        "    def __init__(self, task: str = \"game_history_record\"):\n",
        "        \"\"\"\n",
        "        tasks:\n",
        "        1. game_history_record\n",
        "        2. temp_memory\n",
        "        \"\"\"\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \"\"\n",
        "        if task == \"game_history_record\":\n",
        "            collection_name = self.get_newest_record_name()\n",
        "            self.json_file_path = CURRENT_DIR + \"./game_history/\" + collection_name + \".json\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)\n",
        "\n",
        "        if os.path.exists(self.json_file_path):\n",
        "            print(\"Loading the json memory file\")\n",
        "            self.memory = self.load(self.json_file_path)\n",
        "        else:\n",
        "            print(\"The json memory file does not exist. Creating new file.\")\n",
        "            self.memory = {\"game_records\": []}  # Direct dictionary instead of json.loads\n",
        "            with open(self.json_file_path, \"w\") as f:\n",
        "                json.dump(self.memory, f)\n",
        "\n",
        "    def get(self):\n",
        "        print(\"Getting the json memory\")\n",
        "        return self.memory\n",
        "\n",
        "    def add_no_limit(self, data: float, ):\n",
        "        \"\"\"\n",
        "        Add a records.\n",
        "\n",
        "        Args:\n",
        "            role: The role of the sender (e.g., 'user', 'assistant')\n",
        "            message: The message content\n",
        "        \"\"\"\n",
        "        self.memory[\"game_records\"].append({\n",
        "            \"game_total_duration\": data,\n",
        "            \"timestamp\": str(datetime.datetime.now())\n",
        "        })\n",
        "\n",
        "        self.save(self.json_file_path)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump(self.memory, f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving memory to {file_path}: {e}\")\n",
        "\n",
        "    def load(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading memory from {file_path}: {e}\")\n",
        "            return {\"game_records\": []}\n",
        "\n",
        "    def get_newest_record_name(self) -> str:\n",
        "        \"\"\"\n",
        "        傳回最新的對話歷史資料和集的名稱 (game_YYYY_MM)\n",
        "            - 例如: \"game_2022-01\"\n",
        "        \"\"\"\n",
        "\n",
        "        this_month = datetime.datetime.now().strftime(\"%Y-%m\")\n",
        "        return \"record_\" + this_month"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnA8wZtosmeN"
      },
      "source": [
        "## Shapes & Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5wMhHMWCsmVD"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Shape:\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape: Optional[pymunk.Shape] = None,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a physical shape with associated body.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the body\n",
        "            velocity: Initial velocity (vx, vy) of the body\n",
        "            body: The pymunk Body to attach to this shape\n",
        "            shape: The pymunk Shape for collision detection\n",
        "        \"\"\"\n",
        "\n",
        "        self.body = body\n",
        "        self.default_position = position\n",
        "        self.default_velocity = velocity\n",
        "        self.body.position = position\n",
        "        self.body.velocity = velocity\n",
        "        self.default_angular_velocity = 0\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the body to its default position, velocity and angular velocity.\"\"\"\n",
        "        self.body.position = self.default_position\n",
        "        self.body.velocity = self.default_velocity\n",
        "        self.body.angular_velocity = self.default_angular_velocity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mfw5tBxBswyF"
      },
      "outputs": [],
      "source": [
        "import pymunk\n",
        "\n",
        "# from shapes.shape import Shape\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class Circle(Shape):\n",
        "\n",
        "    def __init__(\n",
        "                self,\n",
        "                position: Tuple[float, float] = (300, 100),\n",
        "                velocity: Tuple[float, float] = (0, 0),\n",
        "                body: Optional[pymunk.Body] = None,\n",
        "                shape_radio: float = 20,\n",
        "                shape_mass: float = 1,\n",
        "                shape_friction: float = 0.1,\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Initialize a circular physics object.\n",
        "\n",
        "        Args:\n",
        "            position: Initial position (x, y) of the circle\n",
        "            velocity: Initial velocity (vx, vy) of the circle\n",
        "            body: The pymunk Body to attach this circle to\n",
        "            shape_radio: Radius of the circle in pixels\n",
        "            shape_mass: Mass of the circle\n",
        "            shape_friction: Friction coefficient for the circle\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(position, velocity, body)\n",
        "        self.shape_radio = shape_radio\n",
        "        self.shape = pymunk.Circle(self.body, shape_radio)\n",
        "        self.shape.mass = shape_mass\n",
        "        self.shape.friction = shape_friction\n",
        "        self.shape.elasticity = 0.8  # Add some bounce to make the simulation more interesting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8d5fKltI62"
      },
      "source": [
        "## Game class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wiw5Rjks-xw",
        "outputId": "9aeea735-b250-4898-f3e5-6ea6fe4f1118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame-ce 2.5.3 (SDL 2.30.12, Python 3.11.12)\n"
          ]
        }
      ],
      "source": [
        "import pymunk\n",
        "import pygame\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import display, Image, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import IPython.display as ipd\n",
        "# from shapes.circle import Circle\n",
        "# from record import Recorder\n",
        "\n",
        "class BalancingBallGame:\n",
        "    \"\"\"\n",
        "    A physics-based balancing ball game that can run standalone or be used as a Gym environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Game constants\n",
        "\n",
        "\n",
        "    # Visual settings for indie style\n",
        "    BACKGROUND_COLOR = (41, 50, 65)  # Dark blue background\n",
        "    BALL_COLOR = (255, 213, 79)  # Bright yellow ball\n",
        "    PLATFORM_COLOR = (235, 64, 52)  # Red platform\n",
        "    PARTICLE_COLORS = [(252, 186, 3), (252, 127, 3), (252, 3, 3)]  # Fire-like particles\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 render_mode: str = \"human\",\n",
        "                 sound_enabled: bool = True,\n",
        "                 difficulty: str = \"medium\",\n",
        "                 window_x: int = 1000,\n",
        "                 window_y: int = 600,\n",
        "                 max_step: int = 30000,\n",
        "                 player_ball_speed: int = 5,\n",
        "                 reward_staying_alive: float = 0.1,\n",
        "                 reward_ball_centered: float = 0.2,\n",
        "                 penalty_falling: float = -10.0,\n",
        "                 fps: int = 120,\n",
        "                 platform_shape: str = \"circle\",\n",
        "                 platform_proportion: int = 0.4,\n",
        "                 capture_per_second: int = None,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialize the balancing ball game.\n",
        "\n",
        "        Args:\n",
        "            render_mode: \"human\" for visible window, \"rgb_array\" for gym env, \"headless\" for no rendering\n",
        "            sound_enabled: Whether to enable sound effects\n",
        "            difficulty: Game difficulty level (\"easy\", \"medium\", \"hard\")\n",
        "            max_step: 1 step = 1/fps, if fps = 120, 1 step = 1/120\n",
        "            reward_staying_alive: float = 0.1,\n",
        "            reward_ball_centered: float = 0.2,\n",
        "            penalty_falling: float = -10.0,\n",
        "            fps: frame per second\n",
        "            platform_proportion: platform_length = window_x * platform_proportion\n",
        "            capture_per_second: save game screen as a image every second, None means no capture\n",
        "        \"\"\"\n",
        "        # Game parameters\n",
        "        self.max_step = max_step\n",
        "        self.reward_staying_alive = reward_staying_alive\n",
        "        self.reward_ball_centered = reward_ball_centered\n",
        "        self.penalty_falling = penalty_falling\n",
        "        self.fps = fps\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "        self.player_ball_speed = player_ball_speed\n",
        "\n",
        "        self.recorder = Recorder(\"game_history_record\")\n",
        "        self.render_mode = render_mode\n",
        "        self.sound_enabled = sound_enabled\n",
        "        self.difficulty = difficulty\n",
        "\n",
        "        platform_length = int(window_x * platform_proportion)\n",
        "        self._get_x_axis_max_reward_rate(platform_length)\n",
        "\n",
        "        # Initialize physics space\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = (0, 1000)\n",
        "        self.space.damping = 0.9\n",
        "\n",
        "        # Create game bodies\n",
        "        self.dynamic_body = pymunk.Body()  # Ball body\n",
        "        self.kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
        "        self.kinematic_body.position = (self.window_x / 2, (self.window_y / 3) * 2)\n",
        "        self.default_kinematic_position = self.kinematic_body.position\n",
        "\n",
        "        # Create game objects\n",
        "        self._create_ball()\n",
        "        self._create_platform(platform_shape=platform_shape, platform_length=platform_length)\n",
        "        # self._create_platform(\"rectangle\")\n",
        "\n",
        "        # Add all objects to space\n",
        "        self.space.add(self.dynamic_body, self.kinematic_body,\n",
        "                       self.circle.shape, self.platform)\n",
        "\n",
        "        # Game state tracking\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Initialize Pygame if needed\n",
        "        if self.render_mode in [\"human\", \"rgb_array\", \"rgb_array_and_human\", \"rgb_array_and_human_in_colab\"]:\n",
        "            self._setup_pygame()\n",
        "        else:\n",
        "            print(\"render_mode is not human or rgb_array, so no pygame setup.\")\n",
        "\n",
        "        # Set difficulty parameters\n",
        "        self._apply_difficulty()\n",
        "        self.capture_per_second = capture_per_second\n",
        "\n",
        "        # Create folders for captures if needed\n",
        "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "        CURRENT_DIR = \".\"\n",
        "        os.makedirs(os.path.dirname(CURRENT_DIR + \"/capture/\"), exist_ok=True)\n",
        "\n",
        "    def _setup_pygame(self):\n",
        "        \"\"\"Set up PyGame for rendering\"\"\"\n",
        "        pygame.init()\n",
        "        self.frame_count = 0\n",
        "\n",
        "        if self.sound_enabled:\n",
        "            self._load_sounds()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.screen = pygame.display.set_mode((self.window_x, self.window_y))\n",
        "            pygame.display.set_caption(\"Balancing Ball - Indie Game\")\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\": # todo\n",
        "            from pymunk.pygame_util import DrawOptions\n",
        "\n",
        "            self.screen = pygame.Surface((self.window_x, self.window_y))  # Create hidden surface\n",
        "\n",
        "            # Set up display in Colab\n",
        "            self.draw_options = DrawOptions(self.screen)\n",
        "            html_display = ipd.HTML('''\n",
        "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
        "                </div>\n",
        "            ''')\n",
        "            self.display_handle = display(html_display, display_id='pygame_display')\n",
        "\n",
        "            self.last_update_time = time.time()\n",
        "            self.update_interval = 1.0 / 15  # Update display at 15 FPS to avoid overwhelming Colab\n",
        "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid render mode. Using headless mode.\")\n",
        "\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Create custom draw options for indie style\n",
        "\n",
        "    def _load_sounds(self):\n",
        "        \"\"\"Load game sound effects\"\"\"\n",
        "        try:\n",
        "            pygame.mixer.init()\n",
        "            self.sound_bounce = pygame.mixer.Sound(\"assets/bounce.wav\") if os.path.exists(\"assets/bounce.wav\") else None\n",
        "            self.sound_fall = pygame.mixer.Sound(\"assets/fall.wav\") if os.path.exists(\"assets/fall.wav\") else None\n",
        "        except Exception:\n",
        "            print(\"Sound loading error\")\n",
        "            self.sound_enabled = False\n",
        "            pass\n",
        "\n",
        "    def _create_ball(self):\n",
        "        \"\"\"Create the ball with physics properties\"\"\"\n",
        "        self.ball_radius = int(self.window_x / 67)\n",
        "        self.circle = Circle(\n",
        "            position=(self.window_x / 2, self.window_y / 3),\n",
        "            velocity=(0, 0),\n",
        "            body=self.dynamic_body,\n",
        "            shape_radio=self.ball_radius,\n",
        "            shape_friction=100,\n",
        "        )\n",
        "        # Store initial values for reset\n",
        "        self.default_ball_position = self.dynamic_body.position\n",
        "\n",
        "    def _create_platform(self,\n",
        "                         platform_shape: str = \"circle\",\n",
        "                         platform_length: int = 200\n",
        "                        ):\n",
        "        \"\"\"\n",
        "        Create the platform with physics properties\n",
        "        platform_shape: circle, rectangle\n",
        "        platform_length: Length of a rectangle or Diameter of a circle\n",
        "        \"\"\"\n",
        "        if platform_shape == \"circle\":\n",
        "            self.platform_length = platform_length / 2 # radius\n",
        "            self.platform = pymunk.Circle(self.kinematic_body, self.platform_length)\n",
        "            self.platform.mass = 1  # 质量对 Kinematic 物体无意义，但需要避免除以零错误\n",
        "            self.platform.friction = 0.7\n",
        "        elif platform_shape == \"rectangle\":\n",
        "            self.platform_length = platform_length\n",
        "            vs = [(-self.platform_length/2, -10),\n",
        "                (self.platform_length/2, -10),\n",
        "                (self.platform_length/2, 10),\n",
        "                (-self.platform_length/2, 10)]\n",
        "\n",
        "            self.platform = pymunk.Poly(self.kinematic_body, vs)\n",
        "        self.platform.friction = 0.7\n",
        "        self.platform_rotation = 0\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "    def _apply_difficulty(self):\n",
        "        \"\"\"Apply difficulty settings to the game\"\"\"\n",
        "        if self.difficulty == \"easy\":\n",
        "            self.max_platform_speed = 1.5\n",
        "            self.ball_elasticity = 0.5\n",
        "        elif self.difficulty == \"medium\":\n",
        "            self.max_platform_speed = 2.5\n",
        "            self.ball_elasticity = 0.7\n",
        "        else:  # hard\n",
        "            self.max_platform_speed = 3.5\n",
        "            self.ball_elasticity = 0.9\n",
        "\n",
        "        self.circle.shape.elasticity = self.ball_elasticity\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"Reset the game state and return the initial observation\"\"\"\n",
        "        # Reset physics objects\n",
        "        self.dynamic_body.position = self.default_ball_position\n",
        "        self.dynamic_body.velocity = (0, 0)\n",
        "        self.dynamic_body.angular_velocity = 0\n",
        "\n",
        "        self.kinematic_body.position = self.default_kinematic_position\n",
        "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
        "\n",
        "        # Reset game state\n",
        "        self.steps = 0\n",
        "        self.start_time = time.time()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.particles = []\n",
        "\n",
        "        # Return initial observation\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict]:\n",
        "        \"\"\"\n",
        "        Take a step in the game using the given action.\n",
        "\n",
        "        Args:\n",
        "            action: Float value between -1.0 and 1.0 controlling platform rotation\n",
        "\n",
        "        Returns:\n",
        "            observation: Game state observation\n",
        "            reward: Reward for this step\n",
        "            terminated: Whether episode is done\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        # Apply action to platform rotation\n",
        "        action_value = (0 - self.player_ball_speed) if action == 0 else self.player_ball_speed\n",
        "\n",
        "        self.dynamic_body.angular_velocity += action_value\n",
        "\n",
        "        # Step the physics simulation\n",
        "        self.space.step(1/self.fps)\n",
        "\n",
        "        # Update particle effects\n",
        "        self._update_particles()\n",
        "\n",
        "        # Check game state\n",
        "        self.steps += 1\n",
        "        terminated = False\n",
        "        reward = self.reward_staying_alive\n",
        "\n",
        "        # Calculate reward for keeping ball centered on platform\n",
        "        ball_x = self.dynamic_body.position[0]\n",
        "\n",
        "        # Check if ball falls off screen\n",
        "        if (self.dynamic_body.position[1] > self.kinematic_body.position[1] or\n",
        "            self.dynamic_body.position[0] < 0 or\n",
        "            self.dynamic_body.position[0] > self.window_x or\n",
        "            self.steps >= self.max_step\n",
        "            ):\n",
        "\n",
        "            print(\"Score: \", self.score)\n",
        "            terminated = True\n",
        "            reward = self.penalty_falling if self.steps < self.max_step else 0\n",
        "            self.game_over = True\n",
        "\n",
        "            result = {\n",
        "                \"game_total_duration\": f\"{time.time() - self.start_time:.2f}\",\n",
        "                \"score\": self.score,\n",
        "            }\n",
        "            self.recorder.add_no_limit(result)\n",
        "\n",
        "            if self.sound_enabled and self.sound_fall:\n",
        "                self.sound_fall.play()\n",
        "\n",
        "        step_reward = self._reward_calculator(ball_x)\n",
        "        self.score += step_reward\n",
        "        # print(\"ball_x: \", ball_x, \", self.score: \", self.score)\n",
        "        return self._get_observation(), step_reward, terminated\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Convert game state to observation for RL agent\"\"\"\n",
        "        # update particles and draw them\n",
        "        screen_data = self.render() # 获取数据\n",
        "\n",
        "        if self.capture_per_second is not None and self.frame_count % self.capture_per_second == 0:  # Every second at 60 FPS\n",
        "            pygame.image.save(self.screen, f\"capture/frame_{self.frame_count/60}.png\")\n",
        "\n",
        "        self.frame_count += 1\n",
        "        try:\n",
        "            screen_data = np.transpose(screen_data, (1, 0, 2))  # 转置以符合 (height, width, channels)\n",
        "            return screen_data\n",
        "        except ValueError:\n",
        "            return screen_data\n",
        "\n",
        "\n",
        "    def _update_particles(self):\n",
        "        \"\"\"Update particle effects for indie visual style\"\"\"\n",
        "        # Create new particles when ball hits platform\n",
        "        if abs(self.dynamic_body.position[1] - (self.kinematic_body.position[1] - 20)) < 5 and abs(self.dynamic_body.velocity[1]) > 100:\n",
        "            for _ in range(5):\n",
        "                self.particles.append({\n",
        "                    'x': self.dynamic_body.position[0],\n",
        "                    'y': self.dynamic_body.position[1] + self.ball_radius,\n",
        "                    'vx': random.uniform(-2, 2),\n",
        "                    'vy': random.uniform(1, 3),\n",
        "                    'life': 30,\n",
        "                    'size': random.uniform(2, 5),\n",
        "                    'color': random.choice(self.PARTICLE_COLORS)\n",
        "                })\n",
        "\n",
        "            if self.sound_enabled and self.sound_bounce:\n",
        "                self.sound_bounce.play()\n",
        "\n",
        "        # Update existing particles\n",
        "        for particle in self.particles[:]:\n",
        "            particle['x'] += particle['vx']\n",
        "            particle['y'] += particle['vy']\n",
        "            particle['life'] -= 1\n",
        "            if particle['life'] <= 0:\n",
        "                self.particles.remove(particle)\n",
        "\n",
        "    def render(self) -> Optional[np.ndarray]:\n",
        "        \"\"\"Render the current game state\"\"\"\n",
        "        if self.render_mode == \"headless\":\n",
        "            return None\n",
        "\n",
        "        # Clear screen with background color\n",
        "        self.screen.fill(self.BACKGROUND_COLOR)\n",
        "\n",
        "        # Custom drawing (for indie style)\n",
        "        self._draw_indie_style()\n",
        "\n",
        "\n",
        "        # Update display if in human mode\n",
        "        if self.render_mode == \"human\":\n",
        "            # Draw game information\n",
        "            self._draw_game_info()\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(self.fps)\n",
        "            return None\n",
        "\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            # Return RGB array for gym environment\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
        "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
        "\n",
        "        elif self.render_mode == \"rgb_array_and_human_in_colab\":\n",
        "            self.space.debug_draw(self.draw_options)\n",
        "            current_time = time.time()\n",
        "            if current_time - self.last_update_time >= self.update_interval:\n",
        "                # Convert Pygame surface to an image that can be displayed in Colab\n",
        "                buffer = BytesIO()\n",
        "                pygame.image.save(self.screen, buffer, 'PNG')\n",
        "                buffer.seek(0)\n",
        "                img_data = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "\n",
        "                # Update the HTML image\n",
        "                self.display_handle.update(ipd.HTML(f'''\n",
        "                    <div id=\"pygame-output\" style=\"width:100%;\">\n",
        "                        <img id=\"pygame-img\" src=\"data:image/png;base64,{img_data}\" style=\"width:100%;\">\n",
        "                    </div>\n",
        "                '''))\n",
        "\n",
        "                self.last_update_time = current_time\n",
        "            return pygame.surfarray.array3d(self.screen)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def _draw_indie_style(self):\n",
        "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
        "        # # Draw platform with gradient and glow\n",
        "        # platform_points = []\n",
        "        # for v in self.platform.get_vertices():\n",
        "        #     x, y = v.rotated(self.kinematic_body.angle) + self.kinematic_body.position\n",
        "        #     platform_points.append((int(x), int(y)))\n",
        "\n",
        "        # pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
        "        # pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
        "\n",
        "        platform_pos = (int(self.kinematic_body.position[0]), int(self.kinematic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
        "\n",
        "        # Draw rotation direction indicator\n",
        "        self._draw_rotation_indicator(platform_pos, self.platform_length, self.kinematic_body.angular_velocity)\n",
        "\n",
        "        # Draw ball with gradient and glow\n",
        "        ball_pos = (int(self.dynamic_body.position[0]), int(self.dynamic_body.position[1]))\n",
        "        pygame.draw.circle(self.screen, self.BALL_COLOR, ball_pos, self.ball_radius)\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
        "\n",
        "        # Draw particles\n",
        "        for particle in self.particles:\n",
        "            alpha = min(255, int(255 * (particle['life'] / 30)))\n",
        "            pygame.draw.circle(\n",
        "                self.screen,\n",
        "                particle['color'],\n",
        "                (int(particle['x']), int(particle['y'])),\n",
        "                int(particle['size'])\n",
        "            )\n",
        "\n",
        "    def _draw_rotation_indicator(self, position, radius, angular_velocity):\n",
        "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
        "        # Only draw the indicator if there's some rotation\n",
        "        if abs(angular_velocity) < 0.1:\n",
        "            return\n",
        "\n",
        "        # Calculate indicator properties based on angular velocity\n",
        "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
        "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
        "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
        "\n",
        "        # Draw arrow indicators along the platform's circumference\n",
        "        start_angle = self.kinematic_body.angle\n",
        "\n",
        "        for i in range(num_arrows):\n",
        "            # Calculate arrow position\n",
        "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
        "\n",
        "            # Calculate arrow start and end points\n",
        "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
        "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
        "\n",
        "            # Determine arrow direction based on angular velocity\n",
        "            if angular_velocity > 0:  # Clockwise\n",
        "                arrow_end_angle = arrow_angle + 0.3\n",
        "            else:  # Counter-clockwise\n",
        "                arrow_end_angle = arrow_angle - 0.3\n",
        "\n",
        "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
        "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
        "\n",
        "            # Draw arrow line\n",
        "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
        "\n",
        "            # Draw arrowhead\n",
        "            arrowhead_size = 7\n",
        "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)\n",
        "\n",
        "    def _draw_game_info(self):\n",
        "        \"\"\"Draw game information on screen\"\"\"\n",
        "        # Create texts\n",
        "        time_text = f\"Time: {time.time() - self.start_time:.1f}\"\n",
        "        score_text = f\"Score: {self.score}\"\n",
        "\n",
        "        # Render texts\n",
        "        time_surface = self.font.render(time_text, True, (255, 255, 255))\n",
        "        score_surface = self.font.render(score_text, True, (255, 255, 255))\n",
        "\n",
        "        # Draw text backgrounds\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (5, 5, time_surface.get_width() + 10, time_surface.get_height() + 5))\n",
        "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
        "                        (self.window_x - score_surface.get_width() - 15, 5,\n",
        "                         score_surface.get_width() + 10, score_surface.get_height() + 5))\n",
        "\n",
        "        # Draw texts\n",
        "        self.screen.blit(time_surface, (10, 10))\n",
        "        self.screen.blit(score_surface, (self.window_x - score_surface.get_width() - 10, 10))\n",
        "\n",
        "        # Draw game over screen\n",
        "        if self.game_over:\n",
        "            game_over_text = \"GAME OVER - Press R to restart\"\n",
        "            game_over_surface = self.font.render(game_over_text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw semi-transparent background\n",
        "            overlay = pygame.Surface((self.window_x, self.window_y), pygame.SRCALPHA)\n",
        "            overlay.fill((0, 0, 0, 128))\n",
        "            self.screen.blit(overlay, (0, 0))\n",
        "\n",
        "            # Draw text\n",
        "            self.screen.blit(game_over_surface,\n",
        "                           (self.window_x/2 - game_over_surface.get_width()/2,\n",
        "                            self.window_y/2 - game_over_surface.get_height()/2))\n",
        "\n",
        "    def _get_x_axis_max_reward_rate(self, platform_length):\n",
        "        \"\"\"\n",
        "        ((self.platform_length / 2) - 5) for calculate the distance to the\n",
        "        center of game window coordinates. The closer you are, the higher the reward.\n",
        "\n",
        "        When the ball is to be 10 points away from the center coordinates,\n",
        "        it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        \"\"\"\n",
        "        self.reward_width = (platform_length / 2) - 5\n",
        "        self.x_axis_max_reward_rate = 2 / self.reward_width\n",
        "        print(\"self.x_axis_max_reward_rate: \", self.x_axis_max_reward_rate)\n",
        "\n",
        "    # def _reward_calculator(self, ball_x):\n",
        "    #     # score & reward\n",
        "    #     if self.steps < 2000:\n",
        "    #         step_reward = self.steps * 0.01\n",
        "    #     elif self.steps < 5000:\n",
        "    #         step_reward = self.steps * 0.03\n",
        "    #     else:\n",
        "    #         step_reward = self.steps * 0.05\n",
        "\n",
        "    #     rw = abs(ball_x - self.window_x/2)\n",
        "    #     if rw < self.reward_width:\n",
        "    #         x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
        "    #         step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
        "    #         return step_reward\n",
        "    #     else:\n",
        "    #         return 0\n",
        "\n",
        "    def _reward_calculator(self, ball_x):\n",
        "        # score & reward\n",
        "        step_reward = 1/100\n",
        "\n",
        "        rw = abs(ball_x - self.window_x/2)\n",
        "        if rw < self.reward_width:\n",
        "            x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
        "            step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
        "\n",
        "            if self.steps % 500 == 0:\n",
        "                step_reward += self.steps/100\n",
        "\n",
        "            return step_reward\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the game and clean up resources\"\"\"\n",
        "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
        "            pygame.quit()\n",
        "\n",
        "    def run_standalone(self):\n",
        "        \"\"\"Run the game in standalone mode with keyboard controls\"\"\"\n",
        "        if self.render_mode not in [\"human\"]:\n",
        "            raise ValueError(\"Standalone mode requires render_mode='human'\")\n",
        "\n",
        "        running = True\n",
        "        while running:\n",
        "            # Handle events\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.QUIT:\n",
        "                    running = False\n",
        "                elif event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_r and self.game_over:\n",
        "                        self.reset()\n",
        "\n",
        "            # Process keyboard controls\n",
        "            keys = pygame.key.get_pressed()\n",
        "            action = 0\n",
        "            if keys[pygame.K_LEFT]:\n",
        "                action = 0 - self.player_ball_speed\n",
        "            if keys[pygame.K_RIGHT]:\n",
        "                action = self.player_ball_speed\n",
        "\n",
        "            # Take game step\n",
        "            if not self.game_over:\n",
        "                self.step(action)\n",
        "\n",
        "            # Render\n",
        "            self.render()\n",
        "\n",
        "        self.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlW6s_8EKPlb"
      },
      "source": [
        "## Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "43f8apvbKPlb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjobL-nozI81"
      },
      "source": [
        "## GYM env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBzvHTN1zJu1"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "class BalancingBallEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for the Balancing Ball game\n",
        "    \"\"\"\n",
        "    metadata = {'render_modes': ['human', 'rgb_array']}\n",
        "\n",
        "    def __init__(self, render_mode=\"rgb_array\", difficulty=\"medium\", fps=30):\n",
        "        super(BalancingBallEnv, self).__init__()\n",
        "\n",
        "        # Action space: discrete - 0: left, 1: right\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Initialize game\n",
        "        self.window_x = 300\n",
        "        self.window_y = 180\n",
        "        self.platform_shape = \"circle\"\n",
        "        self.platform_proportion = 0.333\n",
        "\n",
        "        self.stack_size = 3  # Number of frames to stack\n",
        "        self.observation_stack = []  # Initialize the stack\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.game = BalancingBallGame(\n",
        "            render_mode=render_mode,\n",
        "            sound_enabled=(render_mode == \"human\"),\n",
        "            difficulty=difficulty,\n",
        "            window_x = self.window_x,\n",
        "            window_y = self.window_y,\n",
        "            fps = fps,\n",
        "            platform_shape = self.platform_shape,\n",
        "            platform_proportion = self.platform_proportion,\n",
        "        )\n",
        "\n",
        "        # Image observation space (RGB) with stacked frames\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255,\n",
        "            shape=(self.window_y, self.window_x, 3 * self.stack_size),  # For stacked frames\n",
        "            dtype=np.uint8\n",
        "        )\n",
        "\n",
        "        # Platform_length /= 2 when for calculate the distance to the\n",
        "        # center of game window coordinates. The closer you are, the higher the reward.\n",
        "        self.platform_reward_length = (self.game.platform_length / 2) - 5\n",
        "\n",
        "        # When the ball is to be 10 points away from the center coordinates,\n",
        "        # it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
        "        self.x_axis_max_reward_rate = 0.5 / self.platform_reward_length\n",
        "\n",
        "        self.step_start_timestamp = None\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        # total_step_duration = datetime.datetime.now() - self.step_start_timestamp if self.step_start_timestamp else None\n",
        "        # print(\"1total_step_duration: \", total_step_duration)\n",
        "        # self.step_start_timestamp = datetime.datetime.now()\n",
        "\n",
        "        # Take step in the game\n",
        "        for _ in range(6):\n",
        "            obs, step_reward, terminated = self.game.step(action)\n",
        "\n",
        "        # step_end_timestamp = datetime.datetime.now()\n",
        "        # step_duration = (step_end_timestamp - self.step_start_timestamp).total_seconds()\n",
        "        # print(\"2step_duration: \", step_duration)\n",
        "\n",
        "        # Stack the frames\n",
        "        self.observation_stack.append(obs)\n",
        "        if len(self.observation_stack) > self.stack_size:\n",
        "            self.observation_stack.pop(0)  # Remove the oldest frame\n",
        "\n",
        "        # If the stack isn't full yet, pad it with the current frame\n",
        "        while len(self.observation_stack) < self.stack_size:\n",
        "            self.observation_stack.insert(0, obs)  # Pad with current frame at the beginning\n",
        "\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        # stack_frames_duration = (datetime.datetime.now() - step_end_timestamp).total_seconds()\n",
        "        # print(\"3stack_frames_duration: \", stack_frames_duration)\n",
        "\n",
        "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
        "        return stacked_obs, step_reward, terminated, False, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"Reset the environment\"\"\"\n",
        "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
        "\n",
        "        observation = self.game.reset()\n",
        "\n",
        "        # Reset the observation stack\n",
        "        self.observation_stack = []\n",
        "\n",
        "        # Fill the stack with the initial observation\n",
        "        for _ in range(self.stack_size):\n",
        "            self.observation_stack.append(observation)\n",
        "\n",
        "        # Create stacked observation\n",
        "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
        "\n",
        "        info = {}\n",
        "        return stacked_obs, info\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the environment\"\"\"\n",
        "        return self.game.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        self.game.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgvyLDYKPlb"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ebr3cvEiKPlc"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "def run_standalone_game(render_mode=\"human\", difficulty=\"medium\", capture_per_second=3):\n",
        "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "    window_x = 1000\n",
        "    window_y = 600\n",
        "    platform_shape = \"circle\"\n",
        "    platform_proportion = 0.333\n",
        "\n",
        "    game = BalancingBallGame(\n",
        "        render_mode = render_mode,\n",
        "        difficulty = difficulty,\n",
        "        window_x = window_x,\n",
        "        window_y = window_y,\n",
        "        platform_shape = platform_shape,\n",
        "        platform_proportion = platform_proportion,\n",
        "        fps = 30,\n",
        "        capture_per_second = 3,\n",
        "    )\n",
        "\n",
        "    game.run_standalone()\n",
        "\n",
        "def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
        "    \"\"\"Test the OpenAI Gym environment\"\"\"\n",
        "    import time\n",
        "    # from gym_env import BalancingBallEnv\n",
        "\n",
        "    fps = 30\n",
        "    env = BalancingBallEnv(\n",
        "        render_mode=\"human\",\n",
        "        difficulty=difficulty,\n",
        "        fps=fps,\n",
        "    )\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample a random action (for testing only)\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            # Render\n",
        "            env.render()\n",
        "\n",
        "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticCnnPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Add the game directory to the system path\n",
        "# sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"game_base_files_test\"))\n",
        "\n",
        "# from gym_env import BalancingBallEnv\n",
        "\n",
        "# support render_mode: human, rgb_array, rgb_array_and_human, rgb_array_and_human_in_colab\n",
        "\n",
        "class Train:\n",
        "    def __init__(self,\n",
        "                 learning_rate=0.0003,\n",
        "                 n_steps=2048,\n",
        "                 batch_size=64,\n",
        "                 n_epochs=10,\n",
        "                 gamma=0.99,\n",
        "                 gae_lambda=0.95,\n",
        "                 ent_coef=0.01,\n",
        "                 vf_coef=0.5,\n",
        "                 max_grad_norm=0.5,\n",
        "                 policy_kwargs=None,\n",
        "                 n_envs=4,\n",
        "                 difficulty=\"medium\",\n",
        "                 load_model=None,\n",
        "                 log_dir=\"./logs/\",\n",
        "                 model_dir=\"./models/\",\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        self.log_dir = log_dir\n",
        "        self.model_dir = model_dir\n",
        "        self.n_envs = n_envs\n",
        "\n",
        "        # Setup environments\n",
        "        # support render_mode: human, rgb_array, rgb_array_and_human, rgb_array_and_human_in_colab\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        # Apply VecTransposeImage to correctly handle image observations\n",
        "        self.env = VecTransposeImage(env)\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = VecTransposeImage(eval_env)\n",
        "\n",
        "        # Define policy kwargs if not provided\n",
        "        if policy_kwargs is None:\n",
        "            policy_kwargs = {\n",
        "                \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "            }\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=log_dir,\n",
        "            )\n",
        "        else:\n",
        "            self.model = PPO(\n",
        "                policy=ActorCriticCnnPolicy,\n",
        "                env=self.env,\n",
        "                learning_rate=learning_rate,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=gamma,\n",
        "                gae_lambda=gae_lambda,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=log_dir,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=1,\n",
        "            )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self,\n",
        "                  total_timesteps=1000000,\n",
        "                  save_freq=10000,\n",
        "                  eval_freq=10000,\n",
        "                  eval_episodes=5,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball\"\n",
        "        )\n",
        "\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=eval_freq // self.n_envs,\n",
        "            n_eval_episodes=eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=[checkpoint_callback, eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final\")\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10, difficulty=\"medium\"):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def optuna_parameter_tuning(self, n_trials):\n",
        "        print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "        pruner = optuna.pruners.HyperbandPruner(\n",
        "            min_resource=100,        # 最小资源量\n",
        "            max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "            reduction_factor=3     # 折减因子 (eta)\n",
        "        )\n",
        "\n",
        "        # 建立 study 物件，並指定剪枝器\n",
        "        study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "        # 執行優化\n",
        "        study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "        # 分析結果\n",
        "        print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "        print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "        import pandas as pd\n",
        "        df = study.trials_dataframe()\n",
        "        print(df.head())\n",
        "\n",
        "\n",
        "    # def evaluate_policy(self, model, n_eval_episodes=10):\n",
        "    #     \"\"\"\n",
        "    #     評估強化學習策略的函數。\n",
        "\n",
        "    #     Args:\n",
        "    #         model: 要評估的 Stable Baselines3 模型。\n",
        "    #         env: 用於評估的環境。\n",
        "    #         n_eval_episodes: 要運行的 episode 數量。\n",
        "\n",
        "    #     Returns:\n",
        "    #         平均回報。\n",
        "    #     \"\"\"\n",
        "    #     rewards = []\n",
        "    #     for _ in range(n_eval_episodes):\n",
        "    #         obs = self.env.reset()[0]  # 注意gymnasium的reset()返回值\n",
        "    #         done = False\n",
        "    #         total_reward = 0\n",
        "    #         while not done:\n",
        "    #             action, _ = model.predict(obs, deterministic=True)  # 使用確定性策略\n",
        "    #             obs, reward, terminated, truncated, _ = self.env.step(action)\n",
        "    #             done = terminated or truncated\n",
        "    #             total_reward += reward\n",
        "    #         rewards.append(total_reward)\n",
        "    #     return sum(rewards) / n_eval_episodes\n",
        "\n",
        "\n",
        "    def objective(self, trial):\n",
        "        import gc\n",
        "\n",
        "        # 1. 建議超參數\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 0.9, 0.999)\n",
        "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "        gae_lambda = trial.suggest_float('gae_lambda', 0.5, 2)\n",
        "        ent_coef = trial.suggest_float('ent_coef', 0.005, 0.05)\n",
        "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "        features_dim = trial.suggest_categorical('features_dim', [32, 64, 128, 256, 512])\n",
        "        policy_kwargs = {\n",
        "            \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "        }\n",
        "\n",
        "\n",
        "        n_steps=2048\n",
        "        batch_size=64\n",
        "        n_epochs=10\n",
        "        # gamma=0.99\n",
        "        # gae_lambda=0.95\n",
        "        # ent_coef=0.01\n",
        "        # vf_coef=0.5\n",
        "        max_grad_norm=0.5\n",
        "        # policy_kwargs = {\n",
        "        #     \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "        # }\n",
        "\n",
        "        # 2. 建立環境\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=\"medium\"),\n",
        "            n_envs=1\n",
        "        )\n",
        "\n",
        "        # 3. 建立模型\n",
        "        model = PPO(\n",
        "                policy=ActorCriticCnnPolicy,\n",
        "                env=env,\n",
        "                learning_rate=learning_rate,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=gamma,\n",
        "                clip_range=clip_range,\n",
        "                gae_lambda=gae_lambda,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=None,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=0,\n",
        "            )\n",
        "\n",
        "\n",
        "        try:\n",
        "            # 4. 訓練模型\n",
        "            model.learn(total_timesteps=7000)\n",
        "            # 5. 評估模型\n",
        "            mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "        finally:\n",
        "            # Always cleanup\n",
        "            env.close()\n",
        "            del model\n",
        "            del env\n",
        "            gc.collect()\n",
        "            \n",
        "            if TPU_AVAILABLE:\n",
        "                import torch_xla.core.xla_model as xm\n",
        "                xm.mark_step()\n",
        "        \n",
        "        return mean_reward\n",
        "\n",
        "# import argparse\n",
        "\n",
        "# parser = argparse.ArgumentParser(description=\"Train or evaluate PPO agent for Balancing Ball\")\n",
        "# parser.add_argument(\"--mode\", type=str, default=\"train\", choices=[\"train\", \"eval\"],\n",
        "#                     help=\"Mode: 'train' to train model, 'eval' to evaluate\")\n",
        "# parser.add_argument(\"--timesteps\", type=int, default=1000000,\n",
        "#                     help=\"Total timesteps for training\")\n",
        "# parser.add_argument(\"--difficulty\", type=str, default=\"medium\",\n",
        "#                     choices=[\"easy\", \"medium\", \"hard\"],\n",
        "#                     help=\"Game difficulty\")\n",
        "# parser.add_argument(\"--load_model\", type=str, default=None,\n",
        "#                     help=\"Path to model to load for continued training or evaluation\")\n",
        "# parser.add_argument(\"--n_envs\", type=int, default=4,\n",
        "#                     help=\"Number of parallel environments for training\")\n",
        "# parser.add_argument(\"--eval_episodes\", type=int, default=5,\n",
        "#                     help=\"Number of episodes for evaluation\")\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         difficulty=args.difficulty,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#             difficulty=args.difficulty\n",
        "#         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ndr9hGp2CZzF",
        "outputId": "4da38106-7eb1-49fd-b953-53c1ce32f5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The json memory file does not exist. Creating new file.\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Using cuda device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-12 14:02:53,898] A new study created in memory with name: no-name-0558c2b2-16c4-47fc-9d4f-fff7b3b2c0d4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using optuna for automatic parameter tuning, it will create a new model\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Score:  10.58200549897589\n",
            "Score:  21.839937961116092\n",
            "Score:  15.46135609867883\n",
            "Score:  13.441658995288066\n",
            "Score:  11.202399055226133\n",
            "Score:  6.720618257289581\n",
            "Score:  7.65032571822089\n",
            "Score:  11.137315873920867\n",
            "Score:  7.966860360152511\n",
            "Score:  15.702145986305867\n",
            "Score:  6.066812296516093\n",
            "Score:  13.761728998524514\n",
            "Score:  11.027374930535172\n",
            "Score:  11.259655908801601\n",
            "Score:  5.071420683158251\n",
            "Score:  45.593215437373466\n",
            "Score:  13.898310817136618\n",
            "Score:  27.799982332998827\n",
            "Score:  7.168332434490525\n",
            "Score:  6.178028765811609\n",
            "Score:  7.396778060666305\n",
            "Score:  5.365709402062415\n",
            "Score:  43.53634315699565\n",
            "Score:  10.647437260728314\n",
            "Score:  11.471240347606942\n",
            "Score:  5.287204395252032\n",
            "Score:  26.800407991590497\n",
            "Score:  24.65261345950817\n",
            "Score:  7.981604069731833\n",
            "Score:  7.88141418217846\n",
            "Score:  8.130377782966045\n",
            "Score:  9.292223378399358\n",
            "Score:  16.823737429393496\n",
            "Score:  21.925237897335304\n",
            "Score:  15.445210729423735\n",
            "Score:  12.316932577389572\n",
            "Score:  10.270266801019947\n",
            "Score:  7.819728120951825\n",
            "Score:  13.848340223326234\n",
            "Score:  9.835312992109964\n",
            "Score:  8.576340313261396\n",
            "Score:  5.373744058604766\n",
            "Score:  17.89873786109565\n",
            "Score:  9.142901175302404\n",
            "Score:  9.863223705547572\n",
            "Score:  26.804055875707974\n",
            "Score:  12.567574500000013\n",
            "Score:  18.808261726296166\n",
            "Score:  11.48792900970876\n",
            "Score:  11.075490127438062\n",
            "Score:  9.952275709918625\n",
            "Score:  8.041844740843203\n",
            "Score:  16.33037603530349\n",
            "Score:  5.758333981191591\n",
            "Score:  10.071791970508288\n",
            "Score:  14.567251912745391\n",
            "Score:  38.153702282556296\n",
            "Score:  6.515337936436602\n",
            "Score:  8.416697310831687\n",
            "Score:  6.091029148554054\n",
            "Score:  7.1302419356978195\n",
            "Score:  6.6504830892087705\n",
            "Score:  14.252071744620542\n",
            "Score:  9.268352935859076\n",
            "Score:  7.393570858977292\n",
            "Score:  8.5382914680014\n",
            "Score:  13.120870545346174\n",
            "Score:  6.832733374888671\n",
            "Score:  8.618270790525596\n",
            "Score:  5.101047372474067\n",
            "Score:  13.046838864608777\n",
            "Score:  14.997063778651786\n",
            "Score:  10.909807736393475\n",
            "Score:  6.209135244310707\n",
            "Score:  13.59517771922303\n",
            "Score:  12.696075606523843\n",
            "Score:  11.333699515874521\n",
            "Score:  14.370000383420045\n",
            "Score:  4.714442415807696\n",
            "Score:  10.197239353821292\n",
            "Score:  8.138082025414757\n",
            "Score:  13.65069857727778\n",
            "Score:  9.485339075742335\n",
            "Score:  4.249878620782677\n",
            "Score:  6.496355392451353\n",
            "Score:  19.341065994076722\n",
            "Score:  21.245732682663345\n",
            "Score:  5.615160337430302\n",
            "Score:  5.615225833903238\n",
            "Score:  4.411055378923578\n",
            "Score:  7.8003896171900555\n",
            "Score:  5.112889370816735\n",
            "Score:  4.865938659861143\n",
            "Score:  11.287102850645285\n",
            "Score:  3.7954528151167697\n",
            "Score:  6.369573062589838\n",
            "Score:  9.568520575626216\n",
            "Score:  8.00968574203762\n",
            "Score:  7.101664847270369\n",
            "Score:  4.239835988207788\n",
            "Score:  6.388939305032046\n",
            "Score:  21.92061563111238\n",
            "Score:  32.270343203703256\n",
            "Score:  6.1440646737733555\n",
            "Score:  17.70612065176801\n",
            "Score:  11.005608037029397\n",
            "Score:  12.357784986150326\n",
            "Score:  26.13492412284682\n",
            "Score:  24.672074459939502\n",
            "Score:  12.291993632983624\n",
            "Score:  6.957372656359793\n",
            "Score:  18.22961824936047\n",
            "Score:  9.321433801693887\n",
            "Score:  51.27577931158493\n",
            "Score:  15.359621122209916\n",
            "Score:  7.201435472661177\n",
            "Score:  7.256507029818185\n",
            "Score:  7.262068082621833\n",
            "Score:  12.39792572819836\n",
            "Score:  30.207383041806416\n",
            "Score:  18.235099680331086\n",
            "Score:  15.168145982152216\n",
            "Score:  8.485724080483335\n",
            "Score:  4.737341019471051\n",
            "Score:  9.65500257534241\n",
            "Score:  4.370624503300578\n",
            "Score:  16.710245402668455\n",
            "Score:  3.859111458683106\n",
            "Score:  4.2934016717445\n",
            "Score:  6.657326778283943\n",
            "Score:  22.203244667479186\n",
            "Score:  7.490519218248212\n",
            "Score:  3.9176415931560933\n",
            "Score:  29.293522406141875\n",
            "Score:  6.3554275673991185\n",
            "Score:  4.689027482712121\n",
            "Score:  6.170115593240787\n",
            "Score:  12.073295800377718\n",
            "Score:  5.436887485709051\n",
            "Score:  8.12623051244038\n",
            "Score:  5.488089890441997\n",
            "Score:  5.38754946823205\n",
            "Score:  13.809885790518026\n",
            "Score:  16.978580516519987\n",
            "Score:  32.055466907314205\n",
            "Score:  4.778684378834888\n",
            "Score:  3.8665410435388536\n",
            "Score:  5.419548043658186\n",
            "Score:  7.301101594302865\n",
            "Score:  4.5237520199046894\n",
            "Score:  6.149128263288633\n",
            "Score:  46.0887350508221\n",
            "Score:  12.055525235766954\n",
            "Score:  15.42729319864838\n",
            "Score:  8.120573850132207\n",
            "Score:  7.76315267581622\n",
            "Score:  12.00705264268743\n",
            "Score:  13.0527398935804\n",
            "Score:  3.858723095307538\n",
            "Score:  11.690210709946726\n",
            "Score:  15.364889588342663\n",
            "Score:  29.366604350894033\n",
            "Score:  14.59338477398834\n",
            "Score:  4.568644879534009\n",
            "Score:  16.420470141101895\n",
            "Score:  9.459715532260363\n",
            "Score:  6.172883761763216\n",
            "Score:  16.49011709629091\n",
            "Score:  9.65274965536798\n",
            "Score:  4.204932317486579\n",
            "Score:  7.998110286112707\n",
            "Score:  4.085256341927989\n",
            "Score:  40.014919899856466\n",
            "Score:  14.436850450096875\n",
            "Score:  24.540445929895345\n",
            "Score:  6.6993887979193785\n",
            "Score:  17.593471245913427\n",
            "Score:  13.078771347129559\n",
            "Score:  5.940185674662124\n",
            "Score:  30.39027948077431\n",
            "Score:  15.016644456161721\n",
            "Score:  10.914284666580809\n",
            "Score:  7.70177583171597\n",
            "Score:  7.44902607543555\n",
            "Score:  4.570546924350855\n",
            "Score:  12.247569347790796\n",
            "Score:  7.780290865875169\n",
            "Score:  5.486419072869918\n",
            "Score:  12.219074632699117\n",
            "Score:  5.524009815928828\n",
            "Score:  9.381973784696505\n",
            "Score:  14.00875294510463\n",
            "Score:  15.661727300649261\n",
            "Score:  42.73506696461574\n",
            "Score:  14.510754936785638\n",
            "Score:  7.508131650920688\n",
            "Score:  5.163017753855241\n",
            "Score:  17.89346760126268\n",
            "Score:  5.259990026454472\n",
            "Score:  12.704860560451518\n",
            "Score:  9.062807999683011\n",
            "Score:  4.209792732032571\n",
            "Score:  22.586876316415477\n",
            "Score:  19.495440054904225\n",
            "Score:  30.164216872538542\n",
            "Score:  10.742284185152105\n",
            "Score:  9.601716044078445\n",
            "Score:  31.39434582758419\n",
            "Score:  7.578315036864225\n",
            "Score:  10.02292039210849\n",
            "Score:  8.745025579652632\n",
            "Score:  10.44602438436567\n",
            "Score:  12.570616420230962\n",
            "Score:  7.106123267630874\n",
            "Score:  6.105947732508771\n",
            "Score:  8.041405606645032\n",
            "Score:  9.09764134990758\n",
            "Score:  7.31204055367261\n",
            "Score:  5.123314547739773\n",
            "Score:  5.0796237947648315\n",
            "Score:  8.640127743801912\n",
            "Score:  12.514894223965136\n",
            "Score:  5.021581043619226\n",
            "Score:  40.74885545840468\n",
            "Score:  5.071780840590464\n",
            "Score:  4.697933799397148\n",
            "Score:  11.12589396116842\n",
            "Score:  13.00536484065559\n",
            "Score:  29.246762380299366\n",
            "Score:  16.042758108690897\n",
            "Score:  35.12065366255547\n",
            "Score:  19.74485279106308\n",
            "Score:  6.572194632610671\n",
            "Score:  5.107393552847476\n",
            "Score:  10.808761069895349\n",
            "Score:  8.699579489983634\n",
            "Score:  11.742229669140341\n",
            "Score:  15.80376371216097\n",
            "Score:  5.061374409114283\n",
            "Score:  8.346143879247702\n",
            "Score:  7.8941154294034455\n",
            "Score:  5.858320488892673\n",
            "Score:  16.78577734160314\n",
            "Score:  6.295328706813401\n",
            "Score:  5.618483928890466\n",
            "Score:  5.837530261466071\n",
            "Score:  5.226812343483695\n",
            "Score:  6.291192933184784\n",
            "Score:  7.159000529262422\n",
            "Score:  10.12734586617112\n",
            "Score:  22.746413270526354\n",
            "Score:  17.214487593367153\n",
            "Score:  21.319941322763338\n",
            "Score:  6.732601248136762\n",
            "Score:  7.450194098216651\n",
            "Score:  8.612469731160822\n",
            "Score:  4.705039379815218\n",
            "Score:  10.032064318117174\n",
            "Score:  5.072028221706393\n",
            "Score:  6.050754427868972\n",
            "Score:  5.73583916476108\n",
            "Score:  13.56226941630311\n",
            "Score:  4.665858610940763\n",
            "Score:  18.89665671653503\n",
            "Score:  8.11648985793783\n",
            "Score:  10.019777548023697\n",
            "Score:  6.296017295245718\n",
            "Score:  22.58886160057219\n",
            "Score:  21.79804680854304\n",
            "Score:  5.678028918540276\n",
            "Score:  14.363908584990867\n",
            "Score:  10.23709683521666\n",
            "Score:  58.233939305266496\n",
            "Score:  12.272853999430161\n",
            "Score:  8.765037075505047\n",
            "Score:  10.182344479025314\n",
            "Score:  14.125802695990767\n",
            "Score:  10.949330795898515\n",
            "Score:  5.637557078544464\n",
            "Score:  5.128566342808887\n",
            "Score:  4.886834444239665\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.1709413711746133\n",
            "Score:  4.886834444239665\n",
            "Score:  4.886834444239665\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.1709413711746133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-12 14:06:14,529] Trial 0 finished with value: 3.8572982 and parameters: {'learning_rate': 5.0831415843948e-05, 'gamma': 0.9174324676022229, 'clip_range': 0.11611856870727955, 'gae_lambda': 0.7855933898383414, 'ent_coef': 0.016856570266031053, 'vf_coef': 0.4195742333942373, 'features_dim': 64}. Best is trial 0 with value: 3.8572982.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score:  3.1709413711746133\n",
            "Score:  4.886834444239665\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Score:  8.809199781088623\n",
            "Score:  9.2969283491838\n",
            "Score:  22.197653490589417\n",
            "Score:  9.0206844251954\n",
            "Score:  10.598937204332184\n",
            "Score:  14.487686334985828\n",
            "Score:  7.027756832765123\n",
            "Score:  6.00150894133219\n",
            "Score:  9.121935423899625\n",
            "Score:  13.466483440982513\n",
            "Score:  8.097204928738883\n",
            "Score:  30.09957118319369\n",
            "Score:  13.352225271175975\n",
            "Score:  16.6816961329519\n",
            "Score:  11.07939681889259\n",
            "Score:  7.2867693774162445\n",
            "Score:  18.657396776108076\n",
            "Score:  20.219010412254104\n",
            "Score:  9.988331896207704\n",
            "Score:  16.879794123476625\n",
            "Score:  10.0272144190696\n",
            "Score:  11.313750528498074\n",
            "Score:  9.304871170198334\n",
            "Score:  16.05007229779544\n",
            "Score:  5.927692679697454\n",
            "Score:  9.830230908415038\n",
            "Score:  10.033520111442847\n",
            "Score:  13.686217789863349\n",
            "Score:  15.289835572025286\n",
            "Score:  5.898153581909954\n",
            "Score:  9.912912493195527\n",
            "Score:  15.97218800360386\n",
            "Score:  10.676302374304253\n",
            "Score:  6.536908356126542\n",
            "Score:  24.156529313894207\n",
            "Score:  17.41496503231598\n",
            "Score:  5.730128022296532\n",
            "Score:  15.014694883996574\n",
            "Score:  13.769845161243232\n",
            "Score:  4.670464590417516\n",
            "Score:  9.838168277888892\n",
            "Score:  4.161777187951031\n",
            "Score:  5.300784261584922\n",
            "Score:  6.360889560062344\n",
            "Score:  17.84034622599002\n",
            "Score:  14.834955566261762\n",
            "Score:  9.95784137645799\n",
            "Score:  10.07920563448174\n",
            "Score:  7.327552209632209\n",
            "Score:  25.897758873150064\n",
            "Score:  5.90388080938414\n",
            "Score:  9.870033858892233\n",
            "Score:  14.303059099150506\n",
            "Score:  28.61808283831664\n",
            "Score:  12.691088184234552\n",
            "Score:  12.814249212767196\n",
            "Score:  6.424742401192355\n",
            "Score:  14.539534513658744\n",
            "Score:  6.886150754378461\n",
            "Score:  14.037671721797587\n",
            "Score:  6.2075495988162706\n",
            "Score:  13.476851541441404\n",
            "Score:  21.038380902647305\n",
            "Score:  12.527232548821457\n",
            "Score:  21.78624161581029\n",
            "Score:  6.16784971896318\n",
            "Score:  11.62477459630699\n",
            "Score:  5.416422454762507\n",
            "Score:  11.487883984571221\n",
            "Score:  13.678453297238697\n",
            "Score:  7.42996459876508\n",
            "Score:  7.626610985335654\n",
            "Score:  35.59869351050715\n",
            "Score:  16.89642403893713\n",
            "Score:  10.638413259422846\n",
            "Score:  4.892979609745747\n",
            "Score:  7.93336261865726\n",
            "Score:  5.053373826089762\n",
            "Score:  9.853031911750296\n",
            "Score:  26.505648202214974\n",
            "Score:  15.4476372755222\n",
            "Score:  6.299947084995675\n",
            "Score:  7.211095253650885\n",
            "Score:  9.578238910009848\n",
            "Score:  10.569806977998175\n",
            "Score:  18.94788717482542\n",
            "Score:  18.68578821009957\n",
            "Score:  11.086669990703742\n",
            "Score:  10.91876472517931\n",
            "Score:  7.177189233735728\n",
            "Score:  7.635485793578771\n",
            "Score:  7.950636258564054\n",
            "Score:  5.137827314953354\n",
            "Score:  6.782994079209002\n",
            "Score:  30.245877540854078\n",
            "Score:  8.193886601641399\n",
            "Score:  27.76503747927854\n",
            "Score:  8.674149900402501\n",
            "Score:  6.057492421743874\n",
            "Score:  15.55093075662401\n",
            "Score:  7.652936359126097\n",
            "Score:  6.097598025871379\n",
            "Score:  5.329451670307779\n",
            "Score:  6.5915387412095425\n",
            "Score:  6.352878346868672\n",
            "Score:  7.259350846701857\n",
            "Score:  8.821564075496632\n",
            "Score:  32.37645612349802\n",
            "Score:  7.3016549809217\n",
            "Score:  10.13744706372281\n",
            "Score:  13.609120848307654\n",
            "Score:  41.86333338502134\n",
            "Score:  6.9259143200502\n",
            "Score:  29.023746680055268\n",
            "Score:  6.674879436552476\n",
            "Score:  6.111577976288143\n",
            "Score:  6.18061250670058\n",
            "Score:  12.734116343885109\n",
            "Score:  6.191240940386199\n",
            "Score:  12.52610832029847\n",
            "Score:  9.812039397780511\n",
            "Score:  16.22360351622854\n",
            "Score:  5.4691496816530725\n",
            "Score:  11.717466206034082\n",
            "Score:  21.7270425264227\n",
            "Score:  6.93863053527079\n",
            "Score:  8.904700770426972\n",
            "Score:  12.332242678894874\n",
            "Score:  9.250171113516119\n",
            "Score:  8.211112624151477\n",
            "Score:  43.94424371659167\n",
            "Score:  6.979199492997778\n",
            "Score:  11.679256853203002\n",
            "Score:  15.536907712464142\n",
            "Score:  17.00358514756108\n",
            "Score:  8.056794161751643\n",
            "Score:  11.617872515411134\n",
            "Score:  5.088483065015875\n",
            "Score:  42.202191594844805\n",
            "Score:  5.75187389717523\n",
            "Score:  6.231105275209268\n",
            "Score:  11.135181985214881\n",
            "Score:  17.0379884230094\n",
            "Score:  25.264664569449998\n",
            "Score:  6.871202162320156\n",
            "Score:  6.980578030880138\n",
            "Score:  32.314553853304076\n",
            "Score:  9.147466640409945\n",
            "Score:  5.330435254329274\n",
            "Score:  4.608875719294046\n",
            "Score:  12.945712186729942\n",
            "Score:  6.732881243680025\n",
            "Score:  4.2303050643567754\n",
            "Score:  8.846503128559517\n",
            "Score:  29.639947252341738\n",
            "Score:  6.867710662419516\n",
            "Score:  8.476310799314856\n",
            "Score:  19.01045379154416\n",
            "Score:  14.234218888896741\n",
            "Score:  12.941522659277771\n",
            "Score:  7.511920588552316\n",
            "Score:  7.464340689676713\n",
            "Score:  7.074253171907388\n",
            "Score:  9.313051638731778\n",
            "Score:  7.070397923655169\n",
            "Score:  5.447088816838411\n",
            "Score:  5.924522742070241\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  4.379842390303121\n",
            "Score:  5.448591612543929\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.4373358683976507\n",
            "Score:  3.1709413711746133\n",
            "Score:  4.886834444239665\n",
            "Score:  3.1709413711746133\n",
            "Score:  4.886834444239665\n",
            "Score:  5.920771575169405\n",
            "Score:  5.448591612543929\n",
            "Score:  4.886834444239665\n",
            "Score:  3.791968171974754\n",
            "Score:  3.1709413711746133\n",
            "Score:  6.000504617821591\n",
            "Score:  3.82575013880958\n",
            "Score:  3.9606020029426365\n",
            "Score:  6.575664584065437\n",
            "Score:  3.376441827371186\n",
            "Score:  3.1789628780256107\n",
            "Score:  5.458778467750197\n",
            "Score:  3.781767282496887\n",
            "Score:  4.975034809378085\n",
            "Score:  5.0117530203619225\n",
            "Score:  3.447691861337862\n",
            "Score:  3.398981137473218\n",
            "Score:  4.886834444239665\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.82575013880958\n",
            "Score:  6.000504617821591\n",
            "Score:  5.514198353258376\n",
            "Score:  3.1709413711746133\n",
            "Score:  4.349676631239386\n",
            "Score:  3.824464961696696\n",
            "Score:  5.001766654000496\n",
            "Score:  3.824464961696696\n",
            "Score:  3.1709413711746133\n",
            "Score:  5.226288807859099\n",
            "Score:  6.098119672928568\n",
            "Score:  4.408016183932527\n",
            "Score:  5.419112297276203\n",
            "Score:  7.217889699813113\n",
            "Score:  5.448591612543929\n",
            "Score:  4.886834444239665\n",
            "Score:  4.349676631239386\n",
            "Score:  4.9535781008570225\n",
            "Score:  4.418827527192647\n",
            "Score:  3.1789347557675045\n",
            "Score:  3.82575013880958\n",
            "Score:  3.1709413711746133\n",
            "Score:  5.514198353258376\n",
            "Score:  5.514198353258376\n",
            "Score:  6.010988041575018\n",
            "Score:  6.580712791414479\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.7712067814449948\n",
            "Score:  5.001766654000496\n",
            "Score:  5.407345396805419\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.780011068625727\n",
            "Score:  5.483269438615856\n",
            "Score:  3.398981137473218\n",
            "Score:  5.818196360160068\n",
            "Score:  6.658826721438831\n",
            "Score:  3.4830057047475558\n",
            "Score:  4.886834444239665\n",
            "Score:  4.683909267750625\n",
            "Score:  5.587798856487777\n",
            "Score:  3.823184694911799\n",
            "Score:  3.5513284786884336\n",
            "Score:  3.1709413711746133\n",
            "Score:  7.119283275378386\n",
            "Score:  5.381125409201493\n",
            "Score:  3.1709413711746133\n",
            "Score:  3.4289005071485907\n",
            "Score:  4.905969418089299\n",
            "Score:  4.886834444239665\n",
            "Score:  5.828277160090459\n",
            "Score:  5.694714388515696\n",
            "Score:  4.052742998801549\n",
            "Score:  4.886834444239665\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n",
            "Score:  4.886834444239666\n",
            "Score:  3.170941371174613\n",
            "Score:  3.170941371174613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-12 14:09:32,941] Trial 1 finished with value: 4.0288875 and parameters: {'learning_rate': 3.5757067085195956e-05, 'gamma': 0.9441227626383657, 'clip_range': 0.11962270271519151, 'gae_lambda': 1.1517225021423338, 'ent_coef': 0.007529479386353045, 'vf_coef': 0.32617617954467476, 'features_dim': 128}. Best is trial 1 with value: 4.0288875.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score:  4.886834444239666\n",
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0449438202247191\n",
            "Score:  10.384473130662213\n",
            "Score:  19.11413439529615\n",
            "Score:  13.332941550243554\n",
            "Score:  12.906164747293298\n",
            "Score:  17.82398766015924\n",
            "Score:  5.199911389384289\n",
            "Score:  6.5587061245302625\n",
            "Score:  7.801793868495709\n",
            "Score:  17.997870724118677\n",
            "Score:  6.455699291216005\n",
            "Score:  49.723044209885636\n",
            "Score:  6.327196268617384\n",
            "Score:  5.0572911330446795\n",
            "Score:  8.087626251308459\n",
            "Score:  17.41339147855813\n",
            "Score:  13.874055248115734\n",
            "Score:  5.02375983629526\n",
            "Score:  22.99713440313177\n",
            "Score:  21.82866169694116\n",
            "Score:  8.082997149950305\n",
            "Score:  7.832436042276379\n",
            "Score:  4.285952613460615\n",
            "Score:  5.628288695967242\n",
            "Score:  11.954044664759822\n",
            "Score:  9.690482625668686\n",
            "Score:  13.295103113471871\n",
            "Score:  19.000159126026787\n",
            "Score:  22.891642898194135\n",
            "Score:  4.3745198851813045\n",
            "Score:  10.773524382175943\n",
            "Score:  6.368038909448257\n",
            "Score:  21.108590366161945\n",
            "Score:  26.655945806467294\n",
            "Score:  8.247941422178451\n",
            "Score:  25.48131696241555\n",
            "Score:  8.983503753165916\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# Memory-optimized TPU training setup\n",
        "def get_tpu_memory_info():\n",
        "    \"\"\"Get memory information from TPU device if available\"\"\"\n",
        "    if TPU_AVAILABLE:\n",
        "        try:\n",
        "            # This is just for diagnostic purposes\n",
        "            import subprocess\n",
        "            result = subprocess.run(['python3', '-c', 'import torch_xla; print(torch_xla._XLAC._xla_get_memory_info(torch_xla._XLAC._xla_get_default_device()))'],\n",
        "                                   stdout=subprocess.PIPE, text=True)\n",
        "            print(f\"TPU Memory Info: {result.stdout}\")\n",
        "        except:\n",
        "            print(\"Could not get detailed TPU memory info\")\n",
        "\n",
        "# Display memory information\n",
        "get_tpu_memory_info()\n",
        "\n",
        "n_envs = 1\n",
        "batch_size = 64\n",
        "n_steps = 1024\n",
        "features_dim = 256\n",
        "\n",
        "# Policy kwargs with memory-efficient parameters\n",
        "policy_kwargs = {\n",
        "    \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "    \"net_arch\": [64, 64]\n",
        "}\n",
        "\n",
        "# Create trainer with optimized parameters for TPU memory\n",
        "training = Train(\n",
        "    n_steps=n_steps,\n",
        "    batch_size=batch_size,\n",
        "    difficulty=\"medium\",\n",
        "    n_envs=n_envs,\n",
        "    load_model=None,\n",
        "    policy_kwargs=policy_kwargs\n",
        ")\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = True\n",
        "\n",
        "if do_optimization:\n",
        "    # Force TPU memory cleanup before starting\n",
        "    if TPU_AVAILABLE:\n",
        "        gc.collect()\n",
        "        xm.mark_step()\n",
        "\n",
        "    n_trials = 10\n",
        "    best_trial = training.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    print(f\"Best parameters found: {best_trial.params}\")\n",
        "else:\n",
        "    # Run training with memory-optimized settings\n",
        "    # Use fewer total timesteps for TPU to avoid memory issues\n",
        "    total_timesteps = 50000\n",
        "\n",
        "    model = training.train_ppo(\n",
        "        total_timesteps=total_timesteps,\n",
        "        eval_episodes=3,  # Fewer eval episodes on TPU\n",
        "        save_freq=5000,\n",
        "        eval_freq=5000\n",
        "    )\n",
        "\n",
        "    # Force memory cleanup after training\n",
        "    if TPU_AVAILABLE:\n",
        "        del model\n",
        "        gc.collect()\n",
        "        xm.mark_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "outputs": [],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "outputs": [],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        n_steps=1024,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=1,  # Use 1 env for evaluation\n",
        "        load_model=model_path\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "        difficulty=\"medium\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4h3jBARKPld"
      },
      "outputs": [],
      "source": [
        "# run_standalone_game(difficulty=\"medium\")\n",
        "# test_gym_env(difficulty=\"medium\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
