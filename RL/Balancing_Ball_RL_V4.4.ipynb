{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTUvQ0pDxH6C"
   },
   "source": [
    "V4.2 Update: in readme.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "X4jlMyqiKPlZ",
    "outputId": "2e836dd3-0ab1-4754-eb40-9a854fe5ce55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch XLA not found, will attempt to install\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-bigtable 2.30.1 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-bigquery 3.31.0 requires google-api-core[grpc]<3.0.0,>=2.11.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "earthengine-api 1.5.14 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires google-api-python-client>=1.12.5, but you have google-api-python-client 1.8.0 which is incompatible.\n",
      "yfinance 0.2.59 requires protobuf<6,>=5.29.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "dataproc-spark-connect 0.7.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mTPU support installed. Please restart the runtime now.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for TPU availability and set it up\n",
    "import os\n",
    "\n",
    "# Check if TPU is available\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    print(\"PyTorch XLA already installed\")\n",
    "    TPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TPU_AVAILABLE = False\n",
    "    print(\"PyTorch XLA not found, will attempt to install\")\n",
    "\n",
    "# Install necessary packages including PyTorch/XLA\n",
    "!pip install pygame-ce pymunk stable-baselines3 stable-baselines3[extra] shimmy>=2.0 optuna\n",
    "!pip install -q cloud-tpu-client\n",
    "\n",
    "if not TPU_AVAILABLE:\n",
    "    # Check what version of PyTorch we need\n",
    "    import torch\n",
    "    if torch.__version__.startswith('2'):\n",
    "        # For PyTorch 2.x\n",
    "        !pip install -q torch_xla[tpu]>=2.0\n",
    "    else:\n",
    "        # For PyTorch 1.x\n",
    "        !pip install -q torch_xla\n",
    "\n",
    "    # Restart runtime (required after installing PyTorch/XLA)\n",
    "    print(\"TPU support installed. Please restart the runtime now.\")\n",
    "    import IPython\n",
    "    IPython.display.display(IPython.display.HTML(\n",
    "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
    "    ))\n",
    "else:\n",
    "    # Initialize TPU if available\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "    print(f\"XLA device detected: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W6IzM4yc3RQB"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2nILk-pwsMG",
    "outputId": "8c3c0ccb-28a8-4d28-ee44-e324e252754f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=2.0'\t sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bbqyvjZ1PZu",
    "outputId": "a3e99dfb-1a0e-420b-8e6d-85c9ca8c648f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/capture': No such file or directory\n",
      "rm: cannot remove '/content/game_history': No such file or directory\n",
      "rm: cannot remove '/content/logs': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r /content/capture\n",
    "!rm -r /content/game_history\n",
    "!rm -r /content/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L1Mmc6vu0CX"
   },
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhEqO-xFu4AI"
   },
   "source": [
    "## Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gJwfQb_Yuz1I"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class Recorder:\n",
    "\n",
    "    def __init__(self, task: str = \"game_history_record\"):\n",
    "        \"\"\"\n",
    "        tasks:\n",
    "        1. game_history_record\n",
    "        2. temp_memory\n",
    "        \"\"\"\n",
    "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "        CURRENT_DIR = \"\"\n",
    "        if task == \"game_history_record\":\n",
    "            collection_name = self.get_newest_record_name()\n",
    "            self.json_file_path = CURRENT_DIR + \"./game_history/\" + collection_name + \".json\"\n",
    "\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(self.json_file_path), exist_ok=True)\n",
    "\n",
    "        if os.path.exists(self.json_file_path):\n",
    "            print(\"Loading the json memory file\")\n",
    "            self.memory = self.load(self.json_file_path)\n",
    "        else:\n",
    "            print(\"The json memory file does not exist. Creating new file.\")\n",
    "            self.memory = {\"game_records\": []}  # Direct dictionary instead of json.loads\n",
    "            with open(self.json_file_path, \"w\") as f:\n",
    "                json.dump(self.memory, f)\n",
    "\n",
    "    def get(self):\n",
    "        print(\"Getting the json memory\")\n",
    "        return self.memory\n",
    "\n",
    "    def add_no_limit(self, data: float, ):\n",
    "        \"\"\"\n",
    "        Add a records.\n",
    "\n",
    "        Args:\n",
    "            role: The role of the sender (e.g., 'user', 'assistant')\n",
    "            message: The message content\n",
    "        \"\"\"\n",
    "        self.memory[\"game_records\"].append({\n",
    "            \"game_total_duration\": data,\n",
    "            \"timestamp\": str(datetime.datetime.now())\n",
    "        })\n",
    "\n",
    "        self.save(self.json_file_path)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(self.memory, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving memory to {file_path}: {e}\")\n",
    "\n",
    "    def load(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading memory from {file_path}: {e}\")\n",
    "            return {\"game_records\": []}\n",
    "\n",
    "    def get_newest_record_name(self) -> str:\n",
    "        \"\"\"\n",
    "        傳回最新的對話歷史資料和集的名稱 (game_YYYY_MM)\n",
    "            - 例如: \"game_2022-01\"\n",
    "        \"\"\"\n",
    "\n",
    "        this_month = datetime.datetime.now().strftime(\"%Y-%m\")\n",
    "        return \"record_\" + this_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnA8wZtosmeN"
   },
   "source": [
    "## Shapes & Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5wMhHMWCsmVD"
   },
   "outputs": [],
   "source": [
    "import pymunk\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class Shape:\n",
    "\n",
    "    def __init__(\n",
    "                self,\n",
    "                position: Tuple[float, float] = (300, 100),\n",
    "                velocity: Tuple[float, float] = (0, 0),\n",
    "                body: Optional[pymunk.Body] = None,\n",
    "                shape: Optional[pymunk.Shape] = None,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a physical shape with associated body.\n",
    "\n",
    "        Args:\n",
    "            position: Initial position (x, y) of the body\n",
    "            velocity: Initial velocity (vx, vy) of the body\n",
    "            body: The pymunk Body to attach to this shape\n",
    "            shape: The pymunk Shape for collision detection\n",
    "        \"\"\"\n",
    "\n",
    "        self.body = body\n",
    "        self.default_position = position\n",
    "        self.default_velocity = velocity\n",
    "        self.body.position = position\n",
    "        self.body.velocity = velocity\n",
    "        self.default_angular_velocity = 0\n",
    "\n",
    "        self.shape = shape\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the body to its default position, velocity and angular velocity.\"\"\"\n",
    "        self.body.position = self.default_position\n",
    "        self.body.velocity = self.default_velocity\n",
    "        self.body.angular_velocity = self.default_angular_velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mfw5tBxBswyF"
   },
   "outputs": [],
   "source": [
    "import pymunk\n",
    "\n",
    "# from shapes.shape import Shape\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class Circle(Shape):\n",
    "\n",
    "    def __init__(\n",
    "                self,\n",
    "                position: Tuple[float, float] = (300, 100),\n",
    "                velocity: Tuple[float, float] = (0, 0),\n",
    "                body: Optional[pymunk.Body] = None,\n",
    "                shape_radio: float = 20,\n",
    "                shape_mass: float = 1,\n",
    "                shape_friction: float = 0.1,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a circular physics object.\n",
    "\n",
    "        Args:\n",
    "            position: Initial position (x, y) of the circle\n",
    "            velocity: Initial velocity (vx, vy) of the circle\n",
    "            body: The pymunk Body to attach this circle to\n",
    "            shape_radio: Radius of the circle in pixels\n",
    "            shape_mass: Mass of the circle\n",
    "            shape_friction: Friction coefficient for the circle\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(position, velocity, body)\n",
    "        self.shape_radio = shape_radio\n",
    "        self.shape = pymunk.Circle(self.body, shape_radio)\n",
    "        self.shape.mass = shape_mass\n",
    "        self.shape.friction = shape_friction\n",
    "        self.shape.elasticity = 0.8  # Add some bounce to make the simulation more interesting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-8d5fKltI62"
   },
   "source": [
    "## Game class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wiw5Rjks-xw",
    "outputId": "faddd13b-31a9-473e-e2ae-082482f7a1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame-ce 2.5.3 (SDL 2.30.12, Python 3.11.12)\n"
     ]
    }
   ],
   "source": [
    "import pymunk\n",
    "import pygame\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display, Image, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import IPython.display as ipd\n",
    "# from shapes.circle import Circle\n",
    "# from record import Recorder\n",
    "\n",
    "class BalancingBallGame:\n",
    "    \"\"\"\n",
    "    A physics-based balancing ball game that can run standalone or be used as a Gym environment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Game constants\n",
    "\n",
    "\n",
    "    # Visual settings for indie style\n",
    "    BACKGROUND_COLOR = (41, 50, 65)  # Dark blue background\n",
    "    BALL_COLOR = (255, 213, 79)  # Bright yellow ball\n",
    "    PLATFORM_COLOR = (235, 64, 52)  # Red platform\n",
    "    PARTICLE_COLORS = [(252, 186, 3), (252, 127, 3), (252, 3, 3)]  # Fire-like particles\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 render_mode: str = \"human\",\n",
    "                 sound_enabled: bool = True,\n",
    "                 difficulty: str = \"medium\",\n",
    "                 window_x: int = 1000,\n",
    "                 window_y: int = 600,\n",
    "                 max_step: int = 30000,\n",
    "                 player_ball_speed: int = 5,\n",
    "                 reward_staying_alive: float = 0.1,\n",
    "                 reward_ball_centered: float = 0.2,\n",
    "                 penalty_falling: float = -10.0,\n",
    "                 fps: int = 120,\n",
    "                 platform_shape: str = \"circle\",\n",
    "                 platform_proportion: int = 0.4,\n",
    "                 capture_per_second: int = None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize the balancing ball game.\n",
    "\n",
    "        Args:\n",
    "            render_mode: \"human\" for visible window, \"rgb_array\" for gym env, \"headless\" for no rendering\n",
    "            sound_enabled: Whether to enable sound effects\n",
    "            difficulty: Game difficulty level (\"easy\", \"medium\", \"hard\")\n",
    "            max_step: 1 step = 1/fps, if fps = 120, 1 step = 1/120\n",
    "            reward_staying_alive: float = 0.1,\n",
    "            reward_ball_centered: float = 0.2,\n",
    "            penalty_falling: float = -10.0,\n",
    "            fps: frame per second\n",
    "            platform_proportion: platform_length = window_x * platform_proportion\n",
    "            capture_per_second: save game screen as a image every second, None means no capture\n",
    "        \"\"\"\n",
    "        # Game parameters\n",
    "        self.max_step = max_step\n",
    "        self.reward_staying_alive = reward_staying_alive\n",
    "        self.reward_ball_centered = reward_ball_centered\n",
    "        self.penalty_falling = penalty_falling\n",
    "        self.fps = fps\n",
    "        self.window_x = window_x\n",
    "        self.window_y = window_y\n",
    "        self.player_ball_speed = player_ball_speed\n",
    "\n",
    "        self.recorder = Recorder(\"game_history_record\")\n",
    "        self.render_mode = render_mode\n",
    "        self.sound_enabled = sound_enabled\n",
    "        self.difficulty = difficulty\n",
    "\n",
    "        platform_length = int(window_x * platform_proportion)\n",
    "        self._get_x_axis_max_reward_rate(platform_length)\n",
    "\n",
    "        # Initialize physics space\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = (0, 1000)\n",
    "        self.space.damping = 0.9\n",
    "\n",
    "        # Create game bodies\n",
    "        self.dynamic_body = pymunk.Body()  # Ball body\n",
    "        self.kinematic_body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)  # Platform body\n",
    "        self.kinematic_body.position = (self.window_x / 2, (self.window_y / 3) * 2)\n",
    "        self.default_kinematic_position = self.kinematic_body.position\n",
    "\n",
    "        # Create game objects\n",
    "        self._create_player()\n",
    "        self._create_platform(platform_shape=platform_shape, platform_length=platform_length)\n",
    "        # self._create_platform(\"rectangle\")\n",
    "\n",
    "        # Add all objects to space\n",
    "        self.space.add(self.dynamic_body, self.kinematic_body,\n",
    "                       self.circle.shape, self.platform)\n",
    "\n",
    "        # Game state tracking\n",
    "        self.steps = 0\n",
    "        self.start_time = time.time()\n",
    "        self.game_over = False\n",
    "        self.score = 0\n",
    "        self.particles = []\n",
    "\n",
    "        # Initialize Pygame if needed\n",
    "        if self.render_mode in [\"human\", \"rgb_array\", \"rgb_array_and_human\", \"rgb_array_and_human_in_colab\"]:\n",
    "            self._setup_pygame()\n",
    "        else:\n",
    "            print(\"render_mode is not human or rgb_array, so no pygame setup.\")\n",
    "\n",
    "        # Set difficulty parameters\n",
    "        self._apply_difficulty()\n",
    "        self.capture_per_second = capture_per_second\n",
    "\n",
    "        # Create folders for captures if needed\n",
    "        # CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "        CURRENT_DIR = \".\"\n",
    "        os.makedirs(os.path.dirname(CURRENT_DIR + \"/capture/\"), exist_ok=True)\n",
    "\n",
    "    def _setup_pygame(self):\n",
    "        \"\"\"Set up PyGame for rendering\"\"\"\n",
    "        pygame.init()\n",
    "        self.frame_count = 0\n",
    "\n",
    "        if self.sound_enabled:\n",
    "            self._load_sounds()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.screen = pygame.display.set_mode((self.window_x, self.window_y))\n",
    "            pygame.display.set_caption(\"Balancing Ball - Indie Game\")\n",
    "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            self.screen = pygame.Surface((self.window_x, self.window_y))\n",
    "\n",
    "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
    "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
    "\n",
    "        elif self.render_mode == \"rgb_array_and_human_in_colab\": # todo\n",
    "            from pymunk.pygame_util import DrawOptions\n",
    "\n",
    "            self.screen = pygame.Surface((self.window_x, self.window_y))  # Create hidden surface\n",
    "\n",
    "            # Set up display in Colab\n",
    "            self.draw_options = DrawOptions(self.screen)\n",
    "            html_display = ipd.HTML('''\n",
    "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
    "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
    "                </div>\n",
    "            ''')\n",
    "            self.display_handle = display(html_display, display_id='pygame_display')\n",
    "\n",
    "            self.last_update_time = time.time()\n",
    "            self.update_interval = 1.0 / 15  # Update display at 15 FPS to avoid overwhelming Colab\n",
    "            self.font = pygame.font.Font(None, int(self.window_x / 34))\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid render mode. Using headless mode.\")\n",
    "\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Create custom draw options for indie style\n",
    "\n",
    "    def _load_sounds(self):\n",
    "        \"\"\"Load game sound effects\"\"\"\n",
    "        try:\n",
    "            pygame.mixer.init()\n",
    "            self.sound_bounce = pygame.mixer.Sound(\"assets/bounce.wav\") if os.path.exists(\"assets/bounce.wav\") else None\n",
    "            self.sound_fall = pygame.mixer.Sound(\"assets/fall.wav\") if os.path.exists(\"assets/fall.wav\") else None\n",
    "        except Exception:\n",
    "            print(\"Sound loading error\")\n",
    "            self.sound_enabled = False\n",
    "            pass\n",
    "\n",
    "    def _create_player(self):\n",
    "        \"\"\"Create the ball with physics properties\"\"\"\n",
    "        self.ball_radius = int(self.window_x / 67)\n",
    "        self.circle = Circle(\n",
    "            position=(self.window_x / 2, self.window_y / 3),\n",
    "            velocity=(0, 0),\n",
    "            body=self.dynamic_body,\n",
    "            shape_radio=self.ball_radius,\n",
    "            shape_friction=100,\n",
    "        )\n",
    "        # Store initial values for reset\n",
    "        self.default_ball_position = self.dynamic_body.position\n",
    "\n",
    "    def _create_platform(self,\n",
    "                         platform_shape: str = \"circle\",\n",
    "                         platform_length: int = 200\n",
    "                        ):\n",
    "        \"\"\"\n",
    "        Create the platform with physics properties\n",
    "        platform_shape: circle, rectangle\n",
    "        platform_length: Length of a rectangle or Diameter of a circle\n",
    "        \"\"\"\n",
    "        if platform_shape == \"circle\":\n",
    "            self.platform_length = platform_length / 2 # radius\n",
    "            self.platform = pymunk.Circle(self.kinematic_body, self.platform_length)\n",
    "            self.platform.mass = 1  # 质量对 Kinematic 物体无意义，但需要避免除以零错误\n",
    "            self.platform.friction = 0.7\n",
    "        elif platform_shape == \"rectangle\":\n",
    "            self.platform_length = platform_length\n",
    "            vs = [(-self.platform_length/2, -10),\n",
    "                (self.platform_length/2, -10),\n",
    "                (self.platform_length/2, 10),\n",
    "                (-self.platform_length/2, 10)]\n",
    "\n",
    "            self.platform = pymunk.Poly(self.kinematic_body, vs)\n",
    "        self.platform.friction = 0.7\n",
    "        self.platform_rotation = 0\n",
    "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
    "\n",
    "    def _apply_difficulty(self):\n",
    "        \"\"\"Apply difficulty settings to the game\"\"\"\n",
    "        if self.difficulty == \"easy\":\n",
    "            self.max_platform_speed = 1.5\n",
    "            self.ball_elasticity = 0.5\n",
    "        elif self.difficulty == \"medium\":\n",
    "            self.max_platform_speed = 2.5\n",
    "            self.ball_elasticity = 0.7\n",
    "        else:  # hard\n",
    "            self.max_platform_speed = 3.5\n",
    "            self.ball_elasticity = 0.9\n",
    "\n",
    "        self.circle.shape.elasticity = self.ball_elasticity\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Reset the game state and return the initial observation\"\"\"\n",
    "        # Reset physics objects\n",
    "        self.dynamic_body.position = self.default_ball_position\n",
    "        self.dynamic_body.velocity = (0, 0)\n",
    "        self.dynamic_body.angular_velocity = 0\n",
    "\n",
    "        self.kinematic_body.position = self.default_kinematic_position\n",
    "        self.kinematic_body.angular_velocity = random.randrange(-1, 2, 2)\n",
    "\n",
    "        # Reset game state\n",
    "        self.steps = 0\n",
    "        self.start_time = time.time()\n",
    "        self.game_over = False\n",
    "        self.score = 0\n",
    "        self.particles = []\n",
    "\n",
    "        # Return initial observation\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Take a step in the game using the given action.\n",
    "\n",
    "        Args:\n",
    "            action: Float value between -1.0 and 1.0 controlling platform rotation\n",
    "\n",
    "        Returns:\n",
    "            observation: Game state observation\n",
    "            reward: Reward for this step\n",
    "            terminated: Whether episode is done\n",
    "            info: Additional information\n",
    "        \"\"\"\n",
    "        # Apply action to platform rotation\n",
    "        action_value = (0 - self.player_ball_speed) if action == 0 else self.player_ball_speed\n",
    "\n",
    "        self.dynamic_body.angular_velocity += action_value\n",
    "\n",
    "        # Step the physics simulation\n",
    "        self.space.step(1/self.fps)\n",
    "\n",
    "        # Update particle effects\n",
    "        self._update_particles()\n",
    "\n",
    "        # Check game state\n",
    "        self.steps += 1\n",
    "        terminated = False\n",
    "        reward = self.reward_staying_alive\n",
    "\n",
    "        # Calculate reward for keeping ball centered on platform\n",
    "        ball_x = self.dynamic_body.position[0]\n",
    "\n",
    "        # Check if ball falls off screen\n",
    "        if (self.dynamic_body.position[1] > self.kinematic_body.position[1] or\n",
    "            self.dynamic_body.position[0] < 0 or\n",
    "            self.dynamic_body.position[0] > self.window_x or\n",
    "            self.steps >= self.max_step\n",
    "            ):\n",
    "\n",
    "            print(\"Score: \", self.score)\n",
    "            terminated = True\n",
    "            reward = self.penalty_falling if self.steps < self.max_step else 0\n",
    "            self.game_over = True\n",
    "\n",
    "            result = {\n",
    "                \"game_total_duration\": f\"{time.time() - self.start_time:.2f}\",\n",
    "                \"score\": self.score,\n",
    "            }\n",
    "            self.recorder.add_no_limit(result)\n",
    "\n",
    "            if self.sound_enabled and self.sound_fall:\n",
    "                self.sound_fall.play()\n",
    "\n",
    "        step_reward = self._reward_calculator(ball_x)\n",
    "        self.score += step_reward\n",
    "        # print(\"ball_x: \", ball_x, \", self.score: \", self.score)\n",
    "        return self._get_observation(), step_reward, terminated\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"Convert game state to observation for RL agent\"\"\"\n",
    "        # update particles and draw them\n",
    "        screen_data = self.render() # 获取数据\n",
    "\n",
    "        if self.capture_per_second is not None and self.frame_count % self.capture_per_second == 0:  # Every second at 60 FPS\n",
    "            pygame.image.save(self.screen, f\"capture/frame_{self.frame_count/60}.png\")\n",
    "\n",
    "        self.frame_count += 1\n",
    "        return screen_data\n",
    "\n",
    "\n",
    "    def _update_particles(self):\n",
    "        \"\"\"Update particle effects for indie visual style\"\"\"\n",
    "        # Create new particles when ball hits platform\n",
    "        if abs(self.dynamic_body.position[1] - (self.kinematic_body.position[1] - 20)) < 5 and abs(self.dynamic_body.velocity[1]) > 100:\n",
    "            for _ in range(5):\n",
    "                self.particles.append({\n",
    "                    'x': self.dynamic_body.position[0],\n",
    "                    'y': self.dynamic_body.position[1] + self.ball_radius,\n",
    "                    'vx': random.uniform(-2, 2),\n",
    "                    'vy': random.uniform(1, 3),\n",
    "                    'life': 30,\n",
    "                    'size': random.uniform(2, 5),\n",
    "                    'color': random.choice(self.PARTICLE_COLORS)\n",
    "                })\n",
    "\n",
    "            if self.sound_enabled and self.sound_bounce:\n",
    "                self.sound_bounce.play()\n",
    "\n",
    "        # Update existing particles\n",
    "        for particle in self.particles[:]:\n",
    "            particle['x'] += particle['vx']\n",
    "            particle['y'] += particle['vy']\n",
    "            particle['life'] -= 1\n",
    "            if particle['life'] <= 0:\n",
    "                self.particles.remove(particle)\n",
    "\n",
    "    def render(self) -> Optional[np.ndarray]:\n",
    "        \"\"\"Render the current game state\"\"\"\n",
    "        if self.render_mode == \"headless\":\n",
    "            return None\n",
    "\n",
    "        # Clear screen with background color\n",
    "        self.screen.fill(self.BACKGROUND_COLOR)\n",
    "\n",
    "        # Custom drawing (for indie style)\n",
    "        self._draw_indie_style()\n",
    "\n",
    "\n",
    "        # Update display if in human mode\n",
    "        if self.render_mode == \"human\":\n",
    "            # Draw game information\n",
    "            self._draw_game_info()\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(self.fps)\n",
    "            return None\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            # Return RGB array for gym environment\n",
    "            return pygame.surfarray.array3d(self.screen)\n",
    "\n",
    "        elif self.render_mode == \"rgb_array_and_human\": # todo\n",
    "            print(\"rgb_array_and_human mode is not supported yet.\")\n",
    "\n",
    "        elif self.render_mode == \"rgb_array_and_human_in_colab\":\n",
    "            self.space.debug_draw(self.draw_options)\n",
    "            current_time = time.time()\n",
    "            if current_time - self.last_update_time >= self.update_interval:\n",
    "                # Convert Pygame surface to an image that can be displayed in Colab\n",
    "                buffer = BytesIO()\n",
    "                pygame.image.save(self.screen, buffer, 'PNG')\n",
    "                buffer.seek(0)\n",
    "                img_data = base64.b64encode(buffer.read()).decode('utf-8')\n",
    "\n",
    "                # Update the HTML image\n",
    "                self.display_handle.update(ipd.HTML(f'''\n",
    "                    <div id=\"pygame-output\" style=\"width:100%;\">\n",
    "                        <img id=\"pygame-img\" src=\"data:image/png;base64,{img_data}\" style=\"width:100%;\">\n",
    "                    </div>\n",
    "                '''))\n",
    "\n",
    "                self.last_update_time = current_time\n",
    "            return pygame.surfarray.array3d(self.screen)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def _draw_indie_style(self):\n",
    "        \"\"\"Draw game objects with indie game aesthetic\"\"\"\n",
    "        # # Draw platform with gradient and glow\n",
    "        # platform_points = []\n",
    "        # for v in self.platform.get_vertices():\n",
    "        #     x, y = v.rotated(self.kinematic_body.angle) + self.kinematic_body.position\n",
    "        #     platform_points.append((int(x), int(y)))\n",
    "\n",
    "        # pygame.draw.polygon(self.screen, self.PLATFORM_COLOR, platform_points)\n",
    "        # pygame.draw.polygon(self.screen, (255, 255, 255), platform_points, 2)\n",
    "\n",
    "        platform_pos = (int(self.kinematic_body.position[0]), int(self.kinematic_body.position[1]))\n",
    "        pygame.draw.circle(self.screen, self.PLATFORM_COLOR, platform_pos, self.platform_length)\n",
    "        pygame.draw.circle(self.screen, (255, 255, 255), platform_pos, self.platform_length, 2)\n",
    "\n",
    "        # Draw rotation direction indicator\n",
    "        self._draw_rotation_indicator(platform_pos, self.platform_length, self.kinematic_body.angular_velocity)\n",
    "\n",
    "        # Draw ball with gradient and glow\n",
    "        ball_pos = (int(self.dynamic_body.position[0]), int(self.dynamic_body.position[1]))\n",
    "        pygame.draw.circle(self.screen, self.BALL_COLOR, ball_pos, self.ball_radius)\n",
    "        pygame.draw.circle(self.screen, (255, 255, 255), ball_pos, self.ball_radius, 2)\n",
    "\n",
    "        # Draw particles\n",
    "        for particle in self.particles:\n",
    "            alpha = min(255, int(255 * (particle['life'] / 30)))\n",
    "            pygame.draw.circle(\n",
    "                self.screen,\n",
    "                particle['color'],\n",
    "                (int(particle['x']), int(particle['y'])),\n",
    "                int(particle['size'])\n",
    "            )\n",
    "\n",
    "    def _draw_rotation_indicator(self, position, radius, angular_velocity):\n",
    "        \"\"\"Draw an indicator showing the platform's rotation direction and speed\"\"\"\n",
    "        # Only draw the indicator if there's some rotation\n",
    "        if abs(angular_velocity) < 0.1:\n",
    "            return\n",
    "\n",
    "        # Calculate indicator properties based on angular velocity\n",
    "        indicator_color = (50, 255, 150) if angular_velocity > 0 else (255, 150, 50)\n",
    "        num_arrows = min(3, max(1, int(abs(angular_velocity))))\n",
    "        indicator_radius = radius - 20  # Place indicator inside the platform\n",
    "\n",
    "        # Draw arrow indicators along the platform's circumference\n",
    "        start_angle = self.kinematic_body.angle\n",
    "\n",
    "        for i in range(num_arrows):\n",
    "            # Calculate arrow position\n",
    "            arrow_angle = start_angle + i * (2 * np.pi / num_arrows)\n",
    "\n",
    "            # Calculate arrow start and end points\n",
    "            base_x = position[0] + int(np.cos(arrow_angle) * indicator_radius)\n",
    "            base_y = position[1] + int(np.sin(arrow_angle) * indicator_radius)\n",
    "\n",
    "            # Determine arrow direction based on angular velocity\n",
    "            if angular_velocity > 0:  # Clockwise\n",
    "                arrow_end_angle = arrow_angle + 0.3\n",
    "            else:  # Counter-clockwise\n",
    "                arrow_end_angle = arrow_angle - 0.3\n",
    "\n",
    "            tip_x = position[0] + int(np.cos(arrow_end_angle) * (indicator_radius + 15))\n",
    "            tip_y = position[1] + int(np.sin(arrow_end_angle) * (indicator_radius + 15))\n",
    "\n",
    "            # Draw arrow line\n",
    "            pygame.draw.line(self.screen, indicator_color, (base_x, base_y), (tip_x, tip_y), 3)\n",
    "\n",
    "            # Draw arrowhead\n",
    "            arrowhead_size = 7\n",
    "            pygame.draw.circle(self.screen, indicator_color, (tip_x, tip_y), arrowhead_size)\n",
    "\n",
    "    def _draw_game_info(self):\n",
    "        \"\"\"Draw game information on screen\"\"\"\n",
    "        # Create texts\n",
    "        time_text = f\"Time: {time.time() - self.start_time:.1f}\"\n",
    "        score_text = f\"Score: {self.score}\"\n",
    "\n",
    "        # Render texts\n",
    "        time_surface = self.font.render(time_text, True, (255, 255, 255))\n",
    "        score_surface = self.font.render(score_text, True, (255, 255, 255))\n",
    "\n",
    "        # Draw text backgrounds\n",
    "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
    "                        (5, 5, time_surface.get_width() + 10, time_surface.get_height() + 5))\n",
    "        pygame.draw.rect(self.screen, (0, 0, 0, 128),\n",
    "                        (self.window_x - score_surface.get_width() - 15, 5,\n",
    "                         score_surface.get_width() + 10, score_surface.get_height() + 5))\n",
    "\n",
    "        # Draw texts\n",
    "        self.screen.blit(time_surface, (10, 10))\n",
    "        self.screen.blit(score_surface, (self.window_x - score_surface.get_width() - 10, 10))\n",
    "\n",
    "        # Draw game over screen\n",
    "        if self.game_over:\n",
    "            game_over_text = \"GAME OVER - Press R to restart\"\n",
    "            game_over_surface = self.font.render(game_over_text, True, (255, 255, 255))\n",
    "\n",
    "            # Draw semi-transparent background\n",
    "            overlay = pygame.Surface((self.window_x, self.window_y), pygame.SRCALPHA)\n",
    "            overlay.fill((0, 0, 0, 128))\n",
    "            self.screen.blit(overlay, (0, 0))\n",
    "\n",
    "            # Draw text\n",
    "            self.screen.blit(game_over_surface,\n",
    "                           (self.window_x/2 - game_over_surface.get_width()/2,\n",
    "                            self.window_y/2 - game_over_surface.get_height()/2))\n",
    "\n",
    "    def _get_x_axis_max_reward_rate(self, platform_length):\n",
    "        \"\"\"\n",
    "        ((self.platform_length / 2) - 5) for calculate the distance to the\n",
    "        center of game window coordinates. The closer you are, the higher the reward.\n",
    "\n",
    "        When the ball is to be 10 points away from the center coordinates,\n",
    "        it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
    "        \"\"\"\n",
    "        self.reward_width = (platform_length / 2) - 5\n",
    "        self.x_axis_max_reward_rate = 2 / self.reward_width\n",
    "        print(\"self.x_axis_max_reward_rate: \", self.x_axis_max_reward_rate)\n",
    "\n",
    "    def _reward_calculator(self, ball_x):\n",
    "        # score & reward\n",
    "        step_reward = 1/100\n",
    "\n",
    "        rw = abs(ball_x - self.window_x/2)\n",
    "        if rw < self.reward_width:\n",
    "            x_axis_reward_rate = 1 + ((self.reward_width - abs(ball_x - self.window_x/2)) * self.x_axis_max_reward_rate)\n",
    "            step_reward = self.steps * 0.01 * x_axis_reward_rate  # Simplified reward calculation\n",
    "\n",
    "            if self.steps % 500 == 0:\n",
    "                step_reward += self.steps/100\n",
    "                print(\"check point: \", self.steps/500)\n",
    "\n",
    "            return step_reward\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _reward_calculator2(self, ball_x):\n",
    "        # Base reward for staying alive\n",
    "        step_reward = 0.1\n",
    "\n",
    "        # Distance from center (normalized)\n",
    "        distance_from_center = abs(ball_x - self.window_x/2) / (self.window_x/2)\n",
    "\n",
    "        # Smooth reward based on position (highest at center)\n",
    "        position_reward = max(0, 1.0 - distance_from_center)\n",
    "\n",
    "        # Apply position reward (with higher weight for better position)\n",
    "        step_reward += position_reward * 0.3\n",
    "\n",
    "        # Small bonus for surviving longer (but not dominant)\n",
    "        survival_bonus = min(0.2, self.steps / 10000)\n",
    "        step_reward += survival_bonus\n",
    "\n",
    "        # Checkpoint bonuses remain meaningful but don't explode\n",
    "        if self.steps % 1000 == 0 and self.steps > 0:\n",
    "            step_reward += 1.0\n",
    "            print(f\"Checkpoint reached: {self.steps}\")\n",
    "\n",
    "        return step_reward\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the game and clean up resources\"\"\"\n",
    "        if self.render_mode in [\"human\", \"rgb_array\"]:\n",
    "            pygame.quit()\n",
    "\n",
    "    def run_standalone(self):\n",
    "        \"\"\"Run the game in standalone mode with keyboard controls\"\"\"\n",
    "        if self.render_mode not in [\"human\"]:\n",
    "            raise ValueError(\"Standalone mode requires render_mode='human'\")\n",
    "\n",
    "        running = True\n",
    "        while running:\n",
    "            # Handle events\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_r and self.game_over:\n",
    "                        self.reset()\n",
    "\n",
    "            # Process keyboard controls\n",
    "            keys = pygame.key.get_pressed()\n",
    "            action = 0\n",
    "            if keys[pygame.K_LEFT]:\n",
    "                action = 0 - self.player_ball_speed\n",
    "            if keys[pygame.K_RIGHT]:\n",
    "                action = self.player_ball_speed\n",
    "\n",
    "            # Take game step\n",
    "            if not self.game_over:\n",
    "                self.step(action)\n",
    "\n",
    "            # Render\n",
    "            self.render()\n",
    "\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlW6s_8EKPlb"
   },
   "source": [
    "## Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "43f8apvbKPlb"
   },
   "outputs": [],
   "source": [
    "class Levels:\n",
    "    def __init__(dynamic_body, kinematic_body):\n",
    "        pass\n",
    "\n",
    "    def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm68AGmWIaDR"
   },
   "source": [
    "### Level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IKbIlgaIXor"
   },
   "outputs": [],
   "source": [
    "class Level1:\n",
    "    def __init__(self, dynamic_body, kinematic_body):\n",
    "        self.dynamic_body = dynamic_body\n",
    "        self.kinematic_body = kinematic_body\n",
    "\n",
    "    def setup():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHL2FZgFIY95"
   },
   "source": [
    "### Level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSQqrLe2IYUh"
   },
   "outputs": [],
   "source": [
    "class Level2:\n",
    "    def __init__(dynamic_body, kinematic_body):\n",
    "        pass\n",
    "\n",
    "    def setup():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC2_6BtrIXWq"
   },
   "source": [
    "### Level3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax2pKrx9IZjB"
   },
   "outputs": [],
   "source": [
    "class Level3:\n",
    "    def __init__(dynamic_body, kinematic_body):\n",
    "        pass\n",
    "\n",
    "    def setup():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjobL-nozI81"
   },
   "source": [
    "## GYM env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBzvHTN1zJu1"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "# from balancing_ball_game import BalancingBallGame\n",
    "\n",
    "class BalancingBallEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Gymnasium environment for the Balancing Ball game\n",
    "    \"\"\"\n",
    "    metadata = {'render_modes': ['human', 'rgb_array']}\n",
    "\n",
    "    def __init__(self, \n",
    "                 render_mode=\"rgb_array\", \n",
    "                 difficulty=\"medium\", \n",
    "                 fps=30,\n",
    "                 obs_type=\"game_screen\", \n",
    "                ):\n",
    "        \"\"\"\n",
    "        render_mode: how to render the environment\n",
    "            Example: \"human\" or \"rgb_array\"\n",
    "        fps: Frames per second,\n",
    "            Example: 30\n",
    "        obs_type: type of observation\n",
    "            Example: \"game_screen\" or \"state_based\"\n",
    "        \"\"\"\n",
    "\n",
    "        super(BalancingBallEnv, self).__init__()\n",
    "\n",
    "        # Action space: discrete - 0: left, 1: right\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Initialize game\n",
    "        self.window_x = 300\n",
    "        self.window_y = 180\n",
    "        self.platform_shape = \"circle\"\n",
    "        self.platform_proportion = 0.333\n",
    "\n",
    "        self.stack_size = 3  # Number of frames to stack\n",
    "        self.observation_stack = []  # Initialize the stack\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.game = BalancingBallGame(\n",
    "            render_mode=render_mode,\n",
    "            sound_enabled=(render_mode == \"human\"),\n",
    "            difficulty=difficulty,\n",
    "            window_x = self.window_x,\n",
    "            window_y = self.window_y,\n",
    "            fps = fps,\n",
    "            platform_shape = self.platform_shape,\n",
    "            platform_proportion = self.platform_proportion,\n",
    "        )\n",
    "\n",
    "        if obs_type == \"game_screen\":\n",
    "            # Image observation space (RGB) with stacked frames\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=0, high=255,\n",
    "                shape=(self.window_y, self.window_x, 3 * self.stack_size),  # For stacked frames\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "            self.step = self.step_game_screen\n",
    "            self.reset = self.reset_game_screen\n",
    "        elif obs_type == \"state_based\":\n",
    "            # State-based observation space: [ball_x, ball_y, ball_vx, ball_vy, platform_x, platform_y, platform_angular_velocity]\n",
    "            # Normalize values to be between -1 and 1\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]),\n",
    "                high=np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            self.step = self.step_state_based\n",
    "            self.reset = self.reset_state_based\n",
    "        else:\n",
    "            raise ValueError(\"obs_type must be 'game_screen' or 'state_based'\")\n",
    "\n",
    "        # Platform_length /= 2 when for calculate the distance to the\n",
    "        # center of game window coordinates. The closer you are, the higher the reward.\n",
    "        self.platform_reward_length = (self.game.platform_length / 2) - 5\n",
    "\n",
    "        # When the ball is to be 10 points away from the center coordinates,\n",
    "        # it should be 1 - ((self.platform_length - 10) * self.x_axis_max_reward_rate)\n",
    "        self.x_axis_max_reward_rate = 0.5 / self.platform_reward_length\n",
    "\n",
    "    def _get_state_based_observation(self):\n",
    "        \"\"\"Convert game state to state-based observation for RL agent\"\"\"\n",
    "        # Normalize positions by window dimensions\n",
    "        ball_x = self.game.dynamic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
    "        ball_y = self.game.dynamic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
    "\n",
    "        # Normalize velocities (assuming max velocity around 1000)\n",
    "        max_velocity = 1000\n",
    "        ball_vx = np.clip(self.game.dynamic_body.velocity[0] / max_velocity, -1, 1)\n",
    "        ball_vy = np.clip(self.game.dynamic_body.velocity[1] / max_velocity, -1, 1)\n",
    "\n",
    "        # Normalize platform position\n",
    "        platform_x = self.game.kinematic_body.position[0] / self.window_x * 2 - 1  # Convert to [-1, 1]\n",
    "        platform_y = self.game.kinematic_body.position[1] / self.window_y * 2 - 1  # Convert to [-1, 1]\n",
    "\n",
    "        # Normalize angular velocity (assuming max around 10)\n",
    "        max_angular_velocity = 10\n",
    "        platform_angular_velocity = np.clip(self.game.kinematic_body.angular_velocity / max_angular_velocity, -1, 1)\n",
    "\n",
    "        return np.array([\n",
    "            ball_x,\n",
    "            ball_y,\n",
    "            ball_vx,\n",
    "            ball_vy,\n",
    "            platform_x,\n",
    "            platform_y,\n",
    "            platform_angular_velocity\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step_state_based(self, action):\n",
    "        \"\"\"Take a step in the environment\"\"\"\n",
    "        # Take step in the game\n",
    "        _, step_reward, terminated = self.game.step(action)\n",
    "\n",
    "        # Get state-based observation\n",
    "        observation = self._get_state_based_observation()\n",
    "\n",
    "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
    "        return observation, step_reward, terminated, False, {}\n",
    "\n",
    "    def reset_state_based(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
    "\n",
    "        self.game.reset()\n",
    "        observation = self._get_state_based_observation()\n",
    "\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step_game_screen(self, action):\n",
    "        \"\"\"Take a step in the environment\"\"\"\n",
    "        # Take step in the game\n",
    "        obs, step_reward, terminated = self.game.step(action)\n",
    "        obs = np.transpose(obs, (1, 0, 2))  # 转置以符合 (height, width, channels)\n",
    "\n",
    "        # Stack the frames\n",
    "        self.observation_stack.append(obs)\n",
    "        if len(self.observation_stack) > self.stack_size:\n",
    "            self.observation_stack.pop(0)  # Remove the oldest frame\n",
    "\n",
    "        # If the stack isn't full yet, pad it with the current frame\n",
    "        while len(self.observation_stack) < self.stack_size:\n",
    "            self.observation_stack.insert(0, obs)  # Pad with current frame at the beginning\n",
    "\n",
    "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
    "\n",
    "        # Gymnasium expects (observation, reward, terminated, truncated, info)\n",
    "        return stacked_obs, step_reward, terminated, False, {}\n",
    "\n",
    "    def reset_game_screen(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        super().reset(seed=seed)  # This properly seeds the environment in Gymnasium\n",
    "\n",
    "        observation = self.game.reset()\n",
    "        observation = np.transpose(observation, (1, 0, 2))  # 转置以符合 (height, width, channels)\n",
    "\n",
    "        # Reset the observation stack\n",
    "        self.observation_stack = []\n",
    "\n",
    "        # Fill the stack with the initial observation\n",
    "        for _ in range(self.stack_size):\n",
    "            self.observation_stack.append(observation)\n",
    "\n",
    "        # Create stacked observation\n",
    "        stacked_obs = np.concatenate(self.observation_stack, axis=-1)\n",
    "\n",
    "        info = {}\n",
    "        return stacked_obs, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the environment\"\"\"\n",
    "        return self.game.render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwgvyLDYKPlb"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ebr3cvEiKPlc"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# from balancing_ball_game import BalancingBallGame\n",
    "\n",
    "def run_standalone_game(render_mode=\"human\", difficulty=\"medium\", capture_per_second=3):\n",
    "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
    "    window_x = 1000\n",
    "    window_y = 600\n",
    "    platform_shape = \"circle\"\n",
    "    platform_proportion = 0.333\n",
    "\n",
    "    game = BalancingBallGame(\n",
    "        render_mode = render_mode,\n",
    "        difficulty = difficulty,\n",
    "        window_x = window_x,\n",
    "        window_y = window_y,\n",
    "        platform_shape = platform_shape,\n",
    "        platform_proportion = platform_proportion,\n",
    "        fps = 30,\n",
    "        capture_per_second = 3,\n",
    "    )\n",
    "\n",
    "    game.run_standalone()\n",
    "\n",
    "def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
    "    \"\"\"Test the OpenAI Gym environment\"\"\"\n",
    "    import time\n",
    "    # from gym_env import BalancingBallEnv\n",
    "\n",
    "    fps = 30\n",
    "    env = BalancingBallEnv(\n",
    "        render_mode=\"human\",\n",
    "        difficulty=difficulty,\n",
    "        fps=fps,\n",
    "    )\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        observation, info = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Sample a random action (for testing only)\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Take step\n",
    "            observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            # Render\n",
    "            env.render()\n",
    "\n",
    "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04sj1npeKPlc"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWkLYH5-KPlc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import optuna\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy  # MLP policy instead of CNN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "class Train:\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.0003,\n",
    "                 n_steps=2048,\n",
    "                 batch_size=64,\n",
    "                 n_epochs=10,\n",
    "                 gamma=0.99,\n",
    "                 gae_lambda=0.95,\n",
    "                 ent_coef=0.01,\n",
    "                 vf_coef=0.5,\n",
    "                 max_grad_norm=0.5,\n",
    "                 policy_kwargs=None,\n",
    "                 n_envs=4,\n",
    "                 difficulty=\"medium\",\n",
    "                 load_model=None,\n",
    "                 log_dir=\"./logs/\",\n",
    "                 model_dir=\"./models/\",\n",
    "                ):\n",
    "\n",
    "        # Create directories\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        self.log_dir = log_dir\n",
    "        self.model_dir = model_dir\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        # Setup environments\n",
    "        env = make_vec_env(\n",
    "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty),\n",
    "            n_envs=n_envs\n",
    "        )\n",
    "        self.env = env  # No need for VecTransposeImage with state-based observations\n",
    "\n",
    "        # Setup evaluation environment\n",
    "        eval_env = make_vec_env(\n",
    "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty),\n",
    "            n_envs=1\n",
    "        )\n",
    "        self.eval_env = eval_env  # No need for VecTransposeImage\n",
    "\n",
    "        # Define policy kwargs if not provided\n",
    "        if policy_kwargs is None:\n",
    "            policy_kwargs = {\n",
    "                \"net_arch\": [256, 256]  # MLP architecture\n",
    "            }\n",
    "\n",
    "        # Create the PPO model\n",
    "        if load_model:\n",
    "            print(f\"Loading model from {load_model}\")\n",
    "            self.model = PPO.load(\n",
    "                load_model,\n",
    "                env=self.env,\n",
    "                tensorboard_log=log_dir,\n",
    "            )\n",
    "        else:\n",
    "            hyper_param = {\n",
    "                'learning_rate': 0.0003,\n",
    "                'gamma': 0.99,\n",
    "                'clip_range': 0.2,\n",
    "                'gae_lambda': 0.95,\n",
    "                'ent_coef': 0.01,\n",
    "                'vf_coef': 0.5,\n",
    "            }\n",
    "\n",
    "            self.model = PPO(\n",
    "                policy=ActorCriticPolicy,  # MLP policy for state-based observations\n",
    "                env=self.env,\n",
    "                learning_rate=hyper_param[\"learning_rate\"],\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                n_epochs=n_epochs,\n",
    "                gamma=hyper_param[\"gamma\"],\n",
    "                clip_range=hyper_param[\"clip_range\"],\n",
    "                gae_lambda=hyper_param[\"gae_lambda\"],\n",
    "                ent_coef=hyper_param[\"ent_coef\"],\n",
    "                vf_coef=hyper_param[\"vf_coef\"],\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                tensorboard_log=log_dir,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                verbose=1,\n",
    "            )\n",
    "\n",
    "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\"):\n",
    "        \"\"\"\n",
    "        Create and return an environment function to be used with VecEnv\n",
    "        \"\"\"\n",
    "        def _init():\n",
    "            env = BalancingBallEnv(render_mode=render_mode, difficulty=difficulty, obs_type=\"state_based\")\n",
    "            return env\n",
    "        return _init\n",
    "\n",
    "    def train_ppo(self,\n",
    "                  total_timesteps=1000000,\n",
    "                  save_freq=10000,\n",
    "                  eval_freq=10000,\n",
    "                  eval_episodes=5,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Train a PPO agent to play the Balancing Ball game\n",
    "\n",
    "        Args:\n",
    "            total_timesteps: Total number of steps to train for\n",
    "            n_envs: Number of parallel environments\n",
    "            save_freq: How often to save checkpoints (in timesteps)\n",
    "            log_dir: Directory for tensorboard logs\n",
    "            model_dir: Directory to save models\n",
    "            eval_freq: How often to evaluate the model (in timesteps)\n",
    "            eval_episodes: Number of episodes to evaluate on\n",
    "            difficulty: Game difficulty level\n",
    "            load_model: Path to model to load for continued training\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup callbacks\n",
    "        checkpoint_callback = CheckpointCallback(\n",
    "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
    "            save_path=self.model_dir,\n",
    "            name_prefix=\"ppo_balancing_ball\"\n",
    "        )\n",
    "\n",
    "        eval_callback = EvalCallback(\n",
    "            self.eval_env,\n",
    "            best_model_save_path=self.model_dir,\n",
    "            log_path=self.log_dir,\n",
    "            eval_freq=eval_freq // self.n_envs,\n",
    "            n_eval_episodes=eval_episodes,\n",
    "            deterministic=True,\n",
    "            render=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Starting training...\")\n",
    "        self.model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=[checkpoint_callback, eval_callback],\n",
    "        )\n",
    "\n",
    "        # Save the final model\n",
    "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final\")\n",
    "\n",
    "        print(\"Training completed!\")\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, model_path, n_episodes=10, difficulty=\"medium\"):\n",
    "        \"\"\"\n",
    "        Evaluate a trained model\n",
    "\n",
    "        Args:\n",
    "            model_path: Path to the saved model\n",
    "            n_episodes: Number of episodes to evaluate on\n",
    "            difficulty: Game difficulty level\n",
    "        \"\"\"\n",
    "        # Load the model\n",
    "        model = PPO.load(model_path)\n",
    "\n",
    "        # Evaluate\n",
    "        mean_reward, std_reward = evaluate_policy(\n",
    "            model,\n",
    "            self.env,\n",
    "            n_eval_episodes=n_episodes,\n",
    "            deterministic=True,\n",
    "            render=True\n",
    "        )\n",
    "\n",
    "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "    def optuna_parameter_tuning(self, n_trials):\n",
    "        print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
    "\n",
    "        pruner = optuna.pruners.HyperbandPruner(\n",
    "            min_resource=100,        # 最小资源量\n",
    "            max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
    "            reduction_factor=3     # 折减因子 (eta)\n",
    "        )\n",
    "\n",
    "        # 建立 study 物件，並指定剪枝器\n",
    "        study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "\n",
    "        # 執行優化\n",
    "        study.optimize(self.objective, n_trials=n_trials)\n",
    "\n",
    "        # 分析結果\n",
    "        print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
    "        print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
    "\n",
    "        import pandas as pd\n",
    "        df = study.trials_dataframe()\n",
    "        print(df.head())\n",
    "\n",
    "\n",
    "    # def evaluate_policy(self, model, n_eval_episodes=10):\n",
    "    #     \"\"\"\n",
    "    #     評估強化學習策略的函數。\n",
    "\n",
    "    #     Args:\n",
    "    #         model: 要評估的 Stable Baselines3 模型。\n",
    "    #         env: 用於評估的環境。\n",
    "    #         n_eval_episodes: 要運行的 episode 數量。\n",
    "\n",
    "    #     Returns:\n",
    "    #         平均回報。\n",
    "    #     \"\"\"\n",
    "    #     rewards = []\n",
    "    #     for _ in range(n_eval_episodes):\n",
    "    #         obs = self.env.reset()[0]  # 注意gymnasium的reset()返回值\n",
    "    #         done = False\n",
    "    #         total_reward = 0\n",
    "    #         while not done:\n",
    "    #             action, _ = model.predict(obs, deterministic=True)  # 使用確定性策略\n",
    "    #             obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "    #             done = terminated or truncated\n",
    "    #             total_reward += reward\n",
    "    #         rewards.append(total_reward)\n",
    "    #     return sum(rewards) / n_eval_episodes\n",
    "\n",
    "\n",
    "    def objective(self, trial):\n",
    "        import gc\n",
    "\n",
    "        # 1. 建議超參數\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "        gamma = trial.suggest_float('gamma', 0.9, 0.999)\n",
    "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
    "        gae_lambda = trial.suggest_float('gae_lambda', 0.5, 2)\n",
    "        ent_coef = trial.suggest_float('ent_coef', 0.005, 0.05)\n",
    "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
    "        features_dim = trial.suggest_categorical('features_dim', [32, 64, 128, 256, 512])\n",
    "        policy_kwargs = {\n",
    "            \"net_arch\": [256, 256],  # MLP architecture\n",
    "        }\n",
    "\n",
    "\n",
    "        n_steps=2048\n",
    "        batch_size=64\n",
    "        n_epochs=10\n",
    "        # gamma=0.99\n",
    "        # gae_lambda=0.95\n",
    "        # ent_coef=0.01\n",
    "        # vf_coef=0.5\n",
    "        max_grad_norm=0.5\n",
    "        # policy_kwargs = {\n",
    "        #     \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
    "        # }\n",
    "\n",
    "        # 2. 建立環境\n",
    "        env = make_vec_env(\n",
    "            self.make_env(render_mode=\"rgb_array\", difficulty=\"medium\"),\n",
    "            n_envs=1\n",
    "        )\n",
    "\n",
    "        # 3. 建立模型\n",
    "        model = PPO(\n",
    "                policy=ActorCriticPolicy,  # MLP policy for state-based observations\n",
    "                env=env,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                n_epochs=n_epochs,\n",
    "                gamma=gamma,\n",
    "                clip_range=clip_range,\n",
    "                gae_lambda=gae_lambda,\n",
    "                ent_coef=ent_coef,\n",
    "                vf_coef=vf_coef,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                tensorboard_log=None,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "\n",
    "        try:\n",
    "            # 4. 訓練模型\n",
    "            model.learn(total_timesteps=7000)\n",
    "            # 5. 評估模型\n",
    "            mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
    "        finally:\n",
    "            # Always cleanup\n",
    "            env.close()\n",
    "            del model\n",
    "            del env\n",
    "            gc.collect()\n",
    "\n",
    "            if TPU_AVAILABLE:\n",
    "                import torch_xla.core.xla_model as xm\n",
    "                xm.mark_step()\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Train or evaluate PPO agent for Balancing Ball\")\n",
    "# parser.add_argument(\"--mode\", type=str, default=\"train\", choices=[\"train\", \"eval\"],\n",
    "#                     help=\"Mode: 'train' to train model, 'eval' to evaluate\")\n",
    "# parser.add_argument(\"--timesteps\", type=int, default=1000000,\n",
    "#                     help=\"Total timesteps for training\")\n",
    "# parser.add_argument(\"--difficulty\", type=str, default=\"medium\",\n",
    "#                     choices=[\"easy\", \"medium\", \"hard\"],\n",
    "#                     help=\"Game difficulty\")\n",
    "# parser.add_argument(\"--load_model\", type=str, default=None,\n",
    "#                     help=\"Path to model to load for continued training or evaluation\")\n",
    "# parser.add_argument(\"--n_envs\", type=int, default=4,\n",
    "#                     help=\"Number of parallel environments for training\")\n",
    "# parser.add_argument(\"--eval_episodes\", type=int, default=5,\n",
    "#                     help=\"Number of episodes for evaluation\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# if args.mode == \"train\":\n",
    "#     train_ppo(\n",
    "#         total_timesteps=args.timesteps,\n",
    "#         difficulty=args.difficulty,\n",
    "#         n_envs=args.n_envs,\n",
    "#         load_model=args.load_model,\n",
    "#         eval_episodes=args.eval_episodes,\n",
    "#     )\n",
    "# else:\n",
    "#     if args.load_model is None:\n",
    "#         print(\"Error: Must provide --load_model for evaluation\")\n",
    "#     else:\n",
    "#         evaluate(\n",
    "#             model_path=args.load_model,\n",
    "#             n_episodes=args.eval_episodes,\n",
    "#             difficulty=args.difficulty\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFM-k9MuCmzc"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "Ndr9hGp2CZzF",
    "outputId": "1faca2a3-791c-49b0-8208-0c9f868f44b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The json memory file does not exist. Creating new file.\n",
      "self.x_axis_max_reward_rate:  0.0449438202247191\n",
      "Loading the json memory file\n",
      "self.x_axis_max_reward_rate:  0.0449438202247191\n",
      "Loading the json memory file\n",
      "self.x_axis_max_reward_rate:  0.0449438202247191\n",
      "Loading the json memory file\n",
      "self.x_axis_max_reward_rate:  0.0449438202247191\n",
      "Loading the json memory file\n",
      "self.x_axis_max_reward_rate:  0.0449438202247191\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Logging to ./logs/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Score:  23.44292043529168\n",
      "Score:  38.043381696469545\n",
      "Score:  38.043381696469545\n",
      "Score:  38.043381696469545\n",
      "Eval num_timesteps=100000, episode_reward=38.04 +/- 0.00\n",
      "Episode length: 66.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 66          |\n",
      "|    mean_reward          | 38          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024900936 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.44        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00699    |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "Score:  54.144331753260516\n",
      "Score:  33.85281226881584\n",
      "Score:  43.38919476849735\n",
      "Score:  54.173755857678024\n",
      "Score:  24.422066428243696\n",
      "Score:  40.42539935006009\n",
      "Score:  42.35548137040381\n",
      "Score:  116.16709598877883\n",
      "Score:  23.75972117935738\n",
      "Score:  37.64173051810718\n",
      "Score:  95.04390225088484\n",
      "Score:  49.24891676208257\n",
      "Score:  66.95250615751898\n",
      "Score:  49.886601289558136\n",
      "Score:  33.242985935852495\n",
      "Score:  32.770195492744314\n",
      "Score:  54.47073744239428\n",
      "Score:  65.82714036141486\n",
      "Score:  54.714463132849765\n",
      "Score:  38.143112881956164\n",
      "Score:  20.307251291383192\n",
      "Score:  23.236005890325693\n",
      "Score:  18.888498590538084\n",
      "Score:  40.34257579492251\n",
      "Score:  21.69085190494301\n",
      "Score:  28.813048211647786\n",
      "Score:  20.53507448457758\n",
      "Score:  23.847565661045415\n",
      "Score:  246.73209257913487\n",
      "Score:  67.97224595298789\n",
      "Score:  37.83212775395767\n",
      "Score:  29.61946177293966\n",
      "Score:  79.15264712132155\n",
      "Score:  21.643115781282503\n",
      "Score:  17.235423203275303\n",
      "Score:  34.0993131206628\n",
      "Score:  58.0963776971017\n",
      "Score:  34.98767508701441\n",
      "Score:  50.514530600854414\n",
      "Score:  160.08248707205442\n",
      "Score:  45.217048795542546\n",
      "Score:  49.37053623255335\n",
      "Score:  42.3204867058435\n",
      "Score:  35.54670425647348\n",
      "Score:  119.02437238030629\n",
      "Score:  60.00829532632606\n",
      "Score:  60.73179747992101\n",
      "Score:  38.486324854916276\n",
      "Score:  83.7144386100132\n",
      "Score:  95.94963530595658\n",
      "Score:  27.800736089059562\n",
      "Score:  39.96051514624237\n",
      "Score:  52.325979424885254\n",
      "Score:  17.293918184941017\n",
      "Score:  25.223377230270955\n",
      "Score:  27.52988086497985\n",
      "Score:  48.680123667773394\n",
      "Score:  33.8930090351012\n",
      "Score:  60.350725572472506\n",
      "Score:  35.99164350623021\n",
      "Score:  19.54122318087798\n",
      "Score:  22.106202822108568\n",
      "Score:  28.6484993307487\n",
      "Score:  39.49449393842774\n",
      "Score:  41.04484722679926\n",
      "Score:  56.48056882323668\n",
      "Score:  31.003942670939885\n",
      "Score:  43.08170457806516\n",
      "Score:  24.1737997878845\n",
      "Score:  40.83986413654715\n",
      "Score:  31.70510836051136\n",
      "Score:  22.942942173904694\n",
      "Score:  36.52889862111293\n",
      "Score:  36.52889862111293\n",
      "Score:  38.043381696469545\n",
      "Eval num_timesteps=105000, episode_reward=37.03 +/- 0.71\n",
      "Episode length: 65.33 +/- 0.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 65.3     |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "---------------------------------\n",
      "Score:  60.86778329300534\n",
      "Score:  25.503381396703205\n",
      "Score:  26.046837681838742\n",
      "Score:  43.052461889286235\n",
      "Score:  23.38285400800643\n",
      "Score:  114.9626284542079\n",
      "Score:  42.50403449843285\n",
      "Score:  28.841027017001064\n",
      "Score:  32.255830401806264\n",
      "Score:  49.2202359751244\n",
      "Score:  54.097914871815405\n",
      "Score:  20.218621283791187\n",
      "Score:  42.898910792672545\n",
      "Score:  44.54654473483322\n",
      "Score:  48.79848480015333\n",
      "Score:  20.154379045316773\n",
      "Score:  61.28750145719158\n",
      "Score:  42.009176922079824\n",
      "Score:  68.15406958461966\n",
      "Score:  22.925118195047528\n",
      "Score:  98.37914326941089\n",
      "Score:  93.41045144219753\n",
      "Score:  28.26420415220188\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67.7     |\n",
      "|    ep_rew_mean     | 47.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Score:  30.417553391053033\n",
      "Score:  34.408101870729226\n",
      "Score:  31.295391485424933\n",
      "Score:  63.44580747124206\n",
      "Score:  36.57809951392027\n",
      "Score:  44.769352549631286\n",
      "Score:  41.929142635910566\n",
      "Score:  71.9616377507725\n",
      "Score:  41.489243428834875\n",
      "Score:  56.56368896490438\n",
      "Score:  25.43736592495854\n",
      "Score:  136.15651873052232\n",
      "Score:  149.92705614342827\n",
      "Score:  36.71470996313131\n",
      "Score:  54.05930628916518\n",
      "Score:  134.88452114481015\n",
      "Score:  51.46809046316767\n",
      "Score:  71.07573741246023\n",
      "Score:  48.5177125130529\n",
      "Score:  62.779010662965966\n",
      "Score:  32.31270440625842\n",
      "Score:  28.76198692669153\n",
      "Score:  84.27192164816235\n",
      "Score:  45.551621988112515\n",
      "Score:  22.842818656418064\n",
      "Score:  29.38161354734781\n",
      "Score:  78.84333984410635\n",
      "Score:  92.05171022301678\n",
      "Score:  75.66451724439143\n",
      "Score:  47.11871957976268\n",
      "Score:  43.77116406472868\n",
      "Score:  38.54728262835102\n",
      "Score:  42.60111186622957\n",
      "Score:  31.390961576852455\n",
      "Score:  30.36803050817328\n",
      "Score:  83.49847536466186\n",
      "Score:  36.70051680610374\n",
      "Score:  61.73498834897743\n",
      "Score:  49.78009821038707\n",
      "Score:  36.85978111949167\n",
      "Score:  36.8209386688588\n",
      "Score:  32.18699629460839\n",
      "Score:  82.84308454809006\n",
      "Score:  29.08770199105792\n",
      "Score:  35.02508170958679\n",
      "Score:  34.348766924104496\n",
      "Score:  44.29460681299343\n",
      "Score:  78.08342277944845\n",
      "Score:  94.65481075606095\n",
      "Score:  94.65481075606095\n",
      "Eval num_timesteps=110000, episode_reward=89.13 +/- 7.81\n",
      "Episode length: 94.33 +/- 3.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 94.3        |\n",
      "|    mean_reward          | 89.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014900357 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.87        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00685    |\n",
      "|    value_loss           | 23.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Score:  30.24296199099208\n",
      "Score:  76.32649416544947\n",
      "Score:  52.70583625944123\n",
      "Score:  55.848567111152846\n",
      "Score:  89.99012297333412\n",
      "Score:  92.72458287336184\n",
      "Score:  63.63920456135422\n",
      "Score:  46.47171310382505\n",
      "Score:  132.10375868935566\n",
      "Score:  106.76049035485396\n",
      "Score:  90.36186140134915\n",
      "Score:  32.72052648613265\n",
      "Score:  39.16885906609409\n",
      "Score:  43.44727493235426\n",
      "Score:  34.78113450178465\n",
      "Score:  20.201477272810116\n",
      "Score:  19.238400292993756\n",
      "Score:  64.8003301092832\n",
      "Score:  77.21528458748476\n",
      "Score:  47.881653265458944\n",
      "Score:  28.24104458468697\n",
      "Score:  64.41763471615141\n",
      "Score:  44.71101046923246\n",
      "Score:  47.67718452041303\n",
      "Score:  27.66283235728245\n",
      "Score:  168.73156461362905\n",
      "Score:  35.53731404756418\n",
      "Score:  66.02723894254049\n",
      "Score:  53.76274696471627\n",
      "Score:  67.33283480068582\n",
      "Score:  73.6309473006462\n",
      "Score:  108.73681820970388\n",
      "Score:  69.56554397253726\n",
      "Score:  56.102194134614116\n",
      "Score:  81.15027167880261\n",
      "Score:  60.85924759722713\n",
      "Score:  42.81420289649565\n",
      "Score:  43.981373120756814\n",
      "Score:  46.572289794848494\n",
      "Score:  36.35605521902158\n",
      "Score:  231.29760358771605\n",
      "Score:  35.15779673146192\n",
      "Score:  26.657774058407217\n",
      "Score:  183.29552910661573\n",
      "Score:  69.83017729792248\n",
      "Score:  68.63253906567203\n",
      "Score:  57.722767400300285\n",
      "Score:  191.6244191124415\n",
      "Score:  50.99070180981059\n",
      "Score:  23.918136658656728\n",
      "Score:  55.513945622073614\n",
      "Score:  21.66603206987938\n",
      "Score:  48.2853097841341\n",
      "Score:  29.97912076483151\n",
      "Score:  31.781854704396917\n",
      "Score:  26.16683984917358\n",
      "Score:  41.48000501367672\n",
      "Score:  50.20874871269724\n",
      "Score:  28.736798288590442\n",
      "Score:  35.613599931650285\n",
      "Score:  34.75179467845249\n",
      "Score:  69.74964641675078\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.2     |\n",
      "|    ep_rew_mean     | 59.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Score:  38.87113449746718\n",
      "Score:  52.34540138920268\n",
      "Score:  25.829655668988725\n",
      "Score:  86.57875237983177\n",
      "Score:  188.6218002917092\n",
      "Score:  188.6218002917092\n",
      "Eval num_timesteps=115000, episode_reward=154.61 +/- 48.10\n",
      "Episode length: 118.33 +/- 17.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 118         |\n",
      "|    mean_reward          | 155         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012680555 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Score:  81.37073722958807\n",
      "Score:  140.57782221088314\n",
      "Score:  67.93296039731786\n",
      "Score:  25.731388901113057\n",
      "Score:  35.21321244472205\n",
      "Score:  79.66164255532897\n",
      "Score:  61.3823997119427\n",
      "Score:  129.05050617634504\n",
      "Score:  44.160501059523966\n",
      "Score:  21.834249756312794\n",
      "Score:  68.37041048565135\n",
      "Score:  64.4960258775541\n",
      "Score:  244.85405499786134\n",
      "Score:  95.94385176874931\n",
      "Score:  167.95124633098865\n",
      "Score:  79.40942767992455\n",
      "Score:  150.38890807243624\n",
      "Score:  24.290837860921567\n",
      "Score:  39.40823373990269\n",
      "Score:  105.03488333613575\n",
      "Score:  76.50271995421916\n",
      "Score:  46.6643791801546\n",
      "Score:  49.87443881503946\n",
      "Score:  84.25331010414449\n",
      "Score:  102.27036234846017\n",
      "Score:  74.983946587473\n",
      "Score:  81.69227817731638\n",
      "Score:  132.13873457812846\n",
      "Score:  31.06614202381236\n",
      "Score:  32.564399015614214\n",
      "Score:  87.42533428651176\n",
      "Score:  47.50916178254657\n",
      "Score:  195.5704380933377\n",
      "Score:  253.79437549355578\n",
      "Score:  32.552157756555225\n",
      "Score:  85.19692423076344\n",
      "Score:  60.882248193580146\n",
      "Score:  137.55388127213934\n",
      "Score:  48.4391656976583\n",
      "Score:  68.61337567198653\n",
      "Score:  63.09708101461872\n",
      "Score:  68.11270395524151\n",
      "Score:  40.67251403630174\n",
      "Score:  115.64076837128403\n",
      "Score:  214.48455124034996\n",
      "Score:  81.3397684583788\n",
      "Score:  34.047683929631745\n",
      "Score:  51.742744756717066\n",
      "Score:  36.89133852946719\n",
      "Score:  53.25515978075044\n",
      "Score:  66.45029021171526\n",
      "Score:  49.30284887282827\n",
      "Score:  76.40026386269938\n",
      "Score:  70.46249357731506\n",
      "Score:  151.7612008324948\n",
      "Score:  37.70751082680123\n",
      "Score:  111.64142453682443\n",
      "Score:  83.50078555507959\n",
      "Score:  44.067303695930306\n",
      "Score:  86.57875237983177\n",
      "Score:  86.57875237983177\n",
      "Score:  188.6218002917092\n",
      "Eval num_timesteps=120000, episode_reward=120.59 +/- 48.10\n",
      "Episode length: 105.67 +/- 17.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 106      |\n",
      "|    mean_reward     | 121      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Score:  72.9336189326636\n",
      "Score:  37.52744454694747\n",
      "Score:  37.68761043925781\n",
      "Score:  371.7078526690273\n",
      "Score:  63.769851926616674\n",
      "Score:  102.57088423434107\n",
      "Score:  90.75719155112634\n",
      "Score:  106.86106766339087\n",
      "Score:  47.303712081025346\n",
      "Score:  38.38156338612049\n",
      "Score:  62.52920586368242\n",
      "Score:  17.768227610447894\n",
      "Score:  28.49650486845947\n",
      "Score:  39.63454565741178\n",
      "Score:  48.53433280005752\n",
      "Score:  23.349503191474213\n",
      "Score:  80.12288254940617\n",
      "Score:  41.54949570980902\n",
      "Score:  31.307654420009353\n",
      "Score:  51.24477991653661\n",
      "Score:  191.8990380653\n",
      "Score:  309.0488023209011\n",
      "Score:  55.323951583060534\n",
      "Score:  74.07960702648451\n",
      "Score:  68.84249573262144\n",
      "Score:  59.83375670505645\n",
      "Score:  97.5789905358838\n",
      "Score:  35.40256149376087\n",
      "Score:  30.25221699010138\n",
      "Score:  37.01464086249384\n",
      "Score:  221.25780158678072\n",
      "Score:  73.38826220766057\n",
      "Score:  21.140204340826152\n",
      "Score:  65.1301162248799\n",
      "Score:  58.39803862226317\n",
      "Score:  34.898038330165186\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.3     |\n",
      "|    ep_rew_mean     | 79.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Score:  24.994379244392796\n",
      "Score:  44.59033298787245\n",
      "Score:  78.76730218500576\n",
      "Score:  90.45515571819224\n",
      "Score:  48.17121419820482\n",
      "Score:  86.72132696572785\n",
      "Score:  30.994224174872386\n",
      "Score:  123.75257998335059\n",
      "Score:  44.04863683810631\n",
      "Score:  59.25527904147632\n",
      "Score:  205.59386198834045\n",
      "Score:  70.32324326213649\n",
      "Score:  54.44012543366484\n",
      "Score:  248.37230514486353\n",
      "Score:  51.74269318335226\n",
      "Score:  42.27905830619928\n",
      "Score:  71.85569377962234\n",
      "Score:  103.8317157647966\n",
      "Score:  90.37365381660351\n",
      "Score:  144.29386741589195\n",
      "Score:  54.58867214511526\n",
      "Score:  143.8463542146454\n",
      "Score:  404.94405596682367\n",
      "Score:  104.78442769429033\n",
      "Score:  104.78442769429033\n",
      "Eval num_timesteps=125000, episode_reward=204.84 +/- 141.50\n",
      "Episode length: 129.67 +/- 39.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 130         |\n",
      "|    mean_reward          | 205         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009086547 |\n",
      "|    clip_fraction        | 0.0724      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.386      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00426    |\n",
      "|    value_loss           | 63.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Score:  143.5545663018582\n",
      "Score:  45.29707565324572\n",
      "Score:  75.08771942717158\n",
      "Score:  110.68084727211703\n",
      "Score:  49.28828871165669\n",
      "Score:  117.58883284270797\n",
      "Score:  127.53139360377934\n",
      "Score:  407.7494364160919\n",
      "Score:  136.55265852328026\n",
      "Score:  104.25936076172823\n",
      "Score:  107.138591514705\n",
      "Score:  61.314491441886496\n",
      "Score:  75.44915292805253\n",
      "Score:  113.50388369687026\n",
      "Score:  49.68507337893593\n",
      "Score:  59.695899112821614\n",
      "Score:  39.491331919601315\n",
      "Score:  87.44485366720737\n",
      "Score:  96.94920428002854\n",
      "Score:  159.85283416945006\n",
      "Score:  60.63181843216654\n",
      "Score:  55.12611225985295\n",
      "Score:  44.82435214672549\n",
      "Score:  80.44701342371438\n",
      "Score:  62.86454628704025\n",
      "Score:  87.65419345375615\n",
      "Score:  93.53211043885888\n",
      "Score:  86.92650521128732\n",
      "Score:  83.45863188970723\n",
      "Score:  43.13202569956622\n",
      "Score:  60.896253232767805\n",
      "Score:  80.4544115939408\n",
      "Score:  72.61359368159859\n",
      "Score:  143.0674830310051\n",
      "Score:  115.05441503449089\n",
      "Score:  51.866283348854346\n",
      "Score:  50.4277868129085\n",
      "Score:  102.8016798065337\n",
      "Score:  91.80405599270969\n",
      "Score:  59.21328897255549\n",
      "Score:  57.44227523104007\n",
      "Score:  161.79171220679257\n",
      "Score:  102.04150340857886\n",
      "Score:  109.76731823780506\n",
      "Score:  69.54605872471487\n",
      "Score:  94.20503526784883\n",
      "Score:  329.2868471075453\n",
      "Score:  40.23412033365655\n",
      "Score:  101.24858671565843\n",
      "Score:  100.47145932538255\n",
      "Score:  60.6145011606476\n",
      "Score:  97.99378516343948\n",
      "Score:  43.45749206661197\n",
      "Score:  101.77628203270474\n",
      "Score:  404.94405596682367\n",
      "Score:  404.94405596682367\n",
      "Score:  104.78442769429033\n",
      "Eval num_timesteps=130000, episode_reward=304.89 +/- 141.50\n",
      "Episode length: 157.33 +/- 39.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 157      |\n",
      "|    mean_reward     | 305      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Score:  55.1014011032552\n",
      "Score:  46.837006415138475\n",
      "Score:  81.67755368370926\n",
      "Score:  49.656599473185246\n",
      "Score:  137.12324611897506\n",
      "Score:  113.05545239321528\n",
      "Score:  57.05228741632119\n",
      "Score:  313.50802186680124\n",
      "Score:  145.61618329107483\n",
      "Score:  34.32713911142462\n",
      "Score:  22.102585963495514\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.4     |\n",
      "|    ep_rew_mean     | 90.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 522      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Score:  98.40822863197357\n",
      "Score:  129.65258117953837\n",
      "Score:  94.72593839969038\n",
      "Score:  60.98796725107427\n",
      "Score:  57.823174946353\n",
      "Score:  98.78240546794068\n",
      "Score:  123.7796530808578\n",
      "Score:  72.35860529716048\n",
      "Score:  213.90481860306429\n",
      "Score:  333.6341442422516\n",
      "Score:  145.40138220594227\n",
      "Score:  58.42648387396921\n",
      "Score:  166.16389289539342\n",
      "Score:  123.79118229932287\n",
      "Score:  396.05172602820613\n",
      "Score:  73.58292468419316\n",
      "Score:  73.14451567277685\n",
      "Score:  137.47490676874153\n",
      "Score:  400.302771131677\n",
      "Score:  1049.0414087957693\n",
      "Score:  63.65387330935668\n",
      "Score:  70.70515131378149\n",
      "Score:  620.9625538804513\n",
      "Score:  47.157901890145055\n",
      "Score:  171.02255375491333\n",
      "Score:  172.1645482026273\n",
      "Score:  55.93016829496053\n",
      "Score:  299.2196629148481\n",
      "Score:  78.27841386911804\n",
      "Score:  418.39657773203237\n",
      "Score:  118.90401686430465\n",
      "Score:  52.120058821384305\n",
      "Score:  55.720077903089354\n",
      "Score:  348.2281393362151\n",
      "Score:  348.2281393362151\n",
      "Score:  245.8549118942693\n",
      "Eval num_timesteps=135000, episode_reward=314.10 +/- 48.26\n",
      "Episode length: 168.00 +/- 12.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 168         |\n",
      "|    mean_reward          | 314         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012753306 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00537    |\n",
      "|    value_loss           | 58          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Score:  80.86987003856504\n",
      "Score:  83.49362027388807\n",
      "Score:  186.03799266713781\n",
      "Score:  349.93741555771055\n",
      "Score:  81.16155101634621\n",
      "Score:  81.77420794545955\n",
      "Score:  164.38894466275042\n",
      "Score:  125.15044397959998\n",
      "Score:  73.90169630144139\n",
      "Score:  88.62350548718796\n",
      "Score:  168.9118805270095\n",
      "Score:  308.64666985803717\n",
      "Score:  45.113089552307244\n",
      "Score:  73.61965953375088\n",
      "Score:  211.45406201463635\n",
      "Score:  94.89286586369218\n",
      "Score:  126.87880281000145\n",
      "Score:  66.52951195394856\n",
      "Score:  83.34341978958155\n",
      "Score:  89.03109428778625\n",
      "Score:  285.2870842289029\n",
      "Score:  115.3763739275762\n",
      "Score:  71.44713018095797\n",
      "Score:  44.801642448677946\n",
      "Score:  150.31357140539623\n",
      "Score:  367.15950287775127\n",
      "Score:  200.56738318383387\n",
      "Score:  110.57274950689182\n",
      "Score:  682.9302466252533\n",
      "Score:  474.9073175986943\n",
      "Score:  743.5345614688185\n",
      "Score:  203.05090410240345\n",
      "Score:  93.97972148278743\n",
      "Score:  82.45451439221246\n",
      "Score:  70.19970791648473\n",
      "Score:  92.17808926921725\n",
      "Score:  81.8564772279534\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 522      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Score:  134.43873039664038\n",
      "Score:  52.36698415041881\n",
      "Score:  105.71279492934266\n",
      "Score:  186.47451920087173\n",
      "Score:  124.61629800900627\n",
      "Score:  2216.906616102151\n",
      "Score:  303.9757316359596\n",
      "Score:  303.9757316359596\n",
      "Eval num_timesteps=140000, episode_reward=941.62 +/- 901.76\n",
      "Episode length: 254.00 +/- 124.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 254         |\n",
      "|    mean_reward          | 942         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005529841 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 217         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00126    |\n",
      "|    value_loss           | 274         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Score:  111.11504634275303\n",
      "Score:  181.87278325829638\n",
      "Score:  337.87532797905715\n",
      "Score:  89.2441177628278\n",
      "Score:  76.08498986811013\n",
      "Score:  478.29986590206636\n",
      "Score:  104.08563556819541\n",
      "Score:  73.16536356404151\n",
      "Score:  1737.7637153977007\n",
      "Score:  147.981269023223\n",
      "Score:  53.39068008764694\n",
      "Score:  258.30619749042717\n",
      "Score:  158.27375952036152\n",
      "Score:  124.09241675315927\n",
      "Score:  260.6823793416536\n",
      "Score:  153.28167721268966\n",
      "Score:  51.531622746394454\n",
      "Score:  76.89586196555216\n",
      "Score:  120.35744976230202\n",
      "Score:  797.70981862675\n",
      "Score:  164.8429107473151\n",
      "Score:  117.80515846713656\n",
      "Score:  191.87924028980999\n",
      "Score:  286.70616341181494\n",
      "Score:  107.20271019373463\n",
      "Score:  72.46033867427307\n",
      "Score:  253.16333221479863\n",
      "Score:  90.31017808852509\n",
      "Score:  49.51790028895946\n",
      "Score:  135.5890452127058\n",
      "Score:  112.81683148408901\n",
      "Score:  204.01747760939278\n",
      "Score:  341.9514470036899\n",
      "Score:  278.60820770008047\n",
      "Score:  175.8066973746068\n",
      "Score:  134.29486015783226\n",
      "Score:  108.81223075992222\n",
      "Score:  74.47163174760189\n",
      "Score:  118.03244502139489\n",
      "Score:  129.9578096602828\n",
      "Score:  2216.906616102151\n",
      "Score:  2216.906616102151\n",
      "Score:  303.9757316359596\n",
      "Eval num_timesteps=145000, episode_reward=1579.26 +/- 901.76\n",
      "Episode length: 342.00 +/- 124.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 342      |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Score:  52.798076341307784\n",
      "Score:  559.1579617605975\n",
      "Score:  51.31507298926917\n",
      "Score:  101.72891477277908\n",
      "Score:  212.42651977320222\n",
      "Score:  463.10011406550103\n",
      "Score:  706.2870015838329\n",
      "Score:  135.21757629996262\n",
      "Score:  220.95448861304916\n",
      "Score:  117.13834351371644\n",
      "Score:  399.5945366728465\n",
      "Score:  55.18554116192841\n",
      "Score:  82.90939387281622\n",
      "Score:  134.4442659233147\n",
      "Score:  81.89259859696418\n",
      "Score:  624.6339829300637\n",
      "Score:  237.31239872315913\n",
      "Score:  69.30212240677358\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 199      |\n",
      "| time/              |          |\n",
      "|    fps             | 517      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Score:  161.8967842118292\n",
      "Score:  66.34872950833261\n",
      "Score:  112.47374282143912\n",
      "Score:  170.79018161840676\n",
      "Score:  44.27807709083957\n",
      "Score:  245.7983145159197\n",
      "Score:  70.46979224836947\n",
      "Score:  90.64546551759503\n",
      "Score:  88.75173261972505\n",
      "Score:  637.8521798972308\n",
      "Score:  352.01931599856516\n",
      "Score:  76.6564665120911\n",
      "Score:  103.74699890969197\n",
      "Score:  157.07153095468178\n",
      "Score:  161.03098950784465\n",
      "Score:  59.4509969849126\n",
      "Score:  169.16829005672201\n",
      "Score:  130.7346067341493\n",
      "Score:  177.1591241220229\n",
      "Score:  318.2447416958859\n",
      "Score:  130.00724427724174\n",
      "Score:  71.10899218507782\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12097403.686998978\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12097403.686998978\n",
      "check point:  60.0\n",
      "Score:  366.1712690474757\n",
      "Eval num_timesteps=150000, episode_reward=8065790.04 +/- 5703115.91\n",
      "Episode length: 20060.67 +/- 14056.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.01e+04    |\n",
      "|    mean_reward          | 8.07e+06    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007674595 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 266         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 343         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Score:  614.420723025323\n",
      "Score:  395.08262696559086\n",
      "Score:  114.89491515571763\n",
      "Score:  77.3494380230026\n",
      "Score:  382.0542458245881\n",
      "Score:  61.088968094395064\n",
      "Score:  149.8009308704519\n",
      "Score:  279.70056968884955\n",
      "Score:  415.06511262113247\n",
      "Score:  447.6768364906361\n",
      "Score:  539.8423271701879\n",
      "Score:  64.31236571574154\n",
      "Score:  200.86083838606748\n",
      "Score:  111.66533254724115\n",
      "Score:  125.21490653442429\n",
      "Score:  514.7118932371214\n",
      "Score:  149.8264520956005\n",
      "Score:  84.78946259313388\n",
      "Score:  149.95400778646584\n",
      "Score:  499.7911341902257\n",
      "Score:  144.86043440920045\n",
      "Score:  222.16656675976867\n",
      "Score:  82.86846646722923\n",
      "Score:  208.2215870056399\n",
      "Score:  70.73027890978535\n",
      "Score:  147.2353864753081\n",
      "Score:  82.05377482445618\n",
      "Score:  330.60478697464015\n",
      "Score:  308.853482173481\n",
      "Score:  81.80931302854916\n",
      "Score:  233.10665063974207\n",
      "Score:  97.64415179019223\n",
      "Score:  319.6510085573973\n",
      "Score:  61.069212948559105\n",
      "Score:  181.5649348041114\n",
      "Score:  111.38054514707045\n",
      "Score:  67.89540943392585\n",
      "Score:  151.64562374611464\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12097403.686998978\n",
      "check point:  60.0\n",
      "Score:  366.1712690474757\n",
      "Score:  366.1712690474757\n",
      "Eval num_timesteps=155000, episode_reward=4033078.11 +/- 5703115.91\n",
      "Episode length: 10121.33 +/- 14056.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.01e+04 |\n",
      "|    mean_reward     | 4.03e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Score:  227.78495256719134\n",
      "Score:  84.86227499019233\n",
      "Score:  705.9915970042058\n",
      "Score:  189.25553214578858\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | 203      |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 442      |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Score:  85.82499102467628\n",
      "Score:  695.8077176162573\n",
      "Score:  499.12734945781887\n",
      "Score:  214.96710059732584\n",
      "Score:  644.3756210047679\n",
      "Score:  172.9211757939658\n",
      "Score:  157.08552910546203\n",
      "Score:  181.88185585561902\n",
      "Score:  460.3600781524827\n",
      "Score:  117.46841765936607\n",
      "Score:  380.18599841206844\n",
      "Score:  323.34198127374543\n",
      "Score:  1077.2386062078833\n",
      "Score:  386.48795677985714\n",
      "Score:  336.65304182210946\n",
      "Score:  171.68552651967065\n",
      "Score:  105.42170974386138\n",
      "Score:  124.5171040350891\n",
      "Score:  354.68400945494375\n",
      "Score:  172.44593995337692\n",
      "Score:  750.182822562666\n",
      "Score:  1193.6164488779862\n",
      "Score:  118.38165171211463\n",
      "Score:  164.86600643940844\n",
      "Score:  121.99536797432341\n",
      "Score:  410.3399350004055\n",
      "Score:  719.0741512822449\n",
      "Score:  719.0741512822449\n",
      "Score:  719.0741512822449\n",
      "Eval num_timesteps=160000, episode_reward=719.07 +/- 0.00\n",
      "Episode length: 252.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 252         |\n",
      "|    mean_reward          | 719         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007750254 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 132         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    value_loss           | 211         |\n",
      "-----------------------------------------\n",
      "Score:  355.89116536137414\n",
      "Score:  528.3191239886573\n",
      "Score:  248.57018377892985\n",
      "Score:  135.85943976926558\n",
      "Score:  63.053704682296114\n",
      "Score:  493.32512843800936\n",
      "Score:  1385.8983228962477\n",
      "check point:  1.0\n",
      "Score:  848.1947408320232\n",
      "Score:  4579.687081722322\n",
      "Score:  443.1555633918027\n",
      "Score:  1479.5543025362183\n",
      "Score:  346.50140069952954\n",
      "Score:  328.02211149045814\n",
      "Score:  230.8042128169436\n",
      "Score:  746.219769311323\n",
      "Score:  324.7674650767388\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 154      |\n",
      "|    ep_rew_mean     | 342      |\n",
      "| time/              |          |\n",
      "|    fps             | 357      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 458      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Score:  178.53829066251845\n",
      "Score:  959.3965552494128\n",
      "Score:  719.7485557569664\n",
      "Score:  603.9276710943368\n",
      "Score:  493.743846034375\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13136797.925351707\n",
      "check point:  60.0\n",
      "Score:  1402.25152435466\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13136797.925351707\n",
      "check point:  60.0\n",
      "Eval num_timesteps=165000, episode_reward=8759117.90 +/- 6192640.12\n",
      "Episode length: 20116.00 +/- 13978.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.01e+04     |\n",
      "|    mean_reward          | 8.76e+06     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 165000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037104925 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 706          |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 1.24e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "check point:  1.0\n",
      "Score:  3763.464278304037\n",
      "Score:  929.9754154328785\n",
      "Score:  576.2991950070216\n",
      "Score:  182.36742384246492\n",
      "Score:  67.41607420634219\n",
      "Score:  1081.4660912881911\n",
      "Score:  540.1117015184119\n",
      "Score:  265.7239927769685\n",
      "Score:  223.6234454724846\n",
      "Score:  187.69913180870915\n",
      "Score:  868.14650255606\n",
      "Score:  123.14097877409752\n",
      "Score:  164.69936307256708\n",
      "Score:  1359.4623015726236\n",
      "Score:  1490.0998952593786\n",
      "Score:  226.02122298124755\n",
      "Score:  124.26879914256577\n",
      "Score:  206.02154407730296\n",
      "Score:  922.4837782259805\n",
      "Score:  528.2270526134813\n",
      "Score:  208.7481508687735\n",
      "Score:  824.8242794642655\n",
      "Score:  172.87430791718268\n",
      "Score:  1786.784227491617\n",
      "Score:  648.8444246680813\n",
      "Score:  1402.25152435466\n",
      "Score:  1402.25152435466\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13136797.925351707\n",
      "check point:  60.0\n",
      "Eval num_timesteps=170000, episode_reward=4380260.07 +/- 6192640.12\n",
      "Episode length: 10232.00 +/- 13978.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.02e+04 |\n",
      "|    mean_reward     | 4.38e+06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "---------------------------------\n",
      "Score:  325.33555921290866\n",
      "Score:  608.5023507131082\n",
      "Score:  403.1794660268776\n",
      "Score:  296.28351776520174\n",
      "Score:  887.8189828832119\n",
      "Score:  376.6985181087969\n",
      "Score:  475.8781577781311\n",
      "Score:  813.1477087141426\n",
      "Score:  444.4403868598743\n",
      "Score:  204.28434800713538\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 186      |\n",
      "|    ep_rew_mean     | 507      |\n",
      "| time/              |          |\n",
      "|    fps             | 277      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 619      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Score:  185.89163085398485\n",
      "Score:  474.28132395171195\n",
      "Score:  2158.4609614560964\n",
      "Score:  322.83916692737813\n",
      "Score:  1160.5358587774226\n",
      "Score:  2044.1641186332433\n",
      "Score:  201.7448918934611\n",
      "Score:  239.88971541739116\n",
      "Score:  230.8669479594339\n",
      "Score:  392.1818207283453\n",
      "Score:  702.8915565994951\n",
      "Score:  421.19848801354505\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12065202.846632805\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13390900.00584068\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12065202.846632805\n",
      "check point:  60.0\n",
      "Eval num_timesteps=175000, episode_reward=12508229.92 +/- 624987.94\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3e+04       |\n",
      "|    mean_reward          | 1.25e+07    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 175000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005859095 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 385         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 890         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Score:  629.6906475958268\n",
      "Score:  443.5349533041948\n",
      "Score:  500.6941783022524\n",
      "Score:  665.1503198402295\n",
      "Score:  559.0633998802302\n",
      "Score:  607.3851705401038\n",
      "Score:  358.1719480342871\n",
      "Score:  976.7004787974992\n",
      "Score:  1043.9134671332447\n",
      "Score:  1108.8787196587245\n",
      "Score:  3006.3624277038757\n",
      "Score:  185.0553742821421\n",
      "Score:  189.5673942901947\n",
      "Score:  378.7209305216106\n",
      "Score:  334.20506172446676\n",
      "Score:  425.6890203517783\n",
      "Score:  714.1517455731527\n",
      "Score:  461.48094396388393\n",
      "Score:  338.5824512383374\n",
      "Score:  847.2689923852042\n",
      "Score:  2384.422734834796\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12065202.846632805\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12065202.846632805\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13390900.00584068\n",
      "check point:  60.0\n",
      "Eval num_timesteps=180000, episode_reward=12508229.92 +/- 624987.94\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3e+04    |\n",
      "|    mean_reward     | 1.25e+07 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "Score:  317.8697036005678\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 214      |\n",
      "|    ep_rew_mean     | 663      |\n",
      "| time/              |          |\n",
      "|    fps             | 195      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 920      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Score:  1168.6190930979042\n",
      "Score:  886.6363314147115\n",
      "check point:  1.0\n",
      "Score:  3540.0408546978583\n",
      "Score:  812.7000554641429\n",
      "Score:  433.6044003106816\n",
      "check point:  1.0\n",
      "check point:  1.0\n",
      "Score:  4959.043170000976\n",
      "Score:  1694.5923539766825\n",
      "Score:  2172.241554938502\n",
      "Score:  8306.693329389325\n",
      "Score:  808.1538985602692\n",
      "Score:  2376.0180087390668\n",
      "Score:  597.2335830028543\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13465501.911302002\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12475848.30992333\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13465560.891716344\n",
      "check point:  60.0\n",
      "Eval num_timesteps=185000, episode_reward=13136810.78 +/- 466572.02\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.31e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 185000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061420137 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 503          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 968          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Score:  574.7510585424193\n",
      "check point:  1.0\n",
      "Score:  3296.088540613769\n",
      "Score:  683.2997482142258\n",
      "check point:  1.0\n",
      "Score:  556.174792682832\n",
      "check point:  1.0\n",
      "Score:  845.3757911980186\n",
      "check point:  1.0\n",
      "Score:  3509.7594782992237\n",
      "Score:  13330.968299115006\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 254      |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 1077     |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "Score:  1286.9360323654694\n",
      "check point:  2.0\n",
      "Score:  1460.742242509798\n",
      "Score:  15180.454634021673\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13013376.614613106\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13013376.614613106\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13462741.705017876\n",
      "check point:  60.0\n",
      "Eval num_timesteps=190000, episode_reward=13164346.62 +/- 211843.79\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.32e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 190000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033308319 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.429       |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.97e+03     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000235    |\n",
      "|    value_loss           | 8.2e+03      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "check point:  1.0\n",
      "Score:  3501.05782998772\n",
      "check point:  1.0\n",
      "Score:  3775.418713487517\n",
      "check point:  1.0\n",
      "Score:  4021.568214343675\n",
      "check point:  1.0\n",
      "Score:  4303.507046026647\n",
      "Score:  1549.618710867342\n",
      "Score:  929.6568752755284\n",
      "Score:  952.1163244467848\n",
      "Score:  979.7089268829495\n",
      "check point:  1.0\n",
      "check point:  1.0\n",
      "Score:  4147.664212684809\n",
      "check point:  1.0\n",
      "check point:  1.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13013376.614613106\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13462741.705017876\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13013376.614613106\n",
      "check point:  60.0\n",
      "Eval num_timesteps=195000, episode_reward=13164346.62 +/- 211843.79\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3e+04    |\n",
      "|    mean_reward     | 1.32e+07 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195000   |\n",
      "---------------------------------\n",
      "check point:  2.0\n",
      "Score:  530.2244658363103\n",
      "Score:  20203.82766960192\n",
      "Score:  8493.910420859378\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 298      |\n",
      "|    ep_rew_mean     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 142      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1380     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Score:  362.4517019343992\n",
      "Score:  11349.11236549646\n",
      "Score:  941.6715885840892\n",
      "check point:  1.0\n",
      "Score:  1904.175682858731\n",
      "Score:  2727.5468993957343\n",
      "check point:  1.0\n",
      "Score:  1057.818549989797\n",
      "check point:  2.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13410570.002240114\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13410570.002240114\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13410570.002240114\n",
      "check point:  60.0\n",
      "Eval num_timesteps=200000, episode_reward=13411761.53 +/- 0.00\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.34e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027799485 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.429       |\n",
      "|    explained_variance   | 0.33         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.59e+03     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000272    |\n",
      "|    value_loss           | 1.44e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Score:  1004.4410907470142\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "Score:  5821.78010798511\n",
      "check point:  3.0\n",
      "Score:  833.8591832621469\n",
      "Score:  32094.98335300681\n",
      "check point:  3.0\n",
      "check point:  1.0\n",
      "check point:  1.0\n",
      "check point:  1.0\n",
      "Score:  37328.80364272644\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 348      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 1537     |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13105039.165438743\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13468488.982568404\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13096178.103847371\n",
      "check point:  60.0\n",
      "Eval num_timesteps=205000, episode_reward=13224417.46 +/- 173467.65\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3e+04       |\n",
      "|    mean_reward          | 1.32e+07    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 205000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004253413 |\n",
      "|    clip_fraction        | 0.0207      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.3e+04     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.000467    |\n",
      "|    value_loss           | 3.79e+04    |\n",
      "-----------------------------------------\n",
      "check point:  2.0\n",
      "check point:  2.0\n",
      "Score:  13462.038285351016\n",
      "check point:  1.0\n",
      "Score:  20196.38000782372\n",
      "Score:  1322.3851514617434\n",
      "check point:  3.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "Score:  19365.96965109739\n",
      "Score:  48014.564583130596\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13105039.165438743\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13468488.982568404\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13469367.640706895\n",
      "check point:  60.0\n",
      "Eval num_timesteps=210000, episode_reward=13348820.95 +/- 171550.45\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3e+04    |\n",
      "|    mean_reward     | 1.33e+07 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Score:  15840.11096953977\n",
      "Score:  2251.08713536796\n",
      "Score:  10127.850084585743\n",
      "check point:  1.0\n",
      "Score:  1106.7644860392559\n",
      "Score:  806.4400632115106\n",
      "Score:  168.67078366342918\n",
      "check point:  1.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 411      |\n",
      "|    ep_rew_mean     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 115      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 1840     |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Score:  3001.4525023245283\n",
      "check point:  2.0\n",
      "Score:  1582.1079703164687\n",
      "check point:  2.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13441367.664563397\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13471080.576301908\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13471080.576301908\n",
      "check point:  60.0\n",
      "Eval num_timesteps=215000, episode_reward=13462374.40 +/- 14007.77\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.35e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 215000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015425218 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.471       |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.06e+04     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000272    |\n",
      "|    value_loss           | 5.22e+04     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "check point:  1.0\n",
      "check point:  3.0\n",
      "check point:  1.0\n",
      "Score:  11738.232703445425\n",
      "check point:  3.0\n",
      "check point:  2.0\n",
      "check point:  4.0\n",
      "check point:  1.0\n",
      "check point:  4.0\n",
      "check point:  3.0\n",
      "check point:  5.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13441367.664563397\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13471080.576301908\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13471080.576301908\n",
      "check point:  60.0\n",
      "Eval num_timesteps=220000, episode_reward=13462374.40 +/- 14007.77\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3e+04    |\n",
      "|    mean_reward     | 1.35e+07 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Score:  8081.258265277947\n",
      "check point:  5.0\n",
      "check point:  4.0\n",
      "check point:  6.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 429      |\n",
      "|    ep_rew_mean     | 4.03e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 103      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 2139     |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Score:  56875.71938988991\n",
      "check point:  1.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  7.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13470111.542575888\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13471332.688724691\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13470111.542575888\n",
      "check point:  60.0\n",
      "Eval num_timesteps=225000, episode_reward=13471718.13 +/- 575.16\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.35e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 225000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020376863 |\n",
      "|    clip_fraction        | 0.00966      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | -0.101       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+05      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | 0.000239     |\n",
      "|    value_loss           | 2.57e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "check point:  8.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  4.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 446      |\n",
      "|    ep_rew_mean     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 99       |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 2295     |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13468798.696876641\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13044283.090232318\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13468798.696876641\n",
      "check point:  60.0\n",
      "Eval num_timesteps=230000, episode_reward=13328485.09 +/- 200128.88\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.33e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005759065 |\n",
      "|    clip_fraction        | 0.00319      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.504       |\n",
      "|    explained_variance   | 0.00583      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.71e+05     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | 0.000399     |\n",
      "|    value_loss           | 1.48e+06     |\n",
      "------------------------------------------\n",
      "check point:  5.0\n",
      "Score:  75301.2908463672\n",
      "Score:  403482.60302910645\n",
      "check point:  10.0\n",
      "Score:  122227.57572274271\n",
      "check point:  1.0\n",
      "check point:  1.0\n",
      "check point:  11.0\n",
      "Score:  5250.4942709899415\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "Score:  493217.9908559794\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13044283.090232318\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13468798.696876641\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13468798.696876641\n",
      "check point:  60.0\n",
      "Eval num_timesteps=235000, episode_reward=13328485.09 +/- 200128.88\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3e+04    |\n",
      "|    mean_reward     | 1.33e+07 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "Score:  22965.12197797326\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 621      |\n",
      "|    ep_rew_mean     | 1.58e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 91       |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 2593     |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "Score:  957.5840632147183\n",
      "check point:  4.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13284430.35922051\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13087948.812952768\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13087948.812952768\n",
      "check point:  60.0\n",
      "Eval num_timesteps=240000, episode_reward=13154625.37 +/- 92632.97\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.32e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025978757 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.0283       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.92e+05     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | 0.00061      |\n",
      "|    value_loss           | 1.07e+06     |\n",
      "------------------------------------------\n",
      "Score:  2152.4954611405287\n",
      "check point:  5.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  1.0\n",
      "check point:  6.0\n",
      "Score:  5373.807727683215\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  7.0\n",
      "check point:  1.0\n",
      "check point:  5.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13087948.812952768\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13087948.812952768\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13087948.812952768\n",
      "check point:  60.0\n",
      "Eval num_timesteps=245000, episode_reward=13089123.97 +/- 0.00\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3e+04    |\n",
      "|    mean_reward     | 1.31e+07 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "check point:  6.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 626      |\n",
      "|    ep_rew_mean     | 1.58e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 2888     |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "check point:  8.0\n",
      "check point:  2.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "Score:  167146.9098080369\n",
      "check point:  9.0\n",
      "check point:  3.0\n",
      "check point:  8.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13205706.689400595\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13204220.827445997\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  13290852.155273909\n",
      "check point:  60.0\n",
      "Eval num_timesteps=250000, episode_reward=13234767.42 +/- 40489.59\n",
      "Episode length: 30000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3e+04        |\n",
      "|    mean_reward          | 1.32e+07     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 250000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025447505 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.0491       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.35e+05     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    value_loss           | 6.2e+05      |\n",
      "------------------------------------------\n",
      "check point:  1.0\n",
      "check point:  10.0\n",
      "check point:  4.0\n",
      "check point:  9.0\n",
      "Score:  81887.6829959138\n",
      "check point:  2.0\n",
      "check point:  11.0\n",
      "check point:  10.0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 678      |\n",
      "|    ep_rew_mean     | 1.83e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 3042     |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "check point:  1.0\n",
      "check point:  3.0\n",
      "check point:  12.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n",
      "check point:  26.0\n",
      "check point:  27.0\n",
      "check point:  28.0\n",
      "check point:  29.0\n",
      "check point:  30.0\n",
      "check point:  31.0\n",
      "check point:  32.0\n",
      "check point:  33.0\n",
      "check point:  34.0\n",
      "check point:  35.0\n",
      "check point:  36.0\n",
      "check point:  37.0\n",
      "check point:  38.0\n",
      "check point:  39.0\n",
      "check point:  40.0\n",
      "check point:  41.0\n",
      "check point:  42.0\n",
      "check point:  43.0\n",
      "check point:  44.0\n",
      "check point:  45.0\n",
      "check point:  46.0\n",
      "check point:  47.0\n",
      "check point:  48.0\n",
      "check point:  49.0\n",
      "check point:  50.0\n",
      "check point:  51.0\n",
      "check point:  52.0\n",
      "check point:  53.0\n",
      "check point:  54.0\n",
      "check point:  55.0\n",
      "check point:  56.0\n",
      "check point:  57.0\n",
      "check point:  58.0\n",
      "check point:  59.0\n",
      "Score:  12868700.335328111\n",
      "check point:  60.0\n",
      "check point:  1.0\n",
      "check point:  2.0\n",
      "check point:  3.0\n",
      "check point:  4.0\n",
      "check point:  5.0\n",
      "check point:  6.0\n",
      "check point:  7.0\n",
      "check point:  8.0\n",
      "check point:  9.0\n",
      "check point:  10.0\n",
      "check point:  11.0\n",
      "check point:  12.0\n",
      "check point:  13.0\n",
      "check point:  14.0\n",
      "check point:  15.0\n",
      "check point:  16.0\n",
      "check point:  17.0\n",
      "check point:  18.0\n",
      "check point:  19.0\n",
      "check point:  20.0\n",
      "check point:  21.0\n",
      "check point:  22.0\n",
      "check point:  23.0\n",
      "check point:  24.0\n",
      "check point:  25.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-41646e45d84c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m model = training.train_ppo(\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0meval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d487a0601954>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(self, total_timesteps, save_freq, eval_freq, eval_episodes)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         self.model.learn(\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Give access to local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Return False (stop training) if at least one callback returns False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_success_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             episode_rewards, episode_lengths = evaluate_policy(\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mepisode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode_counts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepisode_count_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         actions, states = model.predict(\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \"\"\"\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert to numpy, and reshape to the original action shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc, assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTaken\u001b[0m \u001b[0maction\u001b[0m \u001b[0maccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \"\"\"\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTorchObs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_features_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mlatent_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTorchObs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;31m# Here mean_actions are the flattened logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip constraints that cannot be checked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 if param not in self.__dict__ and isinstance(\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 ):\n\u001b[1;32m     67\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_lazy_property_and_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Memory-optimized training setup\n",
    "def get_tpu_memory_info():\n",
    "    \"\"\"Get memory information from TPU device if available\"\"\"\n",
    "    if TPU_AVAILABLE:\n",
    "        try:\n",
    "            # This is just for diagnostic purposes\n",
    "            import subprocess\n",
    "            result = subprocess.run(['python3', '-c', 'import torch_xla; print(torch_xla._XLAC._xla_get_memory_info(torch_xla._XLAC._xla_get_default_device()))'],\n",
    "                                   stdout=subprocess.PIPE, text=True)\n",
    "            print(f\"TPU Memory Info: {result.stdout}\")\n",
    "        except:\n",
    "            print(\"Could not get detailed TPU memory info\")\n",
    "\n",
    "# Display memory information\n",
    "get_tpu_memory_info()\n",
    "\n",
    "n_envs = 1\n",
    "batch_size = 64\n",
    "n_steps = 2048\n",
    "\n",
    "# Policy kwargs for MLP (state-based observations)\n",
    "policy_kwargs = {\n",
    "    \"net_arch\": [256, 256]  # Simpler MLP architecture\n",
    "}\n",
    "\n",
    "# Create trainer\n",
    "training = Train(\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    difficulty=\"medium\",\n",
    "    n_envs=n_envs,\n",
    "    load_model=None,  # Start fresh with state-based env\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")\n",
    "\n",
    "# Choose whether to do hyperparameter optimization or direct training\n",
    "do_optimization = True\n",
    "\n",
    "if do_optimization:\n",
    "    # Force TPU memory cleanup before starting\n",
    "    if TPU_AVAILABLE:\n",
    "        gc.collect()\n",
    "        xm.mark_step()\n",
    "\n",
    "    n_trials = 10\n",
    "    best_trial = training.optuna_parameter_tuning(n_trials=n_trials)\n",
    "    print(f\"Best parameters found: {best_trial.params}\")\n",
    "else:\n",
    "    # Run training with memory-optimized settings\n",
    "    # Use fewer total timesteps for TPU to avoid memory issues\n",
    "    total_timesteps = 500000\n",
    "\n",
    "    model = training.train_ppo(\n",
    "        total_timesteps=total_timesteps,\n",
    "        eval_episodes=3,  # Fewer eval episodes on TPU\n",
    "        save_freq=5000,\n",
    "        eval_freq=5000\n",
    "    )\n",
    "\n",
    "    # Force memory cleanup after training\n",
    "    if TPU_AVAILABLE:\n",
    "        del model\n",
    "        gc.collect()\n",
    "        xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD0qswxU0IuD"
   },
   "outputs": [],
   "source": [
    "# Copy the best model to a stable location\n",
    "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
    "\n",
    "# Optional: Monitor TPU usage\n",
    "if TPU_AVAILABLE:\n",
    "    !sudo lsof -w /dev/accel0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLCe2GS6Kb8K"
   },
   "outputs": [],
   "source": [
    "# Load a saved model and continue training or evaluate\n",
    "model_path = \"/content/models/best_model.zip\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Loading model from {model_path} for evaluation\")\n",
    "\n",
    "    # Create trainer with the saved model\n",
    "    eval_trainer = Train(\n",
    "        n_steps=1024,\n",
    "        batch_size=batch_size,\n",
    "        difficulty=\"medium\",\n",
    "        n_envs=1,  # Use 1 env for evaluation\n",
    "        load_model=model_path\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_trainer.evaluate(\n",
    "        model_path=model_path,\n",
    "        n_episodes=5,\n",
    "        difficulty=\"medium\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model not found at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKnme-c5KPlc"
   },
   "source": [
    "# --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4h3jBARKPld"
   },
   "outputs": [],
   "source": [
    "# run_standalone_game(difficulty=\"medium\")\n",
    "# test_gym_env(difficulty=\"medium\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hhEqO-xFu4AI",
    "cnA8wZtosmeN",
    "v-8d5fKltI62",
    "gjobL-nozI81"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
