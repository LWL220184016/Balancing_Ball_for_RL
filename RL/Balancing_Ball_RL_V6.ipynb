{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V6 Update: in readme.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKvrHso92TYW"
      },
      "source": [
        "# Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.isfile('config.py'):\n",
        "    print(\"config.py file not found, changing directory to parent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "6db0a3a6-c0cc-467b-91fe-dceceea052a6"
      },
      "outputs": [],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "global TPU_AVAILABLE\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will disable TPU support\")\n",
        "\n",
        "# if no config.py file, should run the script in colab, so clone the repo\n",
        "if not os.path.isfile('config.py'):\n",
        "    !git clone https://github.com/LWL220184016/Balancing_Ball_for_RL.git\n",
        "%cd Balancing_Ball_for_RL/\n",
        "!ls\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy  # MLP policy instead of CNN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Go up one level from the current notebook's directory to the project root\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Now import from the project root\n",
        "from game.balancing_ball_game import BalancingBallGame\n",
        "from game.gym_env import BalancingBallEnv\n",
        "from RL.levels.level3.config import model_config, train_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "6f3efa36-ac17-4d45-ea64-7ba308eb48ab"
      },
      "outputs": [],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbqyvjZ1PZu",
        "outputId": "5c8fc715-a08c-4201-fd7a-31468934e6e4"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "class Train:\n",
        "    def __init__(self,\n",
        "                 model_cfg=None,\n",
        "                 train_cfg=None,\n",
        "                 n_envs=4,\n",
        "                 load_model=None,\n",
        "                 window_x=1000,\n",
        "                 window_y=600,\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(train_cfg.tensorboard_log, exist_ok=True)\n",
        "        os.makedirs(train_cfg.model_dir, exist_ok=True)\n",
        "        self.log_dir = train_cfg.tensorboard_log\n",
        "        self.model_dir = train_cfg.model_dir\n",
        "        self.n_envs = n_envs\n",
        "        self.obs_type = model_cfg.model_obs_type\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "\n",
        "        # Setup environments\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=train_cfg.render_mode, model_cfg=model_cfg),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        self.env = env\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env(\n",
        "            self.make_env(render_mode=train_cfg.render_mode, model_cfg=model_cfg),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=self.log_dir,\n",
        "            )\n",
        "        else:\n",
        "\n",
        "            print(\"obs type: \", self.obs_type)\n",
        "\n",
        "            # PPO for continuous action space with adversarial training\n",
        "            self.model = PPO(\n",
        "                env=self.env,\n",
        "                tensorboard_log=self.log_dir,\n",
        "                **model_cfg.model_param\n",
        "            )\n",
        "\n",
        "    def make_env(self, \n",
        "                 render_mode: str = None, \n",
        "                 model_cfg: str = None\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(\n",
        "                render_mode=render_mode,\n",
        "                model_cfg=model_cfg,\n",
        "                window_x=self.window_x,\n",
        "                window_y=self.window_y,\n",
        "            )\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self,\n",
        "                  total_timesteps=None,\n",
        "                  save_freq=None,\n",
        "                  eval_freq=None,\n",
        "                  eval_episodes=None,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball_\" + str(self.obs_type),\n",
        "        )\n",
        "\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=eval_freq // self.n_envs,\n",
        "            n_eval_episodes=eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=[checkpoint_callback, eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final_\" + str(self.obs_type))\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpEVcsjfs45Q"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4gwCvLVs45Q"
      },
      "outputs": [],
      "source": [
        "# # TODO 需要修改\n",
        "\n",
        "# class Optuna_optimize:\n",
        "#     def __init__(self, obs_type: str = None, level: int = None):\n",
        "#         self.obs_type = obs_type\n",
        "#         self.env = make_vec_env(\n",
        "#             self.make_env(render_mode=\"rgb_array\", obs_type=self.obs_type),\n",
        "#             n_envs=1\n",
        "#         )\n",
        "#         self.level=level\n",
        "\n",
        "#     def make_env(self, \n",
        "#                  render_mode: str = None, \n",
        "#                  model_cfg: str = None\n",
        "#                 ):\n",
        "#         \"\"\"\n",
        "#         Create and return an environment function to be used with VecEnv\n",
        "#         \"\"\"\n",
        "#         def _init():\n",
        "#             env = BalancingBallEnv(\n",
        "#                 render_mode=render_mode,\n",
        "#                 model_cfg=model_cfg,\n",
        "#                 window_x=self.window_x,\n",
        "#                 window_y=self.window_y,\n",
        "#             )\n",
        "#             return env\n",
        "#         return _init\n",
        "\n",
        "#     def optuna_parameter_tuning(self, n_trials):\n",
        "#         print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "#         pruner = optuna.pruners.HyperbandPruner(\n",
        "#             min_resource=100,        # 最小资源量\n",
        "#             max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "#             reduction_factor=3     # 折减因子 (eta)\n",
        "#         )\n",
        "\n",
        "#         # 建立 study 物件，並指定剪枝器\n",
        "#         study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "#         # 執行優化\n",
        "#         try:\n",
        "#             study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "#             # 分析結果\n",
        "#             print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "#             print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "#             import pandas as pd\n",
        "#             df = study.trials_dataframe()\n",
        "#             print(df.head())\n",
        "#         finally:\n",
        "#             self.env.close()\n",
        "#             del self.env\n",
        "\n",
        "\n",
        "#     def objective(self, trial):\n",
        "#         import gc\n",
        "\n",
        "#         # 1. 建議超參數 - Adjusted for continuous action space\n",
        "#         learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "#         gamma = trial.suggest_float('gamma', 0.95, 0.999)\n",
        "#         clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "#         gae_lambda = trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
        "#         ent_coef = trial.suggest_float('ent_coef', 0.005, 0.02)  # Lower for continuous actions\n",
        "#         vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "#         # features_dim = trial.suggest_categorical('features_dim', [128, 256, 512])\n",
        "\n",
        "#         policy_kwargs = {\n",
        "#             # \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "#             \"net_arch\": [256, 256],  # Architecture for continuous actions\n",
        "#         }\n",
        "\n",
        "#         n_steps=2048\n",
        "#         batch_size=64\n",
        "#         n_epochs=10\n",
        "#         max_grad_norm=0.5\n",
        "\n",
        "#         policy = ActorCriticCnnPolicy if self.obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "#         print(\"obs type: \", self.obs_type)\n",
        "#         print(\"policy: \", policy)\n",
        "\n",
        "#         # 3. 建立模型 - PPO for continuous action space\n",
        "#         model = PPO(\n",
        "#                 policy=policy,\n",
        "#                 env=self.env,\n",
        "#                 learning_rate=learning_rate,\n",
        "#                 n_steps=n_steps,\n",
        "#                 batch_size=batch_size,\n",
        "#                 n_epochs=n_epochs,\n",
        "#                 gamma=gamma,\n",
        "#                 clip_range=clip_range,\n",
        "#                 gae_lambda=gae_lambda,\n",
        "#                 ent_coef=ent_coef,\n",
        "#                 vf_coef=vf_coef,\n",
        "#                 max_grad_norm=max_grad_norm,\n",
        "#                 tensorboard_log=None,\n",
        "#                 policy_kwargs=policy_kwargs,\n",
        "#                 verbose=0,\n",
        "#             )\n",
        "\n",
        "#         try:\n",
        "#             # 4. 訓練模型\n",
        "#             model.learn(total_timesteps=50000)  # Increased timesteps for adversarial training\n",
        "#             # 5. 評估模型\n",
        "#             mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "#         finally:\n",
        "#             # Always cleanup\n",
        "#             del model\n",
        "#             gc.collect()\n",
        "\n",
        "#             if TPU_AVAILABLE:\n",
        "#                 import torch_xla.core.xla_model as xm\n",
        "#                 xm.mark_step()\n",
        "\n",
        "#         return mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "Ndr9hGp2CZzF",
        "outputId": "a3786351-5b69-4033-c3b7-a920c97d1e22"
      },
      "outputs": [],
      "source": [
        "n_envs = 1\n",
        "batch_size = 64\n",
        "n_steps = 2048\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = False\n",
        "\n",
        "model_cfg = model_config()\n",
        "train_cfg = train_config()\n",
        "window_x = 1000\n",
        "window_y = 1000\n",
        "\n",
        "if do_optimization: # game_screen, state_based\n",
        "    # optuna_optimizer = Optuna_optimize(obs_type=model_cfg.model_obs_type, level=1)\n",
        "    # n_trials = 10\n",
        "    # best_trial = optuna_optimizer.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    # print(f\"best_trial found: {best_trial}\")\n",
        "\n",
        "    pass\n",
        "else:\n",
        "    # Create trainer for adversarial training\n",
        "    training = Train(\n",
        "        model_cfg=model_cfg,\n",
        "        train_cfg=train_cfg,\n",
        "        n_envs=n_envs,\n",
        "        load_model=None,  # Start fresh for adversarial training\n",
        "        window_x=window_x,\n",
        "        window_y=window_y,\n",
        "    )\n",
        "\n",
        "    model = training.train_ppo(\n",
        "        total_timesteps=5000000,\n",
        "        save_freq=10000,\n",
        "        eval_freq=10000,\n",
        "        eval_episodes=5,\n",
        "    )\n",
        "\n",
        "    print(\"Adversarial training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "outputs": [],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "outputs": [],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        model_cfg=model_config(),\n",
        "        train_cfg=train_config(),\n",
        "        n_envs=1,  # Use 1 env for evaluation\n",
        "        window_x=window_x,\n",
        "        window_y=window_y,\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4h3jBARKPld"
      },
      "outputs": [],
      "source": [
        "from game.test import run_standalone_game, test_gym_env\n",
        "# Test the adversarial training environment\n",
        "run_standalone_game(render_mode=\"rgb_array_and_human_in_colab\", window_x=1000, window_y=600, level=3)\n",
        "# test_gym_env(level=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCFm3AoWs45R"
      },
      "outputs": [],
      "source": [
        "# Example of creating the environment with continuous action space for adversarial training\n",
        "env = BalancingBallEnv(\n",
        "    render_mode=train_cfg.render_mode,\n",
        "    fps=model_cfg.fps,\n",
        "    obs_type=model_cfg.model_obs_type,\n",
        "    image_size=model_cfg.image_size,\n",
        "    level=model_cfg.level,  # Level 3 for adversarial training\n",
        ")\n",
        "\n",
        "# Reset environment to get initial observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Print observation and action space info\n",
        "print(f\"Observation shape: {obs.shape}\")  # Should be (84, 84, 3) for grayscale with 3 stacked frames\n",
        "print(f\"Action space: {env.action_space}\")  # Should be Box(low=-1, high=1, shape=(2,))\n",
        "print(f\"Action space shape: {env.action_space.shape}\")  # Should be (2,) for two players\n",
        "\n",
        "# Test a random continuous action\n",
        "action = env.action_space.sample()\n",
        "print(f\"Sample action: {action}\")  # Should be array of 2 values between -1 and 1\n",
        "\n",
        "# Take a step\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Step result - Reward: {reward}, Individual rewards: {info.get('individual_rewards', [])}\")\n",
        "\n",
        "# Display a sample observation (first frame only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(obs[:,:,0], cmap='gray')\n",
        "plt.title(\"Adversarial Training - Grayscale Observation\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sekiro_rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
