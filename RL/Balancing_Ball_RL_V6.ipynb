{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V6 Update: in readme.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "1ae15a97-877d-4c9f-f004-d074d7891803"
      },
      "outputs": [],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will attempt to install\")\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install pygame-ce pymunk stable-baselines3 stable-baselines3[extra] shimmy>=2.0 optuna\n",
        "!pip install -q cloud-tpu-client\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "30585815-a413-4320-f39c-65eeaa5808eb"
      },
      "outputs": [],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbqyvjZ1PZu",
        "outputId": "3eec2b11-b4a7-4b18-d14e-83cef4b852ba"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from game.balancing_ball_game import BalancingBallGame\n",
        "from game.gym_env import BalancingBallEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgvyLDYKPlb"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebr3cvEiKPlc"
      },
      "outputs": [],
      "source": [
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "def run_standalone_game(render_mode=\"human\", difficulty=\"medium\", capture_per_second=3, window_x=1000, window_y=600, level=3):\n",
        "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "\n",
        "    platform_shape = \"circle\"\n",
        "    platform_proportion = 0.333\n",
        "\n",
        "    game = BalancingBallGame(\n",
        "        render_mode = render_mode,\n",
        "        difficulty = difficulty,\n",
        "        window_x = window_x,\n",
        "        window_y = window_y,\n",
        "        platform_shape = platform_shape,\n",
        "        platform_proportion = platform_proportion,\n",
        "        level = level,\n",
        "        fps = 30,\n",
        "        capture_per_second = 3,\n",
        "    )\n",
        "\n",
        "    game.run_standalone()\n",
        "\n",
        "def test_gym_env(episodes=3, difficulty=\"medium\"):\n",
        "    \"\"\"Test the OpenAI Gym environment with continuous actions\"\"\"\n",
        "    import time\n",
        "    # from gym_env import BalancingBallEnv\n",
        "\n",
        "    fps = 30\n",
        "    env = BalancingBallEnv(\n",
        "        render_mode=\"rgb_array_and_human_in_colab\",\n",
        "        difficulty=difficulty,\n",
        "        fps=fps,\n",
        "        level=3,  # Use level 3 for adversarial training\n",
        "        num_players=2,\n",
        "    )\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample continuous actions for both players\n",
        "            action = env.action_space.sample()  # Returns array of shape (2,) with values in [-1, 1]\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            # Render\n",
        "            env.render()\n",
        "\n",
        "            # Print some info\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step {step}: Action: {action}, Reward: {reward:.2f}, Individual Rewards: {info.get('individual_rewards', [])}\")\n",
        "\n",
        "        winner = info.get('winner', None)\n",
        "        winner_text = f\"Winner: Player {winner + 1}\" if winner is not None else \"Draw\"\n",
        "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}, {winner_text}\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy  # MLP policy instead of CNN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "class Train:\n",
        "    def __init__(self,\n",
        "                 learning_rate=0.0003,\n",
        "                 n_steps=2048,\n",
        "                 batch_size=64,\n",
        "                 n_epochs=10,\n",
        "                 gamma=0.99,\n",
        "                 gae_lambda=0.95,\n",
        "                 ent_coef=0.01,\n",
        "                 vf_coef=0.5,\n",
        "                 max_grad_norm=0.5,\n",
        "                 policy_kwargs=None,\n",
        "                 n_envs=4,\n",
        "                 difficulty=\"medium\",\n",
        "                 level=3,  # Default to level 3 for adversarial training\n",
        "                 load_model=None,\n",
        "                 log_dir=\"./logs/\",\n",
        "                 model_dir=\"./models/\",\n",
        "                 obs_type=\"game_screen\",\n",
        "                 num_players=2,  # Number of players for adversarial training\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        self.log_dir = log_dir\n",
        "        self.model_dir = model_dir\n",
        "        self.n_envs = n_envs\n",
        "        self.obs_type = obs_type\n",
        "        self.level = level\n",
        "        self.num_players = num_players\n",
        "\n",
        "        # Setup environments\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=obs_type, num_players=num_players),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        self.env = env\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=obs_type, num_players=num_players),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=log_dir,\n",
        "            )\n",
        "        else:\n",
        "            # 優化的超參數，特別針對對抗訓練\n",
        "            hyper_param = {\n",
        "                'learning_rate': 0.0001,  # 降低學習率以提高穩定性\n",
        "                'gamma': 0.995,  # 提高折扣因子以重視長期獎勵\n",
        "                'clip_range': 0.15,  # 降低裁切範圍以提高穩定性\n",
        "                'gae_lambda': 0.98,  # 提高GAE lambda\n",
        "                'ent_coef': 0.02,  # 提高熵係數以增加探索\n",
        "                'vf_coef': 0.5,\n",
        "            }\n",
        "\n",
        "            policy_kwargs = {\n",
        "                \"features_extractor_kwargs\": {\"features_dim\": 512},\n",
        "                \"net_arch\": [512, 512, 256],  # 增加網絡深度以處理複雜策略\n",
        "                \"activation_fn\": torch.nn.ReLU,\n",
        "            }\n",
        "\n",
        "            policy = ActorCriticCnnPolicy if obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "            print(\"obs type: \", self.obs_type)\n",
        "            print(\"policy: \", policy)\n",
        "            print(\"num_players: \", self.num_players)\n",
        "\n",
        "            # PPO for continuous action space with adversarial training\n",
        "            self.model = PPO(\n",
        "                policy=policy,\n",
        "                env=self.env,\n",
        "                learning_rate=hyper_param[\"learning_rate\"],\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=hyper_param[\"gamma\"],\n",
        "                clip_range=hyper_param[\"clip_range\"],\n",
        "                gae_lambda=hyper_param[\"gae_lambda\"],\n",
        "                ent_coef=hyper_param[\"ent_coef\"],\n",
        "                vf_coef=hyper_param[\"vf_coef\"],\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=log_dir,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=1,\n",
        "            )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\", num_players=2):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(\n",
        "                render_mode=render_mode,\n",
        "                difficulty=difficulty,\n",
        "                level=self.level,\n",
        "                obs_type=obs_type,\n",
        "                num_players=num_players\n",
        "            )\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self,\n",
        "                  total_timesteps=1000000,\n",
        "                  save_freq=10000,\n",
        "                  eval_freq=10000,\n",
        "                  eval_episodes=5,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball_\" + str(self.obs_type),\n",
        "        )\n",
        "\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=eval_freq // self.n_envs,\n",
        "            n_eval_episodes=eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=[checkpoint_callback, eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final_\" + str(self.obs_type))\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10, difficulty=\"medium\"):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         difficulty=args.difficulty,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#             difficulty=args.difficulty\n",
        "#         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpEVcsjfs45Q"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4gwCvLVs45Q"
      },
      "outputs": [],
      "source": [
        "class Optuna_optimize:\n",
        "    def __init__(self, obs_type=\"game_screen\", num_players=2):\n",
        "        self.obs_type = obs_type\n",
        "        self.num_players = num_players\n",
        "        self.env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=self.obs_type, num_players=num_players),\n",
        "            n_envs=1\n",
        "        )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=\"game_screen\", num_players=2):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(\n",
        "                render_mode=render_mode,\n",
        "                difficulty=difficulty,\n",
        "                level=3,  # Level 3 for adversarial training\n",
        "                obs_type=obs_type,\n",
        "                num_players=num_players\n",
        "            )\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def optuna_parameter_tuning(self, n_trials):\n",
        "        print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "        pruner = optuna.pruners.HyperbandPruner(\n",
        "            min_resource=100,        # 最小资源量\n",
        "            max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "            reduction_factor=3     # 折减因子 (eta)\n",
        "        )\n",
        "\n",
        "        # 建立 study 物件，並指定剪枝器\n",
        "        study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "        # 執行優化\n",
        "        try:\n",
        "            study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "            # 分析結果\n",
        "            print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "            print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "            import pandas as pd\n",
        "            df = study.trials_dataframe()\n",
        "            print(df.head())\n",
        "        finally:\n",
        "            self.env.close()\n",
        "            del self.env\n",
        "\n",
        "\n",
        "    def objective(self, trial):\n",
        "        import gc\n",
        "\n",
        "        # 1. 建議超參數 - Adjusted for continuous action space\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 0.95, 0.999)\n",
        "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "        gae_lambda = trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
        "        ent_coef = trial.suggest_float('ent_coef', 0.005, 0.02)  # Lower for continuous actions\n",
        "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "        # features_dim = trial.suggest_categorical('features_dim', [128, 256, 512])\n",
        "\n",
        "        policy_kwargs = {\n",
        "            # \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "            \"net_arch\": [256, 256],  # Architecture for continuous actions\n",
        "        }\n",
        "\n",
        "        n_steps=2048\n",
        "        batch_size=64\n",
        "        n_epochs=10\n",
        "        max_grad_norm=0.5\n",
        "\n",
        "        policy = ActorCriticCnnPolicy if self.obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "        print(\"obs type: \", self.obs_type)\n",
        "        print(\"policy: \", policy)\n",
        "\n",
        "        # 3. 建立模型 - PPO for continuous action space\n",
        "        model = PPO(\n",
        "                policy=policy,\n",
        "                env=self.env,\n",
        "                learning_rate=learning_rate,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=gamma,\n",
        "                clip_range=clip_range,\n",
        "                gae_lambda=gae_lambda,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=None,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=0,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # 4. 訓練模型\n",
        "            model.learn(total_timesteps=50000)  # Increased timesteps for adversarial training\n",
        "            # 5. 評估模型\n",
        "            mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "        finally:\n",
        "            # Always cleanup\n",
        "            del model\n",
        "            gc.collect()\n",
        "\n",
        "            if TPU_AVAILABLE:\n",
        "                import torch_xla.core.xla_model as xm\n",
        "                xm.mark_step()\n",
        "\n",
        "        return mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndr9hGp2CZzF",
        "outputId": "269d2477-bb30-4567-e67f-f9ef617b483e"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Memory-optimized training setup\n",
        "def get_tpu_memory_info():\n",
        "    \"\"\"Get memory information from TPU device if available\"\"\"\n",
        "    pass\n",
        "\n",
        "# Display memory information\n",
        "get_tpu_memory_info()\n",
        "\n",
        "n_envs = 1\n",
        "batch_size = 64\n",
        "n_steps = 2048\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = True\n",
        "\n",
        "if do_optimization: # game_screen, state_based\n",
        "    optuna_optimizer = Optuna_optimize(obs_type=\"state_based\", num_players=2)\n",
        "    n_trials = 10\n",
        "    best_trial = optuna_optimizer.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    print(f\"best_trial found: {best_trial}\")\n",
        "else:\n",
        "    # Create trainer for adversarial training\n",
        "    training = Train(\n",
        "        n_steps=n_steps,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=n_envs,\n",
        "        level=3,  # Level 3 for adversarial training\n",
        "        load_model=None,  # Start fresh for adversarial training\n",
        "        obs_type='game_screen',\n",
        "        num_players=2,  # Two players for adversarial training\n",
        "    )\n",
        "\n",
        "    # Run training with continuous action space\n",
        "    total_timesteps = 1000000  # More timesteps for adversarial training\n",
        "\n",
        "    model = training.train_ppo(\n",
        "        total_timesteps=total_timesteps,\n",
        "        eval_episodes=5,\n",
        "        save_freq=10000,\n",
        "        eval_freq=10000\n",
        "    )\n",
        "\n",
        "    print(\"Adversarial training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "outputs": [],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "outputs": [],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        n_steps=1024,\n",
        "        batch_size=batch_size,\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=1  # Use 1 env for evaluation\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "        difficulty=\"medium\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4h3jBARKPld"
      },
      "outputs": [],
      "source": [
        "# Test the adversarial training environment\n",
        "run_standalone_game(render_mode=\"rgb_array_and_human_in_colab\", difficulty=\"medium\", window_x=1000, window_y=600, level=3)\n",
        "# test_gym_env(difficulty=\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCFm3AoWs45R"
      },
      "outputs": [],
      "source": [
        "# Example of creating the environment with continuous action space for adversarial training\n",
        "env = BalancingBallEnv(\n",
        "    render_mode=\"rgb_array\",\n",
        "    difficulty=\"medium\",\n",
        "    fps=30,\n",
        "    obs_type=\"game_screen\",\n",
        "    image_size=(84, 84),\n",
        "    level=3,  # Level 3 for adversarial training\n",
        "    num_players=2,  # Two players\n",
        ")\n",
        "\n",
        "# Reset environment to get initial observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Print observation and action space info\n",
        "print(f\"Observation shape: {obs.shape}\")  # Should be (84, 84, 3) for grayscale with 3 stacked frames\n",
        "print(f\"Action space: {env.action_space}\")  # Should be Box(low=-1, high=1, shape=(2,))\n",
        "print(f\"Action space shape: {env.action_space.shape}\")  # Should be (2,) for two players\n",
        "\n",
        "# Test a random continuous action\n",
        "action = env.action_space.sample()\n",
        "print(f\"Sample action: {action}\")  # Should be array of 2 values between -1 and 1\n",
        "\n",
        "# Take a step\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Step result - Reward: {reward}, Individual rewards: {info.get('individual_rewards', [])}\")\n",
        "\n",
        "# Display a sample observation (first frame only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(obs[:,:,0], cmap='gray')\n",
        "plt.title(\"Adversarial Training - Grayscale Observation\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
