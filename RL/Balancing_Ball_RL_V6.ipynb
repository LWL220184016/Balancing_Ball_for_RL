{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V6 Update: in readme.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKvrHso92TYW"
      },
      "source": [
        "# Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.isfile('config.py'):\n",
        "    print(\"config.py file not found, changing directory to parent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "6db0a3a6-c0cc-467b-91fe-dceceea052a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch XLA not found, will disable TPU support\n",
            "TPU support installed. Please restart the runtime now.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch_xla[tpu] (from versions: none)\n",
            "ERROR: No matching distribution found for torch_xla[tpu]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lotun\\miniconda3\\envs\\sekiro_rl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "global TPU_AVAILABLE\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will disable TPU support\")\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")\n",
        "\n",
        "\n",
        "# if no config.py file, should run the script in colab, so clone the repo\n",
        "if not os.path.isfile('config.py'):\n",
        "    !git clone https://github.com/LWL220184016/Balancing_Ball_for_RL.git\n",
        "%cd Balancing_Ball_for_RL/\n",
        "!ls\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy  # MLP policy instead of CNN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Go up one level from the current notebook's directory to the project root\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Now import from the project root\n",
        "from game.balancing_ball_game import BalancingBallGame\n",
        "from game.gym_env import BalancingBallEnv\n",
        "from RL.config import model_config, train_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "6f3efa36-ac17-4d45-ea64-7ba308eb48ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbqyvjZ1PZu",
        "outputId": "5c8fc715-a08c-4201-fd7a-31468934e6e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgvyLDYKPlb"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebr3cvEiKPlc"
      },
      "outputs": [],
      "source": [
        "# from balancing_ball_game import BalancingBallGame\n",
        "\n",
        "def run_standalone_game(render_mode=\"human\", difficulty=None, capture_per_second=None, window_x=1000, window_y=600, level=None):\n",
        "    \"\"\"Run the game in standalone mode with visual display\"\"\"\n",
        "\n",
        "\n",
        "    game = BalancingBallGame(\n",
        "        render_mode = render_mode,\n",
        "        difficulty = difficulty,\n",
        "        window_x = window_x,\n",
        "        window_y = window_y,\n",
        "        level = level,\n",
        "        fps = 30,\n",
        "        capture_per_second = capture_per_second,\n",
        "    )\n",
        "\n",
        "    game.run_standalone()\n",
        "\n",
        "def test_gym_env(episodes=3, difficulty=None, level=None):\n",
        "    \"\"\"Test the OpenAI Gym environment with continuous actions\"\"\"\n",
        "    import time\n",
        "    # from gym_env import BalancingBallEnv\n",
        "\n",
        "    fps = 30\n",
        "    env = BalancingBallEnv(\n",
        "        render_mode=\"rgb_array_and_human_in_colab\",\n",
        "        difficulty=difficulty,\n",
        "        fps=fps,\n",
        "        level=level,  # Use level 3 for adversarial training\n",
        "    )\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample continuous actions for both players\n",
        "            action = env.action_space.sample()  # Returns array of shape (2,) with values in [-1, 1]\n",
        "\n",
        "            # Take step\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            # Render\n",
        "            env.render()\n",
        "\n",
        "            # Print some info\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step {step}: Action: {action}, Reward: {reward:.2f}, Individual Rewards: {info.get('individual_rewards', [])}\")\n",
        "\n",
        "        winner = info.get('winner', None)\n",
        "        winner_text = f\"Winner: Player {winner + 1}\" if winner is not None else \"Draw\"\n",
        "        print(f\"Episode {episode+1}: Steps: {step}, Total Reward: {total_reward:.2f}, {winner_text}\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "class Train:\n",
        "    def __init__(self,\n",
        "                 model_cfg=None,\n",
        "                 train_cfg=None,\n",
        "                 n_envs=4,\n",
        "                 difficulty=None,\n",
        "                 level=None,  # Default to level 3 for adversarial training\n",
        "                 load_model=None,\n",
        "                 window_x=1000,\n",
        "                 window_y=600,\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(train_cfg.tensorboard_log, exist_ok=True)\n",
        "        os.makedirs(train_cfg.model_dir, exist_ok=True)\n",
        "        self.log_dir = train_cfg.tensorboard_log\n",
        "        self.model_dir = train_cfg.model_dir\n",
        "        self.n_envs = n_envs\n",
        "        self.obs_type = model_cfg.model_obs_type\n",
        "        self.level = level\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "\n",
        "        # Setup environments\n",
        "        env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=self.obs_type),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        self.env = env\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=difficulty, obs_type=self.obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=self.log_dir,\n",
        "            )\n",
        "        else:\n",
        "\n",
        "            print(\"obs type: \", self.obs_type)\n",
        "\n",
        "            # PPO for continuous action space with adversarial training\n",
        "            self.model = PPO(\n",
        "                env=self.env,\n",
        "                tensorboard_log=self.log_dir,\n",
        "                **model_cfg.model_param\n",
        "            )\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=None, obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(\n",
        "                render_mode=render_mode,\n",
        "                difficulty=difficulty,\n",
        "                level=self.level,\n",
        "                obs_type=obs_type,\n",
        "                window_x=self.window_x,\n",
        "                window_y=self.window_y,\n",
        "            )\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self,\n",
        "                  total_timesteps=1000000,\n",
        "                  save_freq=10000,\n",
        "                  eval_freq=10000,\n",
        "                  eval_episodes=5,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "        # Setup callbacks\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball_\" + str(self.obs_type),\n",
        "        )\n",
        "\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=eval_freq // self.n_envs,\n",
        "            n_eval_episodes=eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=[checkpoint_callback, eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final_\" + str(self.obs_type))\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "            difficulty: Game difficulty level\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         difficulty=args.difficulty,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#             difficulty=args.difficulty\n",
        "#         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpEVcsjfs45Q"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B4gwCvLVs45Q"
      },
      "outputs": [],
      "source": [
        "class Optuna_optimize:\n",
        "    def __init__(self, obs_type=\"game_screen\", level=None):\n",
        "        self.obs_type = obs_type\n",
        "        self.env = make_vec_env(\n",
        "            self.make_env(render_mode=\"rgb_array\", difficulty=\"medium\", obs_type=self.obs_type),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.level=level\n",
        "\n",
        "    def make_env(self, render_mode=\"rgb_array\", difficulty=None, obs_type=\"game_screen\"):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(\n",
        "                render_mode=render_mode,\n",
        "                difficulty=difficulty,\n",
        "                level=self.level,\n",
        "                obs_type=obs_type,\n",
        "            )\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def optuna_parameter_tuning(self, n_trials):\n",
        "        print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "        pruner = optuna.pruners.HyperbandPruner(\n",
        "            min_resource=100,        # 最小资源量\n",
        "            max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "            reduction_factor=3     # 折减因子 (eta)\n",
        "        )\n",
        "\n",
        "        # 建立 study 物件，並指定剪枝器\n",
        "        study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "        # 執行優化\n",
        "        try:\n",
        "            study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "            # 分析結果\n",
        "            print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "            print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "            import pandas as pd\n",
        "            df = study.trials_dataframe()\n",
        "            print(df.head())\n",
        "        finally:\n",
        "            self.env.close()\n",
        "            del self.env\n",
        "\n",
        "\n",
        "    def objective(self, trial):\n",
        "        import gc\n",
        "\n",
        "        # 1. 建議超參數 - Adjusted for continuous action space\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "        gamma = trial.suggest_float('gamma', 0.95, 0.999)\n",
        "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "        gae_lambda = trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
        "        ent_coef = trial.suggest_float('ent_coef', 0.005, 0.02)  # Lower for continuous actions\n",
        "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "        # features_dim = trial.suggest_categorical('features_dim', [128, 256, 512])\n",
        "\n",
        "        policy_kwargs = {\n",
        "            # \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "            \"net_arch\": [256, 256],  # Architecture for continuous actions\n",
        "        }\n",
        "\n",
        "        n_steps=2048\n",
        "        batch_size=64\n",
        "        n_epochs=10\n",
        "        max_grad_norm=0.5\n",
        "\n",
        "        policy = ActorCriticCnnPolicy if self.obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "        print(\"obs type: \", self.obs_type)\n",
        "        print(\"policy: \", policy)\n",
        "\n",
        "        # 3. 建立模型 - PPO for continuous action space\n",
        "        model = PPO(\n",
        "                policy=policy,\n",
        "                env=self.env,\n",
        "                learning_rate=learning_rate,\n",
        "                n_steps=n_steps,\n",
        "                batch_size=batch_size,\n",
        "                n_epochs=n_epochs,\n",
        "                gamma=gamma,\n",
        "                clip_range=clip_range,\n",
        "                gae_lambda=gae_lambda,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                tensorboard_log=None,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=0,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # 4. 訓練模型\n",
        "            model.learn(total_timesteps=50000)  # Increased timesteps for adversarial training\n",
        "            # 5. 評估模型\n",
        "            mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "        finally:\n",
        "            # Always cleanup\n",
        "            del model\n",
        "            gc.collect()\n",
        "\n",
        "            if TPU_AVAILABLE:\n",
        "                import torch_xla.core.xla_model as xm\n",
        "                xm.mark_step()\n",
        "\n",
        "        return mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "Ndr9hGp2CZzF",
        "outputId": "a3786351-5b69-4033-c3b7-a920c97d1e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the json memory file\n",
            "self.x_axis_max_reward_rate:  0.0547945205479452\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "high is out of bounds for int32",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_trial found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_trial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Create trainer for adversarial training\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Level 3 for adversarial training\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start fresh for adversarial training\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgame_screen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Run training with continuous action space\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     total_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000\u001b[39m  \u001b[38;5;66;03m# More timesteps for adversarial training\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[1;34m(self, model_cfg, train_cfg, n_envs, difficulty, level, load_model, obs_type, window_x, window_y)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_y \u001b[38;5;241m=\u001b[39m window_y\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Setup environments\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mmake_vec_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifficulty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_envs\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Setup evaluation environment\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\lotun\\miniconda3\\envs\\sekiro_rl\\lib\\site-packages\\stable_baselines3\\common\\env_util.py:127\u001b[0m, in \u001b[0;36mmake_vec_env\u001b[1;34m(env_id, n_envs, seed, start_index, monitor_dir, wrapper_class, env_kwargs, vec_env_cls, vec_env_kwargs, monitor_kwargs, wrapper_kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m vec_env_cls([make_env(i \u001b[38;5;241m+\u001b[39m start_index) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_envs)], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvec_env_kwargs)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Prepare the seeds for the first reset\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vec_env\n",
            "File \u001b[1;32mc:\\Users\\lotun\\miniconda3\\envs\\sekiro_rl\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:281\u001b[0m, in \u001b[0;36mVecEnv.seed\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03mSets the random seeds for all environments, based on a given seed.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mEach individual environment will still get its own seed, by incrementing the given seed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    Note that all list elements may be None, if the env does not return anything when being seeded.\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# To ensure that subprocesses have different seeds,\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# we still populate the seed variable when no argument is passed\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     seed \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m [seed \u001b[38;5;241m+\u001b[39m idx \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs)]\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds\n",
            "File \u001b[1;32mmtrand.pyx:763\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_bounded_integers.pyx:1336\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int32\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: high is out of bounds for int32"
          ]
        }
      ],
      "source": [
        "n_envs = 1\n",
        "batch_size = 64\n",
        "n_steps = 2048\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = False\n",
        "\n",
        "model_cfg = model_config()\n",
        "train_cfg = train_config()\n",
        "window_x = 250\n",
        "window_y = 150\n",
        "\n",
        "if do_optimization: # game_screen, state_based\n",
        "    optuna_optimizer = Optuna_optimize(obs_type=model_cfg.model_obs_type, level=1)\n",
        "    n_trials = 10\n",
        "    best_trial = optuna_optimizer.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    print(f\"best_trial found: {best_trial}\")\n",
        "else:\n",
        "    # Create trainer for adversarial training\n",
        "    training = Train(\n",
        "        model_cfg=model_cfg,\n",
        "        train_cfg=train_cfg,\n",
        "        n_envs=n_envs,\n",
        "        difficulty=\"medium\",\n",
        "        level=1, \n",
        "        load_model=None,  # Start fresh for adversarial training\n",
        "        window_x=window_x,\n",
        "        window_y=window_y,\n",
        "    )\n",
        "\n",
        "    # Run training with continuous action space\n",
        "    total_timesteps = 1000000  # More timesteps for adversarial training\n",
        "\n",
        "    model = training.train_ppo(\n",
        "        total_timesteps=total_timesteps,\n",
        "        eval_episodes=5,\n",
        "        save_freq=10000,\n",
        "        eval_freq=10000\n",
        "    )\n",
        "\n",
        "    print(\"Adversarial training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0qswxU0IuD"
      },
      "outputs": [],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLCe2GS6Kb8K"
      },
      "outputs": [],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        model_cfg=model_config(),\n",
        "        train_cfg=train_config(),\n",
        "        difficulty=\"medium\",\n",
        "        n_envs=1,  # Use 1 env for evaluation\n",
        "        window_x=window_x,\n",
        "        window_y=window_y,\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "        difficulty=\"medium\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4h3jBARKPld"
      },
      "outputs": [],
      "source": [
        "# Test the adversarial training environment\n",
        "run_standalone_game(render_mode=\"rgb_array_and_human_in_colab\", difficulty=\"medium\", window_x=1000, window_y=600, level=3)\n",
        "# test_gym_env(difficulty=\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCFm3AoWs45R"
      },
      "outputs": [],
      "source": [
        "# Example of creating the environment with continuous action space for adversarial training\n",
        "env = BalancingBallEnv(\n",
        "    render_mode=\"rgb_array\",\n",
        "    difficulty=\"medium\",\n",
        "    fps=30,\n",
        "    obs_type=\"game_screen\",\n",
        "    image_size=(84, 84),\n",
        "    level=3,  # Level 3 for adversarial training\n",
        ")\n",
        "\n",
        "# Reset environment to get initial observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Print observation and action space info\n",
        "print(f\"Observation shape: {obs.shape}\")  # Should be (84, 84, 3) for grayscale with 3 stacked frames\n",
        "print(f\"Action space: {env.action_space}\")  # Should be Box(low=-1, high=1, shape=(2,))\n",
        "print(f\"Action space shape: {env.action_space.shape}\")  # Should be (2,) for two players\n",
        "\n",
        "# Test a random continuous action\n",
        "action = env.action_space.sample()\n",
        "print(f\"Sample action: {action}\")  # Should be array of 2 values between -1 and 1\n",
        "\n",
        "# Take a step\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Step result - Reward: {reward}, Individual rewards: {info.get('individual_rewards', [])}\")\n",
        "\n",
        "# Display a sample observation (first frame only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(obs[:,:,0], cmap='gray')\n",
        "plt.title(\"Adversarial Training - Grayscale Observation\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sekiro_rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
