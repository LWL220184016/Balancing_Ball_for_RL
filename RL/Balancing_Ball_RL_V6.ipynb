{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUvQ0pDxH6C"
      },
      "source": [
        "V6 Update: in readme.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKvrHso92TYW"
      },
      "source": [
        "# Setup and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kf3EbjWPz7ex",
        "outputId": "1d67f2b2-ed3e-476c-da27-092408ce5452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py file not found, changing directory to parent\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.isfile('config.py'):\n",
        "    print(\"config.py file not found, changing directory to parent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4jlMyqiKPlZ",
        "outputId": "d39c9914-5a2a-480e-9dcf-2c9bcd88c162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n",
            "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch XLA already installed\n",
            "fatal: destination path 'Balancing_Ball_for_RL' already exists and is not an empty directory.\n",
            "/content/Balancing_Ball_for_RL\n",
            "'=2.0'\t\t      game\t     models\t requirements.txt\n",
            " capture\t      game_history   note.txt\t RL\n",
            " check_torch_gpu.py   logs\t     readme.md\t test\n",
            "Requirement already satisfied: stable-baselines3==2.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.7.0)\n",
            "Requirement already satisfied: torch==2.8.0+cu126 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: tensorboard==2.19.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.19.0)\n",
            "Requirement already satisfied: gymnasium==1.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: pymunk==7.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (7.1.0)\n",
            "Requirement already satisfied: pygame==2.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.6.1)\n",
            "Requirement already satisfied: pygame-ce==2.5.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.5.5)\n",
            "Requirement already satisfied: shimmy==2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2.0.0)\n",
            "Requirement already satisfied: optuna==4.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.7.0->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.7.0->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.7.0->-r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (3.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.19.0->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==1.2.0->-r requirements.txt (line 4)) (0.0.4)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.12/dist-packages (from pymunk==7.1.0->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (1.16.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna==4.5.0->-r requirements.txt (line 10)) (1.3.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.17.1->pymunk==7.1.0->-r requirements.txt (line 6)) (2.23)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna==4.5.0->-r requirements.txt (line 10)) (3.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0+cu126->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard==2.19.0->-r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.7.0->-r requirements.txt (line 1)) (2025.2)\n",
            "XLA device detected: xla:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-19969390.py:43: DeprecationWarning: Use torch_xla.device instead\n",
            "  device = xm.xla_device()\n"
          ]
        }
      ],
      "source": [
        "# Check for TPU availability and set it up\n",
        "import os\n",
        "\n",
        "# Check if TPU is available\n",
        "global TPU_AVAILABLE\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"PyTorch XLA already installed\")\n",
        "    TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"PyTorch XLA not found, will disable TPU support\")\n",
        "\n",
        "# if no config.py file, should run the script in colab, so clone the repo\n",
        "if not os.path.isfile('config.py'):\n",
        "    !git clone https://github.com/LWL220184016/Balancing_Ball_for_RL.git\n",
        "%cd Balancing_Ball_for_RL/\n",
        "!ls\n",
        "\n",
        "# Install necessary packages including PyTorch/XLA\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "if not TPU_AVAILABLE:\n",
        "    # Check what version of PyTorch we need\n",
        "    import torch\n",
        "    if torch.__version__.startswith('2'):\n",
        "        # For PyTorch 2.x\n",
        "        !pip install -q torch_xla[tpu]>=2.0\n",
        "    else:\n",
        "        # For PyTorch 1.x\n",
        "        !pip install -q torch_xla\n",
        "\n",
        "    # Restart runtime (required after installing PyTorch/XLA)\n",
        "    print(\"TPU support installed. Please restart the runtime now.\")\n",
        "    import IPython\n",
        "    IPython.display.display(IPython.display.HTML(\n",
        "        \"<script>google.colab.kernel.invokeFunction('notebook.Runtime.restartRuntime', [], {})</script>\"\n",
        "    ))\n",
        "else:\n",
        "    # Initialize TPU if available\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"XLA device detected: {device}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0aQwNXO3z7ey",
        "outputId": "7fcf1425-46eb-4cd9-ad9d-74366ca06df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import optuna\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy  # MLP policy instead of CNN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Go up one level from the current notebook's directory to the project root\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Now import from the project root\n",
        "from game.balancing_ball_game import BalancingBallGame\n",
        "from game.gym_env import BalancingBallEnv\n",
        "from RL.levels.level3.config import model_config, train_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W6IzM4yc3RQB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nILk-pwsMG",
        "outputId": "78b2f839-0b74-4362-e2e2-00951cfbd530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balancing_Ball_for_RL  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbqyvjZ1PZu",
        "outputId": "38ed0b75-e048-437f-a0d5-7dd6c9f0268e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/capture': No such file or directory\n",
            "rm: cannot remove '/content/game_history': No such file or directory\n",
            "rm: cannot remove '/content/logs': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/capture\n",
        "!rm -r /content/game_history\n",
        "!rm -r /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L1Mmc6vu0CX"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04sj1npeKPlc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rWkLYH5-KPlc"
      },
      "outputs": [],
      "source": [
        "class Train:\n",
        "    def __init__(self,\n",
        "                 model_cfg=None,\n",
        "                 train_cfg=None,\n",
        "                 n_envs=4,\n",
        "                 load_model=None,\n",
        "                 window_x=1000,\n",
        "                 window_y=600,\n",
        "                ):\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(train_cfg.tensorboard_log, exist_ok=True)\n",
        "        os.makedirs(train_cfg.model_dir, exist_ok=True)\n",
        "        self.model_cfg = model_cfg\n",
        "        self.train_cfg = train_cfg\n",
        "        self.log_dir = train_cfg.tensorboard_log\n",
        "        self.model_dir = train_cfg.model_dir\n",
        "        self.n_envs = n_envs\n",
        "        self.obs_type = model_cfg.model_obs_type\n",
        "        self.window_x = window_x\n",
        "        self.window_y = window_y\n",
        "\n",
        "        # Setup environments\n",
        "        env = make_vec_env(\n",
        "            self.make_env(),\n",
        "            n_envs=n_envs\n",
        "        )\n",
        "        self.env = env\n",
        "\n",
        "        # Setup evaluation environment\n",
        "        eval_env = make_vec_env(\n",
        "            self.make_env(),\n",
        "            n_envs=1\n",
        "        )\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        # Create the PPO model\n",
        "        if load_model:\n",
        "            print(f\"Loading model from {load_model}\")\n",
        "            self.model = PPO.load(\n",
        "                load_model,\n",
        "                env=self.env,\n",
        "                tensorboard_log=self.log_dir,\n",
        "            )\n",
        "        else:\n",
        "\n",
        "            print(\"obs type: \", self.obs_type)\n",
        "\n",
        "            # PPO for continuous action space with adversarial training\n",
        "            self.model = PPO(\n",
        "                env=self.env,\n",
        "                tensorboard_log=self.log_dir,\n",
        "                **model_cfg.model_param\n",
        "            )\n",
        "\n",
        "        # Setup callbacks\n",
        "        self.checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=train_cfg.save_freq // self.n_envs,  # Divide by n_envs as save_freq is in timesteps\n",
        "            save_path=self.model_dir,\n",
        "            name_prefix=\"ppo_balancing_ball_\" + str(self.obs_type),\n",
        "        )\n",
        "\n",
        "        self.eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=self.model_dir,\n",
        "            log_path=self.log_dir,\n",
        "            eval_freq=train_cfg.eval_freq // self.n_envs,\n",
        "            n_eval_episodes=train_cfg.eval_episodes,\n",
        "            deterministic=True,\n",
        "            render=False\n",
        "        )\n",
        "\n",
        "    def make_env(self):\n",
        "        \"\"\"\n",
        "        Create and return an environment function to be used with VecEnv\n",
        "        \"\"\"\n",
        "        def _init():\n",
        "            env = BalancingBallEnv(\n",
        "                render_mode=self.train_cfg.render_mode,\n",
        "                model_cfg=self.model_cfg,\n",
        "                train_cfg=self.train_cfg,\n",
        "                window_x=self.window_x,\n",
        "                window_y=self.window_y,\n",
        "            )\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    def train_ppo(self):\n",
        "        \"\"\"\n",
        "        Train a PPO agent to play the Balancing Ball game\n",
        "\n",
        "        Args:\n",
        "            total_timesteps: Total number of steps to train for\n",
        "            n_envs: Number of parallel environments\n",
        "            save_freq: How often to save checkpoints (in timesteps)\n",
        "            log_dir: Directory for tensorboard logs\n",
        "            model_dir: Directory to save models\n",
        "            eval_freq: How often to evaluate the model (in timesteps)\n",
        "            eval_episodes: Number of episodes to evaluate on\n",
        "            load_model: Path to model to load for continued training\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        self.model.learn(\n",
        "            total_timesteps=self.train_cfg.total_timesteps,\n",
        "            callback=[self.checkpoint_callback, self.eval_callback],\n",
        "        )\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save(f\"{self.model_dir}/ppo_balancing_ball_final_\" + str(self.obs_type))\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, model_path, n_episodes=10):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model\n",
        "            n_episodes: Number of episodes to evaluate on\n",
        "        \"\"\"\n",
        "        # Load the model\n",
        "        model = PPO.load(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            model,\n",
        "            self.env,\n",
        "            n_eval_episodes=n_episodes,\n",
        "            deterministic=True,\n",
        "            render=True\n",
        "        )\n",
        "\n",
        "        print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "# if args.mode == \"train\":\n",
        "#     train_ppo(\n",
        "#         total_timesteps=args.timesteps,\n",
        "#         n_envs=args.n_envs,\n",
        "#         load_model=args.load_model,\n",
        "#         eval_episodes=args.eval_episodes,\n",
        "#     )\n",
        "# else:\n",
        "#     if args.load_model is None:\n",
        "#         print(\"Error: Must provide --load_model for evaluation\")\n",
        "#     else:\n",
        "#         evaluate(\n",
        "#             model_path=args.load_model,\n",
        "#             n_episodes=args.eval_episodes,\n",
        "#         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpEVcsjfs45Q"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B4gwCvLVs45Q"
      },
      "outputs": [],
      "source": [
        "# # TODO 需要修改\n",
        "\n",
        "# class Optuna_optimize:\n",
        "#     def __init__(self, obs_type: str = None, level: int = None):\n",
        "#         self.obs_type = obs_type\n",
        "#         self.env = make_vec_env(\n",
        "#             self.make_env(render_mode=\"rgb_array\", obs_type=self.obs_type),\n",
        "#             n_envs=1\n",
        "#         )\n",
        "#         self.level=level\n",
        "\n",
        "#     def make_env(self,\n",
        "#                  render_mode: str = None,\n",
        "#                  model_cfg: str = None\n",
        "#                 ):\n",
        "#         \"\"\"\n",
        "#         Create and return an environment function to be used with VecEnv\n",
        "#         \"\"\"\n",
        "#         def _init():\n",
        "#             env = BalancingBallEnv(\n",
        "#                 render_mode=render_mode,\n",
        "#                 model_cfg=model_cfg,\n",
        "#                 window_x=self.window_x,\n",
        "#                 window_y=self.window_y,\n",
        "#             )\n",
        "#             return env\n",
        "#         return _init\n",
        "\n",
        "#     def optuna_parameter_tuning(self, n_trials):\n",
        "#         print(\"You are using optuna for automatic parameter tuning, it will create a new model\")\n",
        "\n",
        "#         pruner = optuna.pruners.HyperbandPruner(\n",
        "#             min_resource=100,        # 最小资源量\n",
        "#             max_resource='auto',   # 最大资源量 ('auto' 或 整数)\n",
        "#             reduction_factor=3     # 折减因子 (eta)\n",
        "#         )\n",
        "\n",
        "#         # 建立 study 物件，並指定剪枝器\n",
        "#         study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "#         # 執行優化\n",
        "#         try:\n",
        "#             study.optimize(self.objective, n_trials=n_trials)\n",
        "\n",
        "#             # 分析結果\n",
        "#             print(\"最佳試驗的超參數：\", study.best_trial.params)\n",
        "#             print(\"最佳試驗的平均回報：\", study.best_trial.value)\n",
        "\n",
        "#             import pandas as pd\n",
        "#             df = study.trials_dataframe()\n",
        "#             print(df.head())\n",
        "#         finally:\n",
        "#             self.env.close()\n",
        "#             del self.env\n",
        "\n",
        "\n",
        "#     def objective(self, trial):\n",
        "#         import gc\n",
        "\n",
        "#         # 1. 建議超參數 - Adjusted for continuous action space\n",
        "#         learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "#         gamma = trial.suggest_float('gamma', 0.95, 0.999)\n",
        "#         clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
        "#         gae_lambda = trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
        "#         ent_coef = trial.suggest_float('ent_coef', 0.005, 0.02)  # Lower for continuous actions\n",
        "#         vf_coef = trial.suggest_float('vf_coef', 0.1, 1)\n",
        "#         # features_dim = trial.suggest_categorical('features_dim', [128, 256, 512])\n",
        "\n",
        "#         policy_kwargs = {\n",
        "#             # \"features_extractor_kwargs\": {\"features_dim\": features_dim},\n",
        "#             \"net_arch\": [256, 256],  # Architecture for continuous actions\n",
        "#         }\n",
        "\n",
        "#         n_steps=2048\n",
        "#         batch_size=64\n",
        "#         n_epochs=10\n",
        "#         max_grad_norm=0.5\n",
        "\n",
        "#         policy = ActorCriticCnnPolicy if self.obs_type == \"game_screen\" else ActorCriticPolicy\n",
        "#         print(\"obs type: \", self.obs_type)\n",
        "#         print(\"policy: \", policy)\n",
        "\n",
        "#         # 3. 建立模型 - PPO for continuous action space\n",
        "#         model = PPO(\n",
        "#                 policy=policy,\n",
        "#                 env=self.env,\n",
        "#                 learning_rate=learning_rate,\n",
        "#                 n_steps=n_steps,\n",
        "#                 batch_size=batch_size,\n",
        "#                 n_epochs=n_epochs,\n",
        "#                 gamma=gamma,\n",
        "#                 clip_range=clip_range,\n",
        "#                 gae_lambda=gae_lambda,\n",
        "#                 ent_coef=ent_coef,\n",
        "#                 vf_coef=vf_coef,\n",
        "#                 max_grad_norm=max_grad_norm,\n",
        "#                 tensorboard_log=None,\n",
        "#                 policy_kwargs=policy_kwargs,\n",
        "#                 verbose=0,\n",
        "#             )\n",
        "\n",
        "#         try:\n",
        "#             # 4. 訓練模型\n",
        "#             model.learn(total_timesteps=50000)  # Increased timesteps for adversarial training\n",
        "#             # 5. 評估模型\n",
        "#             mean_reward = evaluate_policy(model, self.env, n_eval_episodes=10)[0]\n",
        "#         finally:\n",
        "#             # Always cleanup\n",
        "#             del model\n",
        "#             gc.collect()\n",
        "\n",
        "#             if TPU_AVAILABLE:\n",
        "#                 import torch_xla.core.xla_model as xm\n",
        "#                 xm.mark_step()\n",
        "\n",
        "#         return mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM-k9MuCmzc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndr9hGp2CZzF",
        "outputId": "5533bec9-2c5d-4c62-c084-67066bae8757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing BalancingBallEnv...\n",
            "Loading the json memory file\n",
            "Loading default level configurations...\n",
            "Using collision_type: {'player': 1000, 'platform': 2000, 'fallingRock': 3000}\n",
            "Using player_configs: [{'shape_type': 'circle', 'size': [0.03], 'shape_mass': 1, 'shape_friction': 100, 'shape_elasticity': 0.8, 'default_position': [0.5, 0.6], 'default_velocity': [0, 0], 'abilities': ['Collision'], 'health': 10, 'color': [255, 213, 79]}]\n",
            "Using level_configs: {'platform_configs': [{'shape_type': 'rectangle', 'size': [0.8, 0.04], 'shape_mass': 1, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': [0.5, 0.67], 'default_velocity': [0, 0], 'abilities': [], 'health': 'infinite', 'color': [235, 64, 52]}], 'falling_rock_configs': [{'shape_type': 'rectangle', 'size': [0.05, 0.05], 'shape_mass': 0.5, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': ['random', -0.001], 'default_velocity': ['random', 0], 'abilities': [], 'health': 'infinite', 'color': [100, 100, 100]}], 'entities_configs': {'quantity': {'fallingRock': 1}}, 'environment_configs': [{'gravity': [0, 4000], 'damping': 0.5}], 'reward': {'reward_per_step': 0.0, 'collision_falling_rock': 10, 'falling_rock_fall_outside_platform': 0.0, 'falling_rock_fall_on_platform': -2.5, 'falling_rock_near': 0.4, 'falling_rock_near_distance_threshold': 200, 'falling_rock_near_distance_reward_multiplier': 5, 'steps_limit_for_movement_penalty': 100, 'movement_penalty': -0.08, 'fail_penalty': 1.0, 'speed_reward_proportion': 0.0, 'opponent_fell_reward': 0.0, 'survival_bonus': 0.0}}\n",
            "Loading default ability configurations for the first time...\n",
            "Initialized Role with abilities: ['Collision']\n",
            "Created 1 players and 1 platforms.\n",
            "PlayerFallingRockCollisionReward c._terminates_function: True\n",
            "render_mode is not human or rgb_array, so no pygame setup.\n",
            "Initializing BalancingBallEnv...\n",
            "Loading the json memory file\n",
            "Loading default level configurations...\n",
            "Using collision_type: {'player': 1000, 'platform': 2000, 'fallingRock': 3000}\n",
            "Using player_configs: [{'shape_type': 'circle', 'size': [0.03], 'shape_mass': 1, 'shape_friction': 100, 'shape_elasticity': 0.8, 'default_position': [0.5, 0.6], 'default_velocity': [0, 0], 'abilities': ['Collision'], 'health': 10, 'color': [255, 213, 79]}]\n",
            "Using level_configs: {'platform_configs': [{'shape_type': 'rectangle', 'size': [0.8, 0.04], 'shape_mass': 1, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': [0.5, 0.67], 'default_velocity': [0, 0], 'abilities': [], 'health': 'infinite', 'color': [235, 64, 52]}], 'falling_rock_configs': [{'shape_type': 'rectangle', 'size': [0.05, 0.05], 'shape_mass': 0.5, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': ['random', -0.001], 'default_velocity': ['random', 0], 'abilities': [], 'health': 'infinite', 'color': [100, 100, 100]}], 'entities_configs': {'quantity': {'fallingRock': 1}}, 'environment_configs': [{'gravity': [0, 4000], 'damping': 0.5}], 'reward': {'reward_per_step': 0.0, 'collision_falling_rock': 10, 'falling_rock_fall_outside_platform': 0.0, 'falling_rock_fall_on_platform': -2.5, 'falling_rock_near': 0.4, 'falling_rock_near_distance_threshold': 200, 'falling_rock_near_distance_reward_multiplier': 5, 'steps_limit_for_movement_penalty': 100, 'movement_penalty': -0.08, 'fail_penalty': 1.0, 'speed_reward_proportion': 0.0, 'opponent_fell_reward': 0.0, 'survival_bonus': 0.0}}\n",
            "Initialized Role with abilities: ['Collision']\n",
            "Created 1 players and 1 platforms.\n",
            "PlayerFallingRockCollisionReward c._terminates_function: True\n",
            "render_mode is not human or rgb_array, so no pygame setup.\n",
            "Initializing BalancingBallEnv...\n",
            "Loading the json memory file\n",
            "Loading default level configurations...\n",
            "Using collision_type: {'player': 1000, 'platform': 2000, 'fallingRock': 3000}\n",
            "Using player_configs: [{'shape_type': 'circle', 'size': [0.03], 'shape_mass': 1, 'shape_friction': 100, 'shape_elasticity': 0.8, 'default_position': [0.5, 0.6], 'default_velocity': [0, 0], 'abilities': ['Collision'], 'health': 10, 'color': [255, 213, 79]}]\n",
            "Using level_configs: {'platform_configs': [{'shape_type': 'rectangle', 'size': [0.8, 0.04], 'shape_mass': 1, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': [0.5, 0.67], 'default_velocity': [0, 0], 'abilities': [], 'health': 'infinite', 'color': [235, 64, 52]}], 'falling_rock_configs': [{'shape_type': 'rectangle', 'size': [0.05, 0.05], 'shape_mass': 0.5, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': ['random', -0.001], 'default_velocity': ['random', 0], 'abilities': [], 'health': 'infinite', 'color': [100, 100, 100]}], 'entities_configs': {'quantity': {'fallingRock': 1}}, 'environment_configs': [{'gravity': [0, 4000], 'damping': 0.5}], 'reward': {'reward_per_step': 0.0, 'collision_falling_rock': 10, 'falling_rock_fall_outside_platform': 0.0, 'falling_rock_fall_on_platform': -2.5, 'falling_rock_near': 0.4, 'falling_rock_near_distance_threshold': 200, 'falling_rock_near_distance_reward_multiplier': 5, 'steps_limit_for_movement_penalty': 100, 'movement_penalty': -0.08, 'fail_penalty': 1.0, 'speed_reward_proportion': 0.0, 'opponent_fell_reward': 0.0, 'survival_bonus': 0.0}}\n",
            "Initialized Role with abilities: ['Collision']\n",
            "Created 1 players and 1 platforms.\n",
            "PlayerFallingRockCollisionReward c._terminates_function: True\n",
            "render_mode is not human or rgb_array, so no pygame setup.\n",
            "Initializing BalancingBallEnv...\n",
            "Loading the json memory file\n",
            "Loading default level configurations...\n",
            "Using collision_type: {'player': 1000, 'platform': 2000, 'fallingRock': 3000}\n",
            "Using player_configs: [{'shape_type': 'circle', 'size': [0.03], 'shape_mass': 1, 'shape_friction': 100, 'shape_elasticity': 0.8, 'default_position': [0.5, 0.6], 'default_velocity': [0, 0], 'abilities': ['Collision'], 'health': 10, 'color': [255, 213, 79]}]\n",
            "Using level_configs: {'platform_configs': [{'shape_type': 'rectangle', 'size': [0.8, 0.04], 'shape_mass': 1, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': [0.5, 0.67], 'default_velocity': [0, 0], 'abilities': [], 'health': 'infinite', 'color': [235, 64, 52]}], 'falling_rock_configs': [{'shape_type': 'rectangle', 'size': [0.05, 0.05], 'shape_mass': 0.5, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': ['random', -0.001], 'default_velocity': ['random', 0], 'abilities': [], 'health': 'infinite', 'color': [100, 100, 100]}], 'entities_configs': {'quantity': {'fallingRock': 1}}, 'environment_configs': [{'gravity': [0, 4000], 'damping': 0.5}], 'reward': {'reward_per_step': 0.0, 'collision_falling_rock': 10, 'falling_rock_fall_outside_platform': 0.0, 'falling_rock_fall_on_platform': -2.5, 'falling_rock_near': 0.4, 'falling_rock_near_distance_threshold': 200, 'falling_rock_near_distance_reward_multiplier': 5, 'steps_limit_for_movement_penalty': 100, 'movement_penalty': -0.08, 'fail_penalty': 1.0, 'speed_reward_proportion': 0.0, 'opponent_fell_reward': 0.0, 'survival_bonus': 0.0}}\n",
            "Initialized Role with abilities: ['Collision']\n",
            "Created 1 players and 1 platforms.\n",
            "PlayerFallingRockCollisionReward c._terminates_function: True\n",
            "render_mode is not human or rgb_array, so no pygame setup.\n",
            "Initializing BalancingBallEnv...\n",
            "Loading the json memory file\n",
            "Loading default level configurations...\n",
            "Using collision_type: {'player': 1000, 'platform': 2000, 'fallingRock': 3000}\n",
            "Using player_configs: [{'shape_type': 'circle', 'size': [0.03], 'shape_mass': 1, 'shape_friction': 100, 'shape_elasticity': 0.8, 'default_position': [0.5, 0.6], 'default_velocity': [0, 0], 'abilities': ['Collision'], 'health': 10, 'color': [255, 213, 79]}]\n",
            "Using level_configs: {'platform_configs': [{'shape_type': 'rectangle', 'size': [0.8, 0.04], 'shape_mass': 1, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': [0.5, 0.67], 'default_velocity': [0, 0], 'abilities': [], 'health': 'infinite', 'color': [235, 64, 52]}], 'falling_rock_configs': [{'shape_type': 'rectangle', 'size': [0.05, 0.05], 'shape_mass': 0.5, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': ['random', -0.001], 'default_velocity': ['random', 0], 'abilities': [], 'health': 'infinite', 'color': [100, 100, 100]}], 'entities_configs': {'quantity': {'fallingRock': 1}}, 'environment_configs': [{'gravity': [0, 4000], 'damping': 0.5}], 'reward': {'reward_per_step': 0.0, 'collision_falling_rock': 10, 'falling_rock_fall_outside_platform': 0.0, 'falling_rock_fall_on_platform': -2.5, 'falling_rock_near': 0.4, 'falling_rock_near_distance_threshold': 200, 'falling_rock_near_distance_reward_multiplier': 5, 'steps_limit_for_movement_penalty': 100, 'movement_penalty': -0.08, 'fail_penalty': 1.0, 'speed_reward_proportion': 0.0, 'opponent_fell_reward': 0.0, 'survival_bonus': 0.0}}\n",
            "Initialized Role with abilities: ['Collision']\n",
            "Created 1 players and 1 platforms.\n",
            "PlayerFallingRockCollisionReward c._terminates_function: True\n",
            "render_mode is not human or rgb_array, so no pygame setup.\n",
            "Initializing BalancingBallEnv...\n",
            "Loading the json memory file\n",
            "Loading default level configurations...\n",
            "Using collision_type: {'player': 1000, 'platform': 2000, 'fallingRock': 3000}\n",
            "Using player_configs: [{'shape_type': 'circle', 'size': [0.03], 'shape_mass': 1, 'shape_friction': 100, 'shape_elasticity': 0.8, 'default_position': [0.5, 0.6], 'default_velocity': [0, 0], 'abilities': ['Collision'], 'health': 10, 'color': [255, 213, 79]}]\n",
            "Using level_configs: {'platform_configs': [{'shape_type': 'rectangle', 'size': [0.8, 0.04], 'shape_mass': 1, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': [0.5, 0.67], 'default_velocity': [0, 0], 'abilities': [], 'health': 'infinite', 'color': [235, 64, 52]}], 'falling_rock_configs': [{'shape_type': 'rectangle', 'size': [0.05, 0.05], 'shape_mass': 0.5, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': ['random', -0.001], 'default_velocity': ['random', 0], 'abilities': [], 'health': 'infinite', 'color': [100, 100, 100]}], 'entities_configs': {'quantity': {'fallingRock': 1}}, 'environment_configs': [{'gravity': [0, 4000], 'damping': 0.5}], 'reward': {'reward_per_step': 0.0, 'collision_falling_rock': 10, 'falling_rock_fall_outside_platform': 0.0, 'falling_rock_fall_on_platform': -2.5, 'falling_rock_near': 0.4, 'falling_rock_near_distance_threshold': 200, 'falling_rock_near_distance_reward_multiplier': 5, 'steps_limit_for_movement_penalty': 100, 'movement_penalty': -0.08, 'fail_penalty': 1.0, 'speed_reward_proportion': 0.0, 'opponent_fell_reward': 0.0, 'survival_bonus': 0.0}}\n",
            "Initialized Role with abilities: ['Collision']\n",
            "Created 1 players and 1 platforms.\n",
            "PlayerFallingRockCollisionReward c._terminates_function: True\n",
            "render_mode is not human or rgb_array, so no pygame setup.\n",
            "obs type:  state_based\n",
            "Using cpu device\n",
            "Starting training...\n",
            "Logging to ./logs/PPO_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/spaces/box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/spaces/box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Scores:  [147.07107117191993]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-40.42134242903214]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-81.06919681506865]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [25.35009945779073]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [174.02512994608017]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=10000, episode_reward=42.49 +/- 100.64\n",
            "Episode length: 4239.60 +/- 1024.07\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 4.24e+03 |\n",
            "|    mean_reward     | 42.5     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [-44.09297424836688]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [166.04669320968804]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [31.175582552404617]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-68.26152976381486]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [130.78774392097873]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [99.58956003623643]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [452.5048525236769]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-98.86132784100639]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=20000, episode_reward=100.65 +/- 196.48\n",
            "Episode length: 4496.40 +/- 1320.77\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 4.5e+03  |\n",
            "|    mean_reward     | 101      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.29e+03 |\n",
            "|    ep_rew_mean     | 48.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 846      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 24       |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n",
            "Final Scores:  [359.81221754881767]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [313.7368876153303]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-86.46138089756208]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [131.09531806412048]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [142.43838409352816]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [215.13152532043722]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [278.7040358549938]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=30000, episode_reward=133.68 +/- 123.43\n",
            "Episode length: 5253.00 +/- 765.17\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 5.25e+03    |\n",
            "|    mean_reward          | 134         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 30000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013394932 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -2.96       |\n",
            "|    explained_variance   | -0.000529   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.4        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | 0.00291     |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 94.2        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [123.57442935097005]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [121.75128302832572]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [34.42231148788062]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2.3806107437935196]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [36.96259578413612]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [222.5522549427763]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [281.88848372512285]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=40000, episode_reward=113.14 +/- 113.74\n",
            "Episode length: 3762.80 +/- 437.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.76e+03 |\n",
            "|    mean_reward     | 113      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 40000    |\n",
            "---------------------------------\n",
            "Final Scores:  [281.04291351264334]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-26.88151262367421]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.61e+03 |\n",
            "|    ep_rew_mean     | 145      |\n",
            "| time/              |          |\n",
            "|    fps             | 556      |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Final Scores:  [446.5864215188617]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [4.592425301889105]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [229.5584370002481]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-15.608487747039014]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [111.35018739942713]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [141.0426022217941]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [233.58979620805545]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=50000, episode_reward=137.49 +/- 91.45\n",
            "Episode length: 3900.00 +/- 694.02\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 3.9e+03     |\n",
            "|    mean_reward          | 137         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 50000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010749458 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -3.11       |\n",
            "|    explained_variance   | 0.0856      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 61.2        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | 0.00369     |\n",
            "|    std                  | 1.17        |\n",
            "|    value_loss           | 59.1        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [163.9939668184821]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-69.0091136795824]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [18.287269417032135]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [288.65737836552796]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [55.890072185834164]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-24.982005939566825]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [83.40617170896303]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-64.45350136810906]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=60000, episode_reward=65.20 +/- 122.69\n",
            "Episode length: 3975.20 +/- 1229.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.98e+03 |\n",
            "|    mean_reward     | 65.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 60000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.65e+03 |\n",
            "|    ep_rew_mean     | 133      |\n",
            "| time/              |          |\n",
            "|    fps             | 517      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 118      |\n",
            "|    total_timesteps | 61440    |\n",
            "---------------------------------\n",
            "Final Scores:  [54.755864765817236]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [22.023827958079256]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [246.84126976491646]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-25.57265142661745]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-136.72201967433654]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [196.8076290759336]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [408.1148151921763]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [212.55119718503119]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=70000, episode_reward=128.54 +/- 191.81\n",
            "Episode length: 3709.80 +/- 519.08\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 3.71e+03    |\n",
            "|    mean_reward          | 129         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 70000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011274887 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -3.27       |\n",
            "|    explained_variance   | -0.0123     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.75        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | 0.00566     |\n",
            "|    std                  | 1.25        |\n",
            "|    value_loss           | 15.9        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [185.45374943489458]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [128.85117968711748]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [363.58701418541625]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-9.58880931286753]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [215.54177224218247]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [14.105982456959898]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [167.94671017033983]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=80000, episode_reward=147.82 +/- 137.24\n",
            "Episode length: 4795.80 +/- 691.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 4.8e+03  |\n",
            "|    mean_reward     | 148      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 80000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [351.27314879696104]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [20.776313047298167]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.7e+03  |\n",
            "|    ep_rew_mean     | 136      |\n",
            "| time/              |          |\n",
            "|    fps             | 497      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 164      |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Final Scores:  [134.53326324046725]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [301.7758200244437]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [390.8362856001196]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [77.80338660120333]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [157.79443168483613]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [284.31882916620066]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=90000, episode_reward=240.01 +/- 110.96\n",
            "Episode length: 4247.40 +/- 1305.99\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 4.25e+03    |\n",
            "|    mean_reward          | 240         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 90000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014164278 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -3.42       |\n",
            "|    explained_variance   | 0.305       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.45        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | 0.00341     |\n",
            "|    std                  | 1.36        |\n",
            "|    value_loss           | 31.3        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [6.093294333427903]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [276.24269573500095]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-28.739414472109683]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [721.4380245004357]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [608.2226256986285]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [432.36114995105936]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [204.03465130380374]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [395.3616854948121]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=100000, episode_reward=469.78 +/- 178.90\n",
            "Episode length: 5240.20 +/- 1134.97\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5.24e+03 |\n",
            "|    mean_reward     | 470      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [76.22648962449598]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [393.5005105349678]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.63e+03 |\n",
            "|    ep_rew_mean     | 137      |\n",
            "| time/              |          |\n",
            "|    fps             | 479      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 213      |\n",
            "|    total_timesteps | 102400   |\n",
            "---------------------------------\n",
            "Final Scores:  [137.77986471788094]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [753.5481704924908]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [54.2744649403897]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [347.0769092342892]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [127.11568097811232]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=110000, episode_reward=281.46 +/- 254.25\n",
            "Episode length: 4390.40 +/- 1158.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 4.39e+03    |\n",
            "|    mean_reward          | 281         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 110000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012903148 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -3.58       |\n",
            "|    explained_variance   | 0.29        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 23.4        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | 0.00503     |\n",
            "|    std                  | 1.48        |\n",
            "|    value_loss           | 42.2        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [317.2933606172304]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [208.9316944025553]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-51.99976425180352]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [279.6194250744384]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [217.70256989556165]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [662.1751100948255]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [676.5578169769512]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1170.8968852182095]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [654.1282283458324]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [555.0294587065031]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=120000, episode_reward=741.26 +/- 217.84\n",
            "Episode length: 7167.40 +/- 1459.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.17e+03 |\n",
            "|    mean_reward     | 741      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 120000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.6e+03  |\n",
            "|    ep_rew_mean     | 145      |\n",
            "| time/              |          |\n",
            "|    fps             | 460      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 266      |\n",
            "|    total_timesteps | 122880   |\n",
            "---------------------------------\n",
            "Final Scores:  [295.75777495477917]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [190.51256027711855]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [89.78985828008523]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [278.19733221838027]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [550.1570007062411]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [220.81676880653436]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [105.93043351650684]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [251.7684523215363]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=130000, episode_reward=278.87 +/- 146.68\n",
            "Episode length: 4260.40 +/- 569.61\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 4.26e+03   |\n",
            "|    mean_reward          | 279        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 130000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01350576 |\n",
            "|    clip_fraction        | 0.131      |\n",
            "|    clip_range           | 0.15       |\n",
            "|    entropy_loss         | -3.76      |\n",
            "|    explained_variance   | 0.148      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 16.5       |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | 0.00543    |\n",
            "|    std                  | 1.61       |\n",
            "|    value_loss           | 36.8       |\n",
            "----------------------------------------\n",
            "Final Scores:  [149.74153855991534]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [76.42930427826006]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [244.23496793775516]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [173.75292855808033]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [95.80790294343112]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [199.52384606771454]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [232.8439907012058]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=140000, episode_reward=186.73 +/- 52.92\n",
            "Episode length: 3786.80 +/- 323.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.79e+03 |\n",
            "|    mean_reward     | 187      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 140000   |\n",
            "---------------------------------\n",
            "Final Scores:  [101.27477355507384]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.52e+03 |\n",
            "|    ep_rew_mean     | 146      |\n",
            "| time/              |          |\n",
            "|    fps             | 459      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 311      |\n",
            "|    total_timesteps | 143360   |\n",
            "---------------------------------\n",
            "Final Scores:  [-2.504688886040133]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [224.97659257950744]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [295.13201822872526]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [422.00944974760745]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [616.8422823889435]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [350.37687789335695]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [218.7182706289097]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [340.4234573205897]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=150000, episode_reward=387.17 +/- 131.03\n",
            "Episode length: 4370.00 +/- 810.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 4.37e+03    |\n",
            "|    mean_reward          | 387         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 150000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013904725 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -3.92       |\n",
            "|    explained_variance   | 0.255       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.09        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | 0.0053      |\n",
            "|    std                  | 1.74        |\n",
            "|    value_loss           | 39.3        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [217.39533438900665]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-6.435716781697003]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [143.7093260254008]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [521.5680611974572]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [808.0010587849627]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [462.28933047633933]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [-38.986750991909844]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=160000, episode_reward=376.82 +/- 297.11\n",
            "Episode length: 4695.20 +/- 1466.22\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 4.7e+03  |\n",
            "|    mean_reward     | 377      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 160000   |\n",
            "---------------------------------\n",
            "Final Scores:  [54.26344262132219]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.49e+03 |\n",
            "|    ep_rew_mean     | 143      |\n",
            "| time/              |          |\n",
            "|    fps             | 455      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 359      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "Final Scores:  [325.4750750521096]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [202.16106121740552]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [830.4290191927967]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [538.3199099206568]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [184.7165486393508]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [222.6959634992774]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [584.3004475852503]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=170000, episode_reward=469.59 +/- 240.90\n",
            "Episode length: 4456.00 +/- 1140.15\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 4.46e+03    |\n",
            "|    mean_reward          | 470         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 170000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013815662 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -4.08       |\n",
            "|    explained_variance   | 0.22        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.6        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | 0.00456     |\n",
            "|    std                  | 1.9         |\n",
            "|    value_loss           | 51.4        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [85.0407994307267]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [376.7646223182615]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [96.21766707958119]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [405.0472525267373]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [366.0694839188461]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [340.2512682290719]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [379.44205019855127]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [701.1990405614205]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=180000, episode_reward=435.90 +/- 133.05\n",
            "Episode length: 4508.20 +/- 411.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 4.51e+03 |\n",
            "|    mean_reward     | 436      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 180000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.51e+03 |\n",
            "|    ep_rew_mean     | 151      |\n",
            "| time/              |          |\n",
            "|    fps             | 453      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 406      |\n",
            "|    total_timesteps | 184320   |\n",
            "---------------------------------\n",
            "Final Scores:  [15.021682262353528]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [31.615213211694904]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [164.26377702681523]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1841.5567644059647]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1318.7069657005802]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1078.1643506721741]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [429.4872280675659]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [729.7337094911228]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=190000, episode_reward=1077.03 +/- 486.38\n",
            "Episode length: 7055.00 +/- 1638.10\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.06e+03    |\n",
            "|    mean_reward          | 1.08e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 190000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012133841 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -4.24       |\n",
            "|    explained_variance   | 0.107       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.24        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | 0.00439     |\n",
            "|    std                  | 2.04        |\n",
            "|    value_loss           | 21.7        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [239.05908765116314]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [80.42804310737695]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1464.1626703898569]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1687.7063006763874]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1156.6230025828722]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [670.6334446517042]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1051.1842015642635]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=200000, episode_reward=1203.56 +/- 349.77\n",
            "Episode length: 7782.60 +/- 1367.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.78e+03 |\n",
            "|    mean_reward     | 1.2e+03  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [85.425696005999]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [53.1238143529952]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.5e+03  |\n",
            "|    ep_rew_mean     | 143      |\n",
            "| time/              |          |\n",
            "|    fps             | 436      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 468      |\n",
            "|    total_timesteps | 204800   |\n",
            "---------------------------------\n",
            "Final Scores:  [686.9523737439414]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1072.6024814620548]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [465.96046799424624]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1644.0068212580752]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [962.4878777700789]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=210000, episode_reward=963.90 +/- 399.74\n",
            "Episode length: 7619.20 +/- 2099.70\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.62e+03    |\n",
            "|    mean_reward          | 964         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 210000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014013708 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -4.39       |\n",
            "|    explained_variance   | 0.288       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14.5        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | 0.00441     |\n",
            "|    std                  | 2.22        |\n",
            "|    value_loss           | 35.7        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [-32.54914293450904]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [400.1944738748061]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [560.1313466276108]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [711.340440981302]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1276.7711869438012]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [996.017404376012]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1078.4315200864853]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [559.8846446405746]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [484.60397525267376]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=220000, episode_reward=876.64 +/- 306.29\n",
            "Episode length: 6709.40 +/- 1136.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.71e+03 |\n",
            "|    mean_reward     | 877      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 220000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.56e+03 |\n",
            "|    ep_rew_mean     | 161      |\n",
            "| time/              |          |\n",
            "|    fps             | 427      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 527      |\n",
            "|    total_timesteps | 225280   |\n",
            "---------------------------------\n",
            "Final Scores:  [303.92900057895406]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1711.569077321361]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2615.4961249186335]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1332.7633623919771]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1843.7018366525413]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1355.5967275330083]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=230000, episode_reward=1769.33 +/- 466.18\n",
            "Episode length: 12068.20 +/- 3038.69\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.21e+04    |\n",
            "|    mean_reward          | 1.77e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 230000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013777768 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -4.55       |\n",
            "|    explained_variance   | 0.147       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13.7        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | 0.00484     |\n",
            "|    std                  | 2.41        |\n",
            "|    value_loss           | 45          |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [204.4199730485736]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [251.37927127096447]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1576.6411651968074]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1661.7142228115906]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1822.5178135215588]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [978.2935645786725]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2083.91634570763]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=240000, episode_reward=1622.12 +/- 366.48\n",
            "Episode length: 11275.00 +/- 3062.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.13e+04 |\n",
            "|    mean_reward     | 1.62e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 240000   |\n",
            "---------------------------------\n",
            "Final Scores:  [507.70090029461824]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [722.1386570837029]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.63e+03 |\n",
            "|    ep_rew_mean     | 179      |\n",
            "| time/              |          |\n",
            "|    fps             | 404      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 607      |\n",
            "|    total_timesteps | 245760   |\n",
            "---------------------------------\n",
            "Final Scores:  [103.56604063431915]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2273.0300836061324]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [437.05398848483196]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1760.5733876793165]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1258.1268224126466]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2052.135824131797]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=250000, episode_reward=1553.68 +/- 654.66\n",
            "Episode length: 11380.20 +/- 3526.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.14e+04    |\n",
            "|    mean_reward          | 1.55e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 250000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012146044 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -4.72       |\n",
            "|    explained_variance   | 0.211       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.4        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | 0.00584     |\n",
            "|    std                  | 2.61        |\n",
            "|    value_loss           | 44.3        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [825.3981598193225]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [73.25441518899869]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [297.33804062949946]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1479.7512981397317]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2477.5262468348883]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1136.0092717792256]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1396.5270444655494]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1522.104442425281]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=260000, episode_reward=1599.88 +/- 457.65\n",
            "Episode length: 11472.20 +/- 2070.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.15e+04 |\n",
            "|    mean_reward     | 1.6e+03  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 260000   |\n",
            "---------------------------------\n",
            "Final Scores:  [597.1660056104896]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.66e+03 |\n",
            "|    ep_rew_mean     | 193      |\n",
            "| time/              |          |\n",
            "|    fps             | 388      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 685      |\n",
            "|    total_timesteps | 266240   |\n",
            "---------------------------------\n",
            "Final Scores:  [76.51631780351245]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2062.5367475843314]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1985.3379500612116]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [888.2546633207452]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1103.215079492079]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1910.6576540748224]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=270000, episode_reward=1587.50 +/- 492.30\n",
            "Episode length: 11185.80 +/- 2764.98\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.12e+04    |\n",
            "|    mean_reward          | 1.59e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 270000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013601787 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -4.89       |\n",
            "|    explained_variance   | 0.317       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 65.4        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | 0.00442     |\n",
            "|    std                  | 2.82        |\n",
            "|    value_loss           | 55.8        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [326.92259270564347]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [44.71951325570657]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [440.7491839449903]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2032.8380282155813]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1629.057956054374]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1535.1113653210261]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1069.1576007780434]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2392.303524605741]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=280000, episode_reward=1729.19 +/- 450.61\n",
            "Episode length: 12225.00 +/- 2281.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.22e+04 |\n",
            "|    mean_reward     | 1.73e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 280000   |\n",
            "---------------------------------\n",
            "Final Scores:  [897.1680944514189]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.68e+03 |\n",
            "|    ep_rew_mean     | 204      |\n",
            "| time/              |          |\n",
            "|    fps             | 374      |\n",
            "|    iterations      | 14       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 286720   |\n",
            "---------------------------------\n",
            "Final Scores:  [542.0456289116415]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1663.4282521897244]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2179.6007400514213]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [3386.9320763195697]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2057.7146805781426]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1454.5935117359575]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=290000, episode_reward=2145.95 +/- 672.27\n",
            "Episode length: 14216.80 +/- 2854.20\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1.42e+04   |\n",
            "|    mean_reward          | 2.15e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 290000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01358447 |\n",
            "|    clip_fraction        | 0.12       |\n",
            "|    clip_range           | 0.15       |\n",
            "|    entropy_loss         | -5.04      |\n",
            "|    explained_variance   | 0.303      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 12.2       |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | 0.005      |\n",
            "|    std                  | 3.07       |\n",
            "|    value_loss           | 52.5       |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Final Scores:  [245.38267300660286]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [441.4845803948012]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1878.1647340733666]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1546.9872365791803]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1280.4577150354655]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2144.0489992633875]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1552.487400951474]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=300000, episode_reward=1677.93 +/- 299.40\n",
            "Episode length: 11389.00 +/- 602.87\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.14e+04 |\n",
            "|    mean_reward     | 1.68e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 300000   |\n",
            "---------------------------------\n",
            "Final Scores:  [96.0768593265989]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [728.5035939759949]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.71e+03 |\n",
            "|    ep_rew_mean     | 217      |\n",
            "| time/              |          |\n",
            "|    fps             | 361      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 850      |\n",
            "|    total_timesteps | 307200   |\n",
            "---------------------------------\n",
            "Final Scores:  [394.8834088001903]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1375.437513201156]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1364.5889456454333]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [904.1745963401125]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1480.1366250085678]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1863.6827277638533]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=310000, episode_reward=1395.10 +/- 306.30\n",
            "Episode length: 10346.80 +/- 1540.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.03e+04    |\n",
            "|    mean_reward          | 1.4e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 310000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011729942 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -5.2        |\n",
            "|    explained_variance   | 0.29        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 37.6        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | 0.00508     |\n",
            "|    std                  | 3.3         |\n",
            "|    value_loss           | 49.8        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [1382.9913459010565]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1449.7792545599232]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2448.2701919766578]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1302.9405479336235]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [3574.1594817295513]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=320000, episode_reward=2029.13 +/- 876.73\n",
            "Episode length: 12909.60 +/- 4209.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.29e+04 |\n",
            "|    mean_reward     | 2.03e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 320000   |\n",
            "---------------------------------\n",
            "Final Scores:  [634.4791798164948]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [537.0089188104691]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [939.1100148245341]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [524.6081994297818]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.79e+03 |\n",
            "|    ep_rew_mean     | 239      |\n",
            "| time/              |          |\n",
            "|    fps             | 350      |\n",
            "|    iterations      | 16       |\n",
            "|    time_elapsed    | 935      |\n",
            "|    total_timesteps | 327680   |\n",
            "---------------------------------\n",
            "Final Scores:  [1034.3440690986477]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [848.7011630020431]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2136.6514600143087]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1368.1327773285232]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1307.313507773403]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=330000, episode_reward=1336.53 +/- 440.75\n",
            "Episode length: 9382.40 +/- 1888.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 9.38e+03    |\n",
            "|    mean_reward          | 1.34e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 330000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011632273 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -5.35       |\n",
            "|    explained_variance   | 0.159       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.3        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | 0.00517     |\n",
            "|    std                  | 3.56        |\n",
            "|    value_loss           | 39.3        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [428.8775133358131]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [107.19471481895133]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1079.2997405435146]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1868.0386998902327]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1426.9203400001397]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1188.2217801807415]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1806.9200726345998]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=340000, episode_reward=1471.38 +/- 318.05\n",
            "Episode length: 9852.40 +/- 2360.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 9.85e+03 |\n",
            "|    mean_reward     | 1.47e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 340000   |\n",
            "---------------------------------\n",
            "Final Scores:  [305.677012628204]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [117.53433617398031]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [559.581291517783]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.79e+03 |\n",
            "|    ep_rew_mean     | 243      |\n",
            "| time/              |          |\n",
            "|    fps             | 344      |\n",
            "|    iterations      | 17       |\n",
            "|    time_elapsed    | 1011     |\n",
            "|    total_timesteps | 348160   |\n",
            "---------------------------------\n",
            "Final Scores:  [651.1766323187655]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1693.9455673543873]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1845.7544667107002]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1050.1228384063215]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1717.4024581892284]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=350000, episode_reward=1389.18 +/- 462.31\n",
            "Episode length: 9444.20 +/- 1436.93\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 9.44e+03   |\n",
            "|    mean_reward          | 1.39e+03   |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 350000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01281183 |\n",
            "|    clip_fraction        | 0.113      |\n",
            "|    clip_range           | 0.15       |\n",
            "|    entropy_loss         | -5.5       |\n",
            "|    explained_variance   | 0.183      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 7.28       |\n",
            "|    n_updates            | 170        |\n",
            "|    policy_gradient_loss | 0.00447    |\n",
            "|    std                  | 3.86       |\n",
            "|    value_loss           | 43.6       |\n",
            "----------------------------------------\n",
            "Final Scores:  [123.57381388655979]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1668.4587923847455]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2017.749889637094]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1476.113966861674]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [986.2067705973142]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1418.444693757941]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=360000, episode_reward=1510.89 +/- 336.68\n",
            "Episode length: 10296.00 +/- 1578.78\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.03e+04 |\n",
            "|    mean_reward     | 1.51e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 360000   |\n",
            "---------------------------------\n",
            "Final Scores:  [504.46645022499604]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [85.11560671135102]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [562.8165824316122]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [222.62175154934607]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.8e+03  |\n",
            "|    ep_rew_mean     | 246      |\n",
            "| time/              |          |\n",
            "|    fps             | 338      |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 1088     |\n",
            "|    total_timesteps | 368640   |\n",
            "---------------------------------\n",
            "Final Scores:  [1854.9203928453996]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [869.698460622521]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [411.03623611494953]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1524.4565608850592]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [874.5625951602686]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=370000, episode_reward=1104.43 +/- 515.43\n",
            "Episode length: 8825.00 +/- 2350.30\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.82e+03    |\n",
            "|    mean_reward          | 1.1e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 370000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011141678 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -5.65       |\n",
            "|    explained_variance   | 0.178       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 24.1        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | 0.0053      |\n",
            "|    std                  | 4.17        |\n",
            "|    value_loss           | 35.7        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [490.409681643918]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1312.272228007592]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [587.2813043247318]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1260.3013178749768]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2017.0087321238814]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1002.043494116464]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=380000, episode_reward=1233.28 +/- 467.32\n",
            "Episode length: 8529.40 +/- 2756.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 8.53e+03 |\n",
            "|    mean_reward     | 1.23e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 380000   |\n",
            "---------------------------------\n",
            "Final Scores:  [309.76787211196523]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [257.70079966170385]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [334.761715364664]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.82e+03 |\n",
            "|    ep_rew_mean     | 250      |\n",
            "| time/              |          |\n",
            "|    fps             | 335      |\n",
            "|    iterations      | 19       |\n",
            "|    time_elapsed    | 1160     |\n",
            "|    total_timesteps | 389120   |\n",
            "---------------------------------\n",
            "Final Scores:  [569.9363776332249]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [900.5211128737919]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1478.144335245541]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1695.3847115444266]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [978.6161526220909]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1792.130706939389]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=390000, episode_reward=1366.46 +/- 365.88\n",
            "Episode length: 8616.60 +/- 1312.28\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.62e+03    |\n",
            "|    mean_reward          | 1.37e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 390000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014097676 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -5.83       |\n",
            "|    explained_variance   | 0.335       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.41        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | 0.00544     |\n",
            "|    std                  | 4.54        |\n",
            "|    value_loss           | 35.5        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [170.88260427176866]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [699.0801484403739]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1131.4606398815824]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1347.0553524360328]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1084.5294630095557]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1337.9590553556022]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=400000, episode_reward=1117.52 +/- 235.64\n",
            "Episode length: 7858.60 +/- 1564.67\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.86e+03 |\n",
            "|    mean_reward     | 1.12e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 400000   |\n",
            "---------------------------------\n",
            "Final Scores:  [355.4797559287876]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [141.21693468134933]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.85e+03 |\n",
            "|    ep_rew_mean     | 258      |\n",
            "| time/              |          |\n",
            "|    fps             | 332      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 1230     |\n",
            "|    total_timesteps | 409600   |\n",
            "---------------------------------\n",
            "Final Scores:  [1504.0075308194555]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1043.9293018011538]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1182.7721442040295]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [795.071582214695]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1376.946796846789]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=410000, episode_reward=1178.05 +/- 249.18\n",
            "Episode length: 8644.80 +/- 737.04\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 8.64e+03     |\n",
            "|    mean_reward          | 1.18e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 410000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0123432055 |\n",
            "|    clip_fraction        | 0.118        |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -5.99        |\n",
            "|    explained_variance   | 0.421        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.95         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | 0.00449      |\n",
            "|    std                  | 4.91         |\n",
            "|    value_loss           | 34.2         |\n",
            "------------------------------------------\n",
            "Final Scores:  [238.03991299720482]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [378.41266497760915]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [837.0511115825816]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [138.0501261720147]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1244.1920629655501]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1729.7948624638539]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1287.6976463546468]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [864.1098459429625]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1829.4874734239768]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=420000, episode_reward=1388.56 +/- 351.25\n",
            "Episode length: 10053.00 +/- 1648.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.01e+04 |\n",
            "|    mean_reward     | 1.39e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 420000   |\n",
            "---------------------------------\n",
            "Final Scores:  [318.73683383521933]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [133.57602668386676]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1069.1776962442746]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1056.173637058531]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1223.0750733338082]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1169.401428089146]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1026.9912581952576]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=430000, episode_reward=1106.46 +/- 74.52\n",
            "Episode length: 9210.20 +/- 845.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 9.21e+03 |\n",
            "|    mean_reward     | 1.11e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 430000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.86e+03 |\n",
            "|    ep_rew_mean     | 267      |\n",
            "| time/              |          |\n",
            "|    fps             | 324      |\n",
            "|    iterations      | 21       |\n",
            "|    time_elapsed    | 1325     |\n",
            "|    total_timesteps | 430080   |\n",
            "---------------------------------\n",
            "Final Scores:  [192.37429235673937]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [437.80931030704716]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [15.312295443327505]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [617.8847502112662]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1278.4629764257088]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [759.2473557753929]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [630.9549457367183]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [698.4638700730513]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=440000, episode_reward=794.50 +/- 246.01\n",
            "Episode length: 6724.60 +/- 1678.77\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.72e+03    |\n",
            "|    mean_reward          | 795         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 440000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009794057 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -6.14       |\n",
            "|    explained_variance   | 0.331       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.14        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | 0.00583     |\n",
            "|    std                  | 5.27        |\n",
            "|    value_loss           | 22.3        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [227.69667274937657]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [334.21486185824176]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1664.6437444197652]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [336.5001707460388]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [777.6293726960549]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1565.5428684028382]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1099.0857776490063]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=450000, episode_reward=1086.18 +/- 494.32\n",
            "Episode length: 7383.80 +/- 2000.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.38e+03 |\n",
            "|    mean_reward     | 1.09e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 450000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.84e+03 |\n",
            "|    ep_rew_mean     | 273      |\n",
            "| time/              |          |\n",
            "|    fps             | 324      |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 1389     |\n",
            "|    total_timesteps | 450560   |\n",
            "---------------------------------\n",
            "Final Scores:  [232.4161819847659]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [274.9848883711937]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [303.69428552900695]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [96.86150763381966]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [604.8780766438474]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [607.3588289902619]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [782.8295623172678]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1157.3315752959809]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2789.4597626947766]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=460000, episode_reward=1185.87 +/- 825.46\n",
            "Episode length: 9276.00 +/- 5216.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 9.28e+03    |\n",
            "|    mean_reward          | 1.19e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 460000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013645408 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -6.29       |\n",
            "|    explained_variance   | 0.223       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.45        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | 0.00437     |\n",
            "|    std                  | 5.72        |\n",
            "|    value_loss           | 23.2        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [184.61122527759133]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [63.47344959826228]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1919.1183367621868]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1301.8242196144681]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [826.2938242221142]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1277.8969460846097]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1830.1093847019736]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=470000, episode_reward=1428.55 +/- 400.84\n",
            "Episode length: 9237.40 +/- 1202.05\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 9.24e+03 |\n",
            "|    mean_reward     | 1.43e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 470000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.85e+03 |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 23       |\n",
            "|    time_elapsed    | 1463     |\n",
            "|    total_timesteps | 471040   |\n",
            "---------------------------------\n",
            "Final Scores:  [66.14650400747368]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [493.21692575243924]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [223.72129302609005]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1584.6985067031617]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2090.9728854086406]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2239.3526794465306]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [556.669175138729]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1105.7901185726769]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=480000, episode_reward=1513.00 +/- 624.04\n",
            "Episode length: 9839.40 +/- 2625.24\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 9.84e+03    |\n",
            "|    mean_reward          | 1.51e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 480000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013242805 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -6.45       |\n",
            "|    explained_variance   | 0.259       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.9        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | 0.00469     |\n",
            "|    std                  | 6.22        |\n",
            "|    value_loss           | 25.9        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [227.80513782502624]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [298.8088155749516]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [101.80557097165384]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [583.6851753922869]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1180.9530732485262]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [600.3738155655702]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [263.16591512613456]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1221.5809213649632]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=490000, episode_reward=767.45 +/- 372.34\n",
            "Episode length: 6461.60 +/- 1577.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.46e+03 |\n",
            "|    mean_reward     | 767      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 490000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.89e+03 |\n",
            "|    ep_rew_mean     | 284      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 24       |\n",
            "|    time_elapsed    | 1528     |\n",
            "|    total_timesteps | 491520   |\n",
            "---------------------------------\n",
            "Final Scores:  [353.38177129358]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [141.8097680106479]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2353.6291542327594]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [809.6506013324575]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1249.730864296229]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1197.818955900955]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1400.5824988692327]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=500000, episode_reward=1399.78 +/- 514.07\n",
            "Episode length: 9346.40 +/- 2177.75\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 9.35e+03    |\n",
            "|    mean_reward          | 1.4e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 500000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013763052 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -6.63       |\n",
            "|    explained_variance   | 0.198       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.94        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | 0.00492     |\n",
            "|    std                  | 6.77        |\n",
            "|    value_loss           | 18.4        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [215.80527208547642]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [179.37346069238885]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [123.21019772614876]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1648.6751109269137]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1177.341176661829]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [674.7352454737038]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [711.5708650647321]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2232.9529886561636]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=510000, episode_reward=1286.56 +/- 590.53\n",
            "Episode length: 8473.80 +/- 2594.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 8.47e+03 |\n",
            "|    mean_reward     | 1.29e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 510000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.89e+03 |\n",
            "|    ep_rew_mean     | 283      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 1596     |\n",
            "|    total_timesteps | 512000   |\n",
            "---------------------------------\n",
            "Final Scores:  [297.61528975595223]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [921.9225392362814]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [731.5262597783425]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [706.8532734619112]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [526.8634784530124]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [726.8482216349003]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=520000, episode_reward=720.30 +/- 125.21\n",
            "Episode length: 7372.60 +/- 1254.77\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.37e+03    |\n",
            "|    mean_reward          | 720         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 520000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013577242 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -6.79       |\n",
            "|    explained_variance   | 0.428       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.57        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | 0.00468     |\n",
            "|    std                  | 7.37        |\n",
            "|    value_loss           | 25.1        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [352.6540939035436]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [487.5147062651151]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [499.86638374369045]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1512.8840425814356]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [970.4357235019554]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [328.3341904093914]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1019.5774038428912]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1346.6355462598715]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=530000, episode_reward=1033.07 +/- 407.25\n",
            "Episode length: 7489.40 +/- 2400.05\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.49e+03 |\n",
            "|    mean_reward     | 1.03e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 530000   |\n",
            "---------------------------------\n",
            "Final Scores:  [582.8611044806455]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.97e+03 |\n",
            "|    ep_rew_mean     | 296      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 26       |\n",
            "|    time_elapsed    | 1658     |\n",
            "|    total_timesteps | 532480   |\n",
            "---------------------------------\n",
            "Final Scores:  [383.72623191536167]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1198.9903794470845]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [490.3935131556449]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [616.4074031012301]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [407.06723051011863]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1097.6382896452537]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=540000, episode_reward=759.60 +/- 323.90\n",
            "Episode length: 6726.40 +/- 2000.75\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.73e+03    |\n",
            "|    mean_reward          | 760         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 540000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013967964 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -6.96       |\n",
            "|    explained_variance   | 0.38        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.9         |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | 0.00505     |\n",
            "|    std                  | 8           |\n",
            "|    value_loss           | 43.7        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [367.89381931170004]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [252.1955509472988]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [220.0679412388082]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1772.2667808433446]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1018.6753651090955]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1802.694857573069]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1204.6413208198992]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1048.5228763493499]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=550000, episode_reward=1366.86 +/- 347.32\n",
            "Episode length: 8165.40 +/- 1185.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 8.17e+03 |\n",
            "|    mean_reward     | 1.37e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 550000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4e+03    |\n",
            "|    ep_rew_mean     | 304      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 27       |\n",
            "|    time_elapsed    | 1719     |\n",
            "|    total_timesteps | 552960   |\n",
            "---------------------------------\n",
            "Final Scores:  [201.11022764648618]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [880.4610761992316]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [313.43862725086836]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [589.7844316017755]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [523.5699187353429]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1807.495977844085]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1200.2391456960997]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [888.4991047770937]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=560000, episode_reward=999.42 +/- 469.00\n",
            "Episode length: 7603.00 +/- 2197.80\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.6e+03     |\n",
            "|    mean_reward          | 999         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 560000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013195774 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -7.13       |\n",
            "|    explained_variance   | 0.255       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.46        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | 0.00518     |\n",
            "|    std                  | 8.67        |\n",
            "|    value_loss           | 43.8        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [319.81193672581287]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [316.8977012736741]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [271.19950223473825]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1003.24945519919]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [214.40538018890487]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1249.04746020343]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1367.0969871958216]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=570000, episode_reward=818.50 +/- 486.81\n",
            "Episode length: 7282.40 +/- 2533.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.28e+03 |\n",
            "|    mean_reward     | 818      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 570000   |\n",
            "---------------------------------\n",
            "Final Scores:  [-2.152990726622032]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.02e+03 |\n",
            "|    ep_rew_mean     | 313      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 28       |\n",
            "|    time_elapsed    | 1781     |\n",
            "|    total_timesteps | 573440   |\n",
            "---------------------------------\n",
            "Final Scores:  [1654.9401324313535]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [867.4657585427333]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [835.1234190919901]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1475.4307855393338]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1433.4654005924042]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=580000, episode_reward=1250.79 +/- 336.71\n",
            "Episode length: 8319.40 +/- 2056.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.32e+03    |\n",
            "|    mean_reward          | 1.25e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 580000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014123644 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -7.3        |\n",
            "|    explained_variance   | 0.198       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.94        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | 0.00472     |\n",
            "|    std                  | 9.43        |\n",
            "|    value_loss           | 40.9        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [454.1676773331852]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [687.4714084517334]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [458.1384996142667]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1024.833553419592]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [362.59338650010125]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1034.4739440789556]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [482.02150140428125]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1354.205920799611]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [966.7633334003157]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1060.2438489821493]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=590000, episode_reward=977.04 +/- 281.99\n",
            "Episode length: 7725.20 +/- 2497.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.73e+03 |\n",
            "|    mean_reward     | 977      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 590000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.08e+03 |\n",
            "|    ep_rew_mean     | 337      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 29       |\n",
            "|    time_elapsed    | 1847     |\n",
            "|    total_timesteps | 593920   |\n",
            "---------------------------------\n",
            "Final Scores:  [1013.35645245171]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1449.8969771504646]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [730.681534582969]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1155.45312457098]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1250.1170054463194]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=600000, episode_reward=1117.40 +/- 240.82\n",
            "Episode length: 9043.60 +/- 1066.23\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 9.04e+03    |\n",
            "|    mean_reward          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 600000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010976415 |\n",
            "|    clip_fraction        | 0.0978      |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -7.44       |\n",
            "|    explained_variance   | 0.361       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.8        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | 0.00503     |\n",
            "|    std                  | 10.2        |\n",
            "|    value_loss           | 44.6        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [207.21024464661397]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [182.9689605565335]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [297.6405582766653]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [370.98996366502035]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1030.149639841234]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1968.142194780451]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1325.9473682539513]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1186.3577193366348]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1270.7580640219014]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=610000, episode_reward=1353.77 +/- 321.82\n",
            "Episode length: 8486.00 +/- 2300.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 8.49e+03 |\n",
            "|    mean_reward     | 1.35e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 610000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.1e+03  |\n",
            "|    ep_rew_mean     | 342      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 30       |\n",
            "|    time_elapsed    | 1916     |\n",
            "|    total_timesteps | 614400   |\n",
            "---------------------------------\n",
            "Final Scores:  [670.2291416856375]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [347.06168863235115]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1607.2758666393293]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1096.8052216831843]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [517.1911141588131]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [904.9510640247953]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [718.7105053299447]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=620000, episode_reward=966.49 +/- 372.72\n",
            "Episode length: 6019.40 +/- 1039.81\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.02e+03    |\n",
            "|    mean_reward          | 966         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 620000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007867878 |\n",
            "|    clip_fraction        | 0.0842      |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -7.57       |\n",
            "|    explained_variance   | 0.0624      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.69        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | 0.00451     |\n",
            "|    std                  | 10.8        |\n",
            "|    value_loss           | 38.1        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [259.4384427034496]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [369.2826067760444]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [772.493348958341]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1402.7818987151823]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [368.94997808170336]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1539.745688447616]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1872.5633423698307]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=630000, episode_reward=1188.81 +/- 544.41\n",
            "Episode length: 8686.60 +/- 1855.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 8.69e+03 |\n",
            "|    mean_reward     | 1.19e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 630000   |\n",
            "---------------------------------\n",
            "Final Scores:  [492.0025205890306]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.1e+03  |\n",
            "|    ep_rew_mean     | 346      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 31       |\n",
            "|    time_elapsed    | 1976     |\n",
            "|    total_timesteps | 634880   |\n",
            "---------------------------------\n",
            "Final Scores:  [525.2916309721091]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [653.7656205737543]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2354.1302271658283]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [710.3749699530757]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1181.1739304034143]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2231.684275585732]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=640000, episode_reward=1423.73 +/- 731.98\n",
            "Episode length: 9864.20 +/- 3174.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 9.86e+03    |\n",
            "|    mean_reward          | 1.42e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 640000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012326648 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -7.72       |\n",
            "|    explained_variance   | 0.303       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.01        |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | 0.0048      |\n",
            "|    std                  | 11.7        |\n",
            "|    value_loss           | 31.6        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [352.94608509887564]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [370.7956000475858]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [691.0866581695494]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [955.3986766790727]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [746.5867026175897]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [653.0630947312242]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [416.08253483486897]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1186.9973225987837]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=650000, episode_reward=789.13 +/- 262.83\n",
            "Episode length: 5979.80 +/- 980.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5.98e+03 |\n",
            "|    mean_reward     | 789      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 650000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.11e+03 |\n",
            "|    ep_rew_mean     | 353      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 32       |\n",
            "|    time_elapsed    | 2039     |\n",
            "|    total_timesteps | 655360   |\n",
            "---------------------------------\n",
            "Final Scores:  [665.0836338647212]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1359.7145616294652]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1426.4638320538359]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1487.861492831775]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2866.729672693937]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1256.2406698143884]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=660000, episode_reward=1676.90 +/- 598.61\n",
            "Episode length: 10578.60 +/- 3008.90\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.06e+04    |\n",
            "|    mean_reward          | 1.68e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 660000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011513745 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -7.87       |\n",
            "|    explained_variance   | 0.241       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.7        |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | 0.0053      |\n",
            "|    std                  | 12.6        |\n",
            "|    value_loss           | 32.2        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [412.8859426933382]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [198.7647616101354]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [285.66225158215144]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [643.7795702244664]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1768.0011942137635]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1521.531503578602]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1174.831923277846]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [711.9261091420728]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=670000, episode_reward=1161.51 +/- 439.95\n",
            "Episode length: 9290.60 +/- 2992.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 9.29e+03 |\n",
            "|    mean_reward     | 1.16e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 670000   |\n",
            "---------------------------------\n",
            "Final Scores:  [307.92321511285286]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [90.37316924432251]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.11e+03 |\n",
            "|    ep_rew_mean     | 347      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 33       |\n",
            "|    time_elapsed    | 2111     |\n",
            "|    total_timesteps | 675840   |\n",
            "---------------------------------\n",
            "Final Scores:  [318.55158766288105]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1272.2266151056199]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1121.0487360496982]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1031.716117307429]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [970.595817640893]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1280.6098511446783]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=680000, episode_reward=1132.74 +/- 124.84\n",
            "Episode length: 8236.40 +/- 736.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.24e+03    |\n",
            "|    mean_reward          | 1.13e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 680000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011377829 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -8.03       |\n",
            "|    explained_variance   | 0.46        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.72        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | 0.00492     |\n",
            "|    std                  | 13.7        |\n",
            "|    value_loss           | 33.9        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [577.128648319101]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [445.14796993638174]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [124.90880337762707]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1051.9737674301375]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [681.0400034869014]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2456.7553706549875]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [918.5587603144319]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [300.4762558859386]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=690000, episode_reward=1079.26 +/- 733.36\n",
            "Episode length: 7372.40 +/- 2800.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.37e+03 |\n",
            "|    mean_reward     | 1.08e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 690000   |\n",
            "---------------------------------\n",
            "Final Scores:  [356.2903557409064]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.1e+03  |\n",
            "|    ep_rew_mean     | 347      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 34       |\n",
            "|    time_elapsed    | 2173     |\n",
            "|    total_timesteps | 696320   |\n",
            "---------------------------------\n",
            "Final Scores:  [1243.1320885257744]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1122.4237134155917]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1011.8819791588853]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [630.8224336155422]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [416.50889414970766]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=700000, episode_reward=882.45 +/- 311.39\n",
            "Episode length: 7060.00 +/- 1580.08\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.06e+03    |\n",
            "|    mean_reward          | 882         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 700000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012641999 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -8.19       |\n",
            "|    explained_variance   | 0.258       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.28        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | 0.00502     |\n",
            "|    std                  | 14.8        |\n",
            "|    value_loss           | 44.3        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [509.9664927577345]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [323.8616426094607]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [68.72609131686666]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [632.3305821566609]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [748.8092527231678]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1299.083443039239]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1389.1630942466932]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1543.4844167330452]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1335.3051952388732]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=710000, episode_reward=1260.67 +/- 270.37\n",
            "Episode length: 8142.20 +/- 1586.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 8.14e+03 |\n",
            "|    mean_reward     | 1.26e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 710000   |\n",
            "---------------------------------\n",
            "Final Scores:  [737.7401482258289]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.11e+03 |\n",
            "|    ep_rew_mean     | 349      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 35       |\n",
            "|    time_elapsed    | 2235     |\n",
            "|    total_timesteps | 716800   |\n",
            "---------------------------------\n",
            "Final Scores:  [826.5896476416771]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [639.1711409082462]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [651.2452927133777]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [722.5152002938233]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1156.9726251285342]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=720000, episode_reward=796.80 +/- 190.86\n",
            "Episode length: 5873.00 +/- 847.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 5.87e+03    |\n",
            "|    mean_reward          | 797         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 720000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013463667 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -8.36       |\n",
            "|    explained_variance   | 0.348       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.9        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | 0.00461     |\n",
            "|    std                  | 16.1        |\n",
            "|    value_loss           | 39.6        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [571.9326898514854]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [663.9086562193019]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1662.0641361801597]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1281.011153244104]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1921.7064494062197]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1097.794325950895]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1024.5943180756337]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=730000, episode_reward=1394.93 +/- 342.71\n",
            "Episode length: 9145.40 +/- 2155.22\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 9.15e+03 |\n",
            "|    mean_reward     | 1.39e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 730000   |\n",
            "---------------------------------\n",
            "Final Scores:  [522.4896289457117]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [417.1884183805817]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [231.17885883277438]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.08e+03 |\n",
            "|    ep_rew_mean     | 343      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 36       |\n",
            "|    time_elapsed    | 2297     |\n",
            "|    total_timesteps | 737280   |\n",
            "---------------------------------\n",
            "Final Scores:  [842.3550643179398]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1604.6512079477782]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1991.3491069222955]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1054.5525390033224]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1334.1484787037286]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=740000, episode_reward=1362.91 +/- 405.03\n",
            "Episode length: 8060.40 +/- 1549.17\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.06e+03    |\n",
            "|    mean_reward          | 1.36e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 740000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013288173 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -8.52       |\n",
            "|    explained_variance   | 0.392       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.3         |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | 0.00535     |\n",
            "|    std                  | 17.4        |\n",
            "|    value_loss           | 27.3        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [355.6788636347185]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [221.59895024752387]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [350.51265289234175]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1216.7909065356766]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [986.9189163026319]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1363.2139042809886]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1342.091846990799]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [629.431911079499]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=750000, episode_reward=1105.19 +/- 274.03\n",
            "Episode length: 7549.00 +/- 1356.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.55e+03 |\n",
            "|    mean_reward     | 1.11e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 750000   |\n",
            "---------------------------------\n",
            "Final Scores:  [408.0983594520222]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [752.890925990163]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.1e+03  |\n",
            "|    ep_rew_mean     | 349      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 37       |\n",
            "|    time_elapsed    | 2360     |\n",
            "|    total_timesteps | 757760   |\n",
            "---------------------------------\n",
            "Final Scores:  [801.0491374238824]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [786.4533957922816]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1062.8532260181041]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1225.8141167394688]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1648.3560026876626]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=760000, episode_reward=1102.41 +/- 317.95\n",
            "Episode length: 7489.40 +/- 1851.32\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.49e+03    |\n",
            "|    mean_reward          | 1.1e+03     |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 760000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012006416 |\n",
            "|    clip_fraction        | 0.111       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -8.68       |\n",
            "|    explained_variance   | 0.243       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.79        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | 0.00559     |\n",
            "|    std                  | 18.8        |\n",
            "|    value_loss           | 40.1        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [769.094247142264]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [990.1030531562309]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [64.81377263399757]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [818.6722375990546]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [201.54939532025907]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=770000, episode_reward=566.35 +/- 365.77\n",
            "Episode length: 5288.20 +/- 1543.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5.29e+03 |\n",
            "|    mean_reward     | 566      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 770000   |\n",
            "---------------------------------\n",
            "Final Scores:  [265.4487563092968]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [240.90568578810357]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [595.2798599475525]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [613.2918574639343]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [284.8725444138587]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.13e+03 |\n",
            "|    ep_rew_mean     | 354      |\n",
            "| time/              |          |\n",
            "|    fps             | 322      |\n",
            "|    iterations      | 38       |\n",
            "|    time_elapsed    | 2416     |\n",
            "|    total_timesteps | 778240   |\n",
            "---------------------------------\n",
            "Final Scores:  [2236.9858187534633]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [967.8042149215269]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [617.8303257508875]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1416.4150043020297]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [549.6562331450637]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=780000, episode_reward=1155.24 +/- 621.13\n",
            "Episode length: 7440.00 +/- 2370.77\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.44e+03    |\n",
            "|    mean_reward          | 1.16e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 780000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012924825 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -8.84       |\n",
            "|    explained_variance   | 0.401       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.3        |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | 0.00506     |\n",
            "|    std                  | 20.5        |\n",
            "|    value_loss           | 41          |\n",
            "-----------------------------------------\n",
            "Final Scores:  [131.61359302024476]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [798.7044916368628]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [966.6104617556289]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1131.6080338911138]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1294.3365268344955]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1203.15410806192]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=790000, episode_reward=1076.38 +/- 176.49\n",
            "Episode length: 6900.00 +/- 1145.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.9e+03  |\n",
            "|    mean_reward     | 1.08e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 790000   |\n",
            "---------------------------------\n",
            "Final Scores:  [586.1557512677498]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [292.81677751711936]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [496.67336989013796]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.12e+03 |\n",
            "|    ep_rew_mean     | 355      |\n",
            "| time/              |          |\n",
            "|    fps             | 322      |\n",
            "|    iterations      | 39       |\n",
            "|    time_elapsed    | 2477     |\n",
            "|    total_timesteps | 798720   |\n",
            "---------------------------------\n",
            "Final Scores:  [457.5031581996313]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1103.7605755238915]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [738.2125035331059]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [797.2123245822402]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [992.9367792805871]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1473.433592509234]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=800000, episode_reward=1018.61 +/- 261.68\n",
            "Episode length: 8150.80 +/- 1109.22\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 8.15e+03    |\n",
            "|    mean_reward          | 1.02e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 800000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009655793 |\n",
            "|    clip_fraction        | 0.0942      |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -8.98       |\n",
            "|    explained_variance   | 0.418       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 84.8        |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | 0.00455     |\n",
            "|    std                  | 22          |\n",
            "|    value_loss           | 44.2        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [1554.4506239345828]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1096.1666244587586]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1530.6469002171748]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [989.8726593341521]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [705.2476887863887]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=810000, episode_reward=1172.78 +/- 326.07\n",
            "Episode length: 7702.40 +/- 1489.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 7.7e+03  |\n",
            "|    mean_reward     | 1.17e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 810000   |\n",
            "---------------------------------\n",
            "Final Scores:  [532.4770459896157]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [474.2303267246557]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [291.881267669285]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [486.0995382376252]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.12e+03 |\n",
            "|    ep_rew_mean     | 362      |\n",
            "| time/              |          |\n",
            "|    fps             | 322      |\n",
            "|    iterations      | 40       |\n",
            "|    time_elapsed    | 2539     |\n",
            "|    total_timesteps | 819200   |\n",
            "---------------------------------\n",
            "Final Scores:  [1007.0586432976231]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1912.724533809107]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [708.6048501326285]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1319.1195204084984]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [295.03269194903606]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=820000, episode_reward=1046.01 +/- 548.65\n",
            "Episode length: 6554.20 +/- 2077.59\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.55e+03    |\n",
            "|    mean_reward          | 1.05e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 820000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009630747 |\n",
            "|    clip_fraction        | 0.0983      |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -9.13       |\n",
            "|    explained_variance   | 0.295       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.62        |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | 0.00465     |\n",
            "|    std                  | 23.5        |\n",
            "|    value_loss           | 35.1        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [767.07465712406]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [121.32989344733348]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1546.1546840812944]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1529.5244206717023]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1396.9856531370942]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1814.023987970512]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1368.494758842651]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=830000, episode_reward=1528.54 +/- 157.93\n",
            "Episode length: 9489.60 +/- 1714.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 9.49e+03 |\n",
            "|    mean_reward     | 1.53e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 830000   |\n",
            "---------------------------------\n",
            "Final Scores:  [261.61241219422936]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [101.51415099393093]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [198.92889119200126]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.14e+03 |\n",
            "|    ep_rew_mean     | 359      |\n",
            "| time/              |          |\n",
            "|    fps             | 322      |\n",
            "|    iterations      | 41       |\n",
            "|    time_elapsed    | 2604     |\n",
            "|    total_timesteps | 839680   |\n",
            "---------------------------------\n",
            "Final Scores:  [899.7507576748475]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [704.5381267461821]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [2274.4870099059704]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1807.7424665263725]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [423.93674476459995]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=840000, episode_reward=1219.59 +/- 701.33\n",
            "Episode length: 7001.60 +/- 2170.77\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 7e+03        |\n",
            "|    mean_reward          | 1.22e+03     |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 840000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0095369965 |\n",
            "|    clip_fraction        | 0.102        |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -9.27        |\n",
            "|    explained_variance   | 0.0659       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.1          |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | 0.00527      |\n",
            "|    std                  | 25.3         |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "Final Scores:  [152.87812008297908]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [405.46962748134416]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [853.7147885685889]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [566.8173574642957]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [864.3527845965805]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1177.3107191902602]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1128.6006508401604]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=850000, episode_reward=915.66 +/- 219.97\n",
            "Episode length: 6468.00 +/- 1045.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.47e+03 |\n",
            "|    mean_reward     | 916      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 850000   |\n",
            "---------------------------------\n",
            "Final Scores:  [170.08900940260926]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [122.97076483119126]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [253.0948181842975]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1677.628524518108]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1209.811407581318]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [973.2315671470393]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [718.6055752899573]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1199.3119334403402]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=860000, episode_reward=1153.22 +/- 316.73\n",
            "Episode length: 8234.20 +/- 503.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 8.23e+03 |\n",
            "|    mean_reward     | 1.15e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 860000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.15e+03 |\n",
            "|    ep_rew_mean     | 358      |\n",
            "| time/              |          |\n",
            "|    fps             | 320      |\n",
            "|    iterations      | 42       |\n",
            "|    time_elapsed    | 2681     |\n",
            "|    total_timesteps | 860160   |\n",
            "---------------------------------\n",
            "Final Scores:  [740.6121376736953]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1334.6695989882337]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [813.736770488606]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [603.0973435195336]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1167.4805060537221]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1162.3540338626842]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=870000, episode_reward=1013.77 +/- 267.33\n",
            "Episode length: 7093.00 +/- 1094.56\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.09e+03    |\n",
            "|    mean_reward          | 1.01e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 870000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010637707 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -9.42       |\n",
            "|    explained_variance   | 0.309       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.33        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | 0.00508     |\n",
            "|    std                  | 27.2        |\n",
            "|    value_loss           | 18.1        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [205.84324153673484]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [245.39660415607077]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [241.88398122473558]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [290.59295213894734]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [953.4073865518072]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [686.6937154129031]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1183.2187118103761]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [443.1487789615644]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1316.1853036278403]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=880000, episode_reward=914.03 +/- 319.10\n",
            "Episode length: 6278.80 +/- 1125.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.28e+03 |\n",
            "|    mean_reward     | 914      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 880000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.15e+03 |\n",
            "|    ep_rew_mean     | 364      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 43       |\n",
            "|    time_elapsed    | 2741     |\n",
            "|    total_timesteps | 880640   |\n",
            "---------------------------------\n",
            "Final Scores:  [460.00709105257187]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [246.64932546070276]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [234.00433029145196]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [239.88285761180666]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [300.28183095724836]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [870.8232884306184]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1833.9173069202861]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [521.9533496665397]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1151.3983582134442]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=890000, episode_reward=933.17 +/- 535.25\n",
            "Episode length: 6892.00 +/- 1595.12\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.89e+03    |\n",
            "|    mean_reward          | 933         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 890000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013861006 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -9.57       |\n",
            "|    explained_variance   | 0.383       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.7         |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | 0.00525     |\n",
            "|    std                  | 29.6        |\n",
            "|    value_loss           | 38.3        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [1094.878788802933]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [409.5749868190978]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1251.4281731253966]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [929.0564592117222]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [863.0486146717304]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=900000, episode_reward=907.10 +/- 284.16\n",
            "Episode length: 6115.20 +/- 1550.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.12e+03 |\n",
            "|    mean_reward     | 907      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 900000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.13e+03 |\n",
            "|    ep_rew_mean     | 368      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 44       |\n",
            "|    time_elapsed    | 2800     |\n",
            "|    total_timesteps | 901120   |\n",
            "---------------------------------\n",
            "Final Scores:  [913.699521615823]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [384.68264915161024]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [295.75820184231526]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [730.5238159583772]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1092.5690519020293]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1068.309707686676]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [994.0794773472307]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1199.8455379305049]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=910000, episode_reward=1014.57 +/- 157.71\n",
            "Episode length: 7114.40 +/- 596.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 7.11e+03    |\n",
            "|    mean_reward          | 1.01e+03    |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 910000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012310791 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -9.73       |\n",
            "|    explained_variance   | 0.451       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 55.4        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | 0.00475     |\n",
            "|    std                  | 32.1        |\n",
            "|    value_loss           | 56.5        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [630.328521670969]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [821.4364208955606]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [200.95219112293995]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1775.6957135249959]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [626.4184343008318]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [671.1447686287455]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [846.8942322087646]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [820.3608679462716]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=920000, episode_reward=945.60 +/- 422.29\n",
            "Episode length: 6983.20 +/- 1859.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.98e+03 |\n",
            "|    mean_reward     | 946      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 920000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.16e+03 |\n",
            "|    ep_rew_mean     | 387      |\n",
            "| time/              |          |\n",
            "|    fps             | 321      |\n",
            "|    iterations      | 45       |\n",
            "|    time_elapsed    | 2862     |\n",
            "|    total_timesteps | 921600   |\n",
            "---------------------------------\n",
            "Final Scores:  [574.047210292538]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [117.61762989316559]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [430.48995680187573]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [425.22478753843114]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [680.3126981926448]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1011.5697282075987]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [850.9597185729709]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=930000, episode_reward=677.21 +/- 230.79\n",
            "Episode length: 5833.80 +/- 1368.80\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 5.83e+03    |\n",
            "|    mean_reward          | 677         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 930000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012596753 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -9.91       |\n",
            "|    explained_variance   | 0.383       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.7        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | 0.00557     |\n",
            "|    std                  | 34.8        |\n",
            "|    value_loss           | 32.6        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [589.1034584379761]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [305.6271124710299]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1086.3357885532578]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [461.46190784935857]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [642.7082787409005]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1685.2838567482838]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [726.4650058521614]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=940000, episode_reward=917.95 +/- 433.11\n",
            "Episode length: 6677.60 +/- 1288.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.68e+03 |\n",
            "|    mean_reward     | 918      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 940000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.18e+03 |\n",
            "|    ep_rew_mean     | 393      |\n",
            "| time/              |          |\n",
            "|    fps             | 322      |\n",
            "|    iterations      | 46       |\n",
            "|    time_elapsed    | 2917     |\n",
            "|    total_timesteps | 942080   |\n",
            "---------------------------------\n",
            "Final Scores:  [747.4974731441627]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [397.4640430138466]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [453.5350985951519]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [886.0371899092978]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [661.1952230658699]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [224.86055822950954]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [967.3010422794771]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1538.347750199382]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=950000, episode_reward=853.05 +/- 427.83\n",
            "Episode length: 6782.20 +/- 2046.47\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.78e+03    |\n",
            "|    mean_reward          | 853         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 950000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013158739 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -10.1       |\n",
            "|    explained_variance   | 0.312       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 25.4        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | 0.00502     |\n",
            "|    std                  | 37.8        |\n",
            "|    value_loss           | 46.7        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [270.9286904775105]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [426.6061155183022]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [841.8053587850785]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1633.5682304706363]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [995.7319707481919]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1394.4453432285131]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [834.3815716058942]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=960000, episode_reward=1137.49 +/- 319.84\n",
            "Episode length: 9004.40 +/- 2330.78\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 9e+03    |\n",
            "|    mean_reward     | 1.14e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 960000   |\n",
            "---------------------------------\n",
            "Final Scores:  [195.41778823441422]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.18e+03 |\n",
            "|    ep_rew_mean     | 392      |\n",
            "| time/              |          |\n",
            "|    fps             | 322      |\n",
            "|    iterations      | 47       |\n",
            "|    time_elapsed    | 2983     |\n",
            "|    total_timesteps | 962560   |\n",
            "---------------------------------\n",
            "Final Scores:  [320.860491603814]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [275.72816403674864]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1308.8319522030629]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [521.8443705782857]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [564.1399766512167]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1085.1114012108064]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1204.411777979802]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=970000, episode_reward=934.37 +/- 329.57\n",
            "Episode length: 6675.60 +/- 903.06\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 6.68e+03    |\n",
            "|    mean_reward          | 934         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 970000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009923423 |\n",
            "|    clip_fraction        | 0.0996      |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -10.2       |\n",
            "|    explained_variance   | 0.389       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.69        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | 0.00583     |\n",
            "|    std                  | 40.5        |\n",
            "|    value_loss           | 29.7        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [393.37766864702223]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [643.4452228248498]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [818.9236139693041]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [656.1680728651118]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [656.3224181737172]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=980000, episode_reward=631.15 +/- 136.51\n",
            "Episode length: 5598.60 +/- 1053.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5.6e+03  |\n",
            "|    mean_reward     | 631      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 980000   |\n",
            "---------------------------------\n",
            "Final Scores:  [499.4118343645274]\n",
            "Game Over - Player fell\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.17e+03 |\n",
            "|    ep_rew_mean     | 390      |\n",
            "| time/              |          |\n",
            "|    fps             | 323      |\n",
            "|    iterations      | 48       |\n",
            "|    time_elapsed    | 3040     |\n",
            "|    total_timesteps | 983040   |\n",
            "---------------------------------\n",
            "Final Scores:  [484.00606300242146]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [338.57493452022686]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [426.80273179744546]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1336.7029718902431]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [572.4928961907085]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [362.0859947987072]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [468.2348024251086]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [699.6715361799235]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=990000, episode_reward=685.34 +/- 343.16\n",
            "Episode length: 5939.80 +/- 818.18\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 5.94e+03    |\n",
            "|    mean_reward          | 685         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 990000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013209361 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -10.4       |\n",
            "|    explained_variance   | 0.304       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 31          |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | 0.00504     |\n",
            "|    std                  | 43.9        |\n",
            "|    value_loss           | 42.4        |\n",
            "-----------------------------------------\n",
            "Final Scores:  [432.6019875587478]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [94.38151471282582]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [265.2768824316919]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1475.563600531748]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [633.7231866412732]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [866.1631027675803]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [1007.0834022085978]\n",
            "Game Over - Player fell\n",
            "Final Scores:  [663.5771323310302]\n",
            "Game Over - Player fell\n",
            "Eval num_timesteps=1000000, episode_reward=926.72 +/- 305.42\n",
            "Episode length: 6764.00 +/- 1693.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 6.76e+03 |\n",
            "|    mean_reward     | 927      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.2e+03  |\n",
            "|    ep_rew_mean     | 390      |\n",
            "| time/              |          |\n",
            "|    fps             | 323      |\n",
            "|    iterations      | 49       |\n",
            "|    time_elapsed    | 3098     |\n",
            "|    total_timesteps | 1003520  |\n",
            "---------------------------------\n",
            "Training completed!\n",
            "Adversarial training completed!\n"
          ]
        }
      ],
      "source": [
        "n_envs = 5\n",
        "\n",
        "# Choose whether to do hyperparameter optimization or direct training\n",
        "do_optimization = False\n",
        "\n",
        "model_cfg = model_config()\n",
        "train_cfg = train_config()\n",
        "window_x = 1000\n",
        "window_y = 1000\n",
        "\n",
        "if do_optimization: # game_screen, state_based\n",
        "    # optuna_optimizer = Optuna_optimize(obs_type=model_cfg.model_obs_type, level=1)\n",
        "    # n_trials = 10\n",
        "    # best_trial = optuna_optimizer.optuna_parameter_tuning(n_trials=n_trials)\n",
        "    # print(f\"best_trial found: {best_trial}\")\n",
        "\n",
        "    pass\n",
        "else:\n",
        "    # Create trainer for adversarial training\n",
        "    training = Train(\n",
        "        model_cfg=model_cfg,\n",
        "        train_cfg=train_cfg,\n",
        "        n_envs=n_envs,\n",
        "        load_model=None,  # Start fresh for adversarial training\n",
        "        window_x=window_x,\n",
        "        window_y=window_y,\n",
        "    )\n",
        "\n",
        "    model = training.train_ppo()\n",
        "\n",
        "    print(\"Adversarial training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RD0qswxU0IuD",
        "outputId": "be455da1-a35c-49e8-a0c4-3b808b31cde5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/models/best_model.zip': No such file or directory\n",
            "lsof: status error on /dev/accel0: No such file or directory\n",
            "lsof 4.93.2\n",
            " latest revision: https://github.com/lsof-org/lsof\n",
            " latest FAQ: https://github.com/lsof-org/lsof/blob/master/00FAQ\n",
            " latest (non-formatted) man page: https://github.com/lsof-org/lsof/blob/master/Lsof.8\n",
            " usage: [-?abhKlnNoOPRtUvVX] [+|-c c] [+|-d s] [+D D] [+|-E] [+|-e s] [+|-f[gG]]\n",
            " [-F [f]] [-g [s]] [-i [i]] [+|-L [l]] [+m [m]] [+|-M] [-o [o]] [-p s]\n",
            " [+|-r [t]] [-s [p:s]] [-S [t]] [-T [t]] [-u s] [+|-w] [-x [fl]] [--] [names]\n",
            "Use the ``-h'' option to get more help information.\n"
          ]
        }
      ],
      "source": [
        "# Copy the best model to a stable location\n",
        "!cp /content/models/best_model.zip /content/drive/MyDrive/RL_Models/best_model_$(date +%Y%m%d_%H%M%S).zip\n",
        "\n",
        "# Optional: Monitor TPU usage\n",
        "if TPU_AVAILABLE:\n",
        "    !sudo lsof -w /dev/accel0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tLCe2GS6Kb8K",
        "outputId": "f888af25-720e-4a1d-e574-af29a6389724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model not found at /content/models/best_model.zip\n"
          ]
        }
      ],
      "source": [
        "# Load a saved model and continue training or evaluate\n",
        "model_path = \"/content/models/best_model.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Loading model from {model_path} for evaluation\")\n",
        "\n",
        "    # Create trainer with the saved model\n",
        "    eval_trainer = Train(\n",
        "        model_cfg=model_config(),\n",
        "        train_cfg=train_config(),\n",
        "        n_envs=1,  # Use 1 env for evaluation\n",
        "        window_x=window_x,\n",
        "        window_y=window_y,\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_trainer.evaluate(\n",
        "        model_path=model_path,\n",
        "        n_episodes=5,\n",
        "    )\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnme-c5KPlc"
      },
      "source": [
        "# --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x4h3jBARKPld",
        "outputId": "cd152705-e2a5-422c-a879-05fbc9641f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the json memory file\n",
            "Loading default level configurations...\n",
            "Using collision_type: {'player': 1000, 'platform': 2000, 'fallingRock': 3000}\n",
            "Using player_configs: [{'shape_type': 'circle', 'size': [0.03], 'shape_mass': 1, 'shape_friction': 100, 'shape_elasticity': 0.8, 'default_position': [0.5, 0.6], 'default_velocity': [0, 0], 'abilities': ['Collision'], 'health': 10, 'color': [255, 213, 79]}]\n",
            "Using level_configs: {'platform_configs': [{'shape_type': 'rectangle', 'size': [0.8, 0.04], 'shape_mass': 1, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': [0.5, 0.67], 'default_velocity': [0, 0], 'abilities': [], 'health': 'infinite', 'color': [235, 64, 52]}], 'falling_rock_configs': [{'shape_type': 'rectangle', 'size': [0.05, 0.05], 'shape_mass': 0.5, 'shape_friction': 0.7, 'shape_elasticity': 0.1, 'default_position': ['random', -0.001], 'default_velocity': ['random', 0], 'abilities': [], 'health': 'infinite', 'color': [100, 100, 100]}], 'entities_configs': {'quantity': {'fallingRock': 1}}, 'environment_configs': [{'gravity': [0, 4000], 'damping': 0.5}], 'reward': {'reward_per_step': 0.0, 'collision_falling_rock': 10, 'falling_rock_fall_outside_platform': 0.0, 'falling_rock_fall_on_platform': -2.5, 'falling_rock_near': 0.4, 'falling_rock_near_distance_threshold': 200, 'falling_rock_near_distance_reward_multiplier': 5, 'steps_limit_for_movement_penalty': 100, 'movement_penalty': -0.08, 'fail_penalty': 1.0, 'speed_reward_proportion': 0.0, 'opponent_fell_reward': 0.0, 'survival_bonus': 0.0}}\n",
            "Initialized Role with abilities: ['Collision']\n",
            "Created 1 players and 1 platforms.\n",
            "PlayerFallingRockCollisionReward c._terminates_function: True\n",
            "Sound loading error\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                <div id=\"pygame-output\" style=\"width:100%;\">\n",
              "                    <img id=\"pygame-img\" style=\"width:100%;\">\n",
              "                </div>\n",
              "            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for /: 'int' and 'NoneType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11719748.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_standalone_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gym_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Test the adversarial training environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_standalone_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array_and_human_in_colab\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# test_gym_env(level=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Balancing_Ball_for_RL/game/test.py\u001b[0m in \u001b[0;36mrun_standalone_game\u001b[0;34m(render_mode, capture_per_second, window_x, window_y, max_episode_step, collision_type, player_configs, platform_configs, environment_configs, level, fps)\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_standalone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_gym_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Balancing_Ball_for_RL/game/balancing_ball_game.py\u001b[0m in \u001b[0;36mrun_standalone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# Take game step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# Render\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Balancing_Ball_for_RL/game/balancing_ball_game.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, pactions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# Step the physics simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# 在物理糢擬后執行動作會導致環境數據過時，但是當 FPS 較高時，這種影響可以忽略不計\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "from game.test import run_standalone_game, test_gym_env\n",
        "# Test the adversarial training environment\n",
        "run_standalone_game(render_mode=\"rgb_array_and_human_in_colab\", window_x=1000, window_y=600, level=3)\n",
        "# test_gym_env(level=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCFm3AoWs45R"
      },
      "outputs": [],
      "source": [
        "# Example of creating the environment with continuous action space for adversarial training\n",
        "env = BalancingBallEnv(\n",
        "    render_mode=train_cfg.render_mode,\n",
        "    fps=model_cfg.fps,\n",
        "    obs_type=model_cfg.model_obs_type,\n",
        "    image_size=model_cfg.image_size,\n",
        "    level=model_cfg.level,  # Level 3 for adversarial training\n",
        ")\n",
        "\n",
        "# Reset environment to get initial observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Print observation and action space info\n",
        "print(f\"Observation shape: {obs.shape}\")  # Should be (84, 84, 3) for grayscale with 3 stacked frames\n",
        "print(f\"Action space: {env.action_space}\")  # Should be Box(low=-1, high=1, shape=(2,))\n",
        "print(f\"Action space shape: {env.action_space.shape}\")  # Should be (2,) for two players\n",
        "\n",
        "# Test a random continuous action\n",
        "action = env.action_space.sample()\n",
        "print(f\"Sample action: {action}\")  # Should be array of 2 values between -1 and 1\n",
        "\n",
        "# Take a step\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Step result - Reward: {reward}, Individual rewards: {info.get('individual_rewards', [])}\")\n",
        "\n",
        "# Display a sample observation (first frame only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(obs[:,:,0], cmap='gray')\n",
        "plt.title(\"Adversarial Training - Grayscale Observation\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hhEqO-xFu4AI",
        "cnA8wZtosmeN",
        "v-8d5fKltI62",
        "gjobL-nozI81"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}